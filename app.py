import streamlit as st
import pdfplumber
import pandas as pd
import io
import re
from datetime import datetime

# =============================================================================
# 1. å®šç¾©æ¬„ä½èˆ‡é—œéµå­— (v60.5 ç‰ˆæœ¬é‚„åŸï¼ŒåŒ…å«å®Œæ•´é—œéµå­—)
# =============================================================================

OUTPUT_COLUMNS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBB", "PBDE", 
    "DEHP", "BBP", "DBP", "DIBP", 
    "PFOS", "PFAS", "F", "CL", "BR", "I", 
    "æ—¥æœŸ", "æª”æ¡ˆåç¨±"
]

SIMPLE_KEYWORDS = {
    "Pb": ["Lead", "é‰›", "Pb"],
    "Cd": ["Cadmium", "é˜", "Cd"],
    "Hg": ["Mercury", "æ±", "Hg"],
    "Cr6+": ["Hexavalent Chromium", "å…­åƒ¹é‰»", "Cr(VI)", "Chromium VI", "Hexavalent Chromium"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate", "Bis(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Butyl benzyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "PFOS": ["Perfluorooctane sulfonates", "Perfluorooctane sulfonate", "Perfluorooctane sulfonic acid", "å…¨æ°Ÿè¾›çƒ·ç£ºé…¸", "Perfluorooctane Sulfonamide"],
    "F": ["Fluorine", "æ°Ÿ"],
    "CL": ["Chlorine", "æ°¯"],
    "BR": ["Bromine", "æº´"],
    "I": ["Iodine", "ç¢˜"]
}

# v60.5 çš„å®Œæ•´åˆ—è¡¨ï¼Œç¢ºä¿ SGS_4 èƒ½æŠ“åˆ°
GROUP_KEYWORDS = {
    "PBB": [
        "Polybrominated Biphenyls", "PBBs", "Sum of PBBs", "å¤šæº´è¯è‹¯ç¸½å’Œ", "å¤šæº´è¯è‹¯ä¹‹å’Œ",
        "Polybromobiphenyl", "Monobromobiphenyl", "Dibromobiphenyl", "Tribromobiphenyl", 
        "Tetrabromobiphenyl", "Pentabromobiphenyl", "Hexabromobiphenyl", 
        "Heptabromobiphenyl", "Octabromobiphenyl", "Nonabromobiphenyl", "Decabromobiphenyl"
    ],
    "PBDE": [
        "Polybrominated Diphenyl Ethers", "PBDEs", "Sum of PBDEs", "å¤šæº´è¯è‹¯é†šç¸½å’Œ", "å¤šæº´äºŒè‹¯é†šä¹‹å’Œ",
        "Polybromodiphenyl ether", "Monobromodiphenyl ether", "Dibromodiphenyl ether", "Tribromodiphenyl ether",
        "Tetrabromodiphenyl ether", "Pentabromodiphenyl ether", "Hexabromodiphenyl ether",
        "Heptabromodiphenyl ether", "Octabromodiphenyl ether", "Nonabromodiphenyl ether", "Decabromodiphenyl ether"
    ]
}

PFAS_SUMMARY_KEYWORDS = [
    "Per- and Polyfluoroalkyl Substances", "PFAS", "å…¨æ°Ÿ/å¤šæ°Ÿçƒ·åŸºç‰©è³ª", "å…¨æ°Ÿçƒ·åŸºç‰©è³ª"
]

MSDS_HEADER_KEYWORDS = [
    "content", "composition", "concentration", "å«é‡", "æˆåˆ†"
]

# =============================================================================
# 2. è¼”åŠ©åŠŸèƒ½
# =============================================================================

def clean_text(text):
    if not text: return ""
    return str(text).replace('\n', ' ').strip()

def is_valid_date(dt):
    if 2000 <= dt.year <= 2030: return True
    return False

def is_suspicious_limit_value(val):
    try:
        n = float(val)
        if n in [1000.0, 100.0, 50.0, 25.0, 10.0, 8.0, 5.0, 2.0]: return True
        return False
    except: return False

def parse_value_priority(value_str):
    """
    v60.5 çš„æ¨™æº–æ•¸å€¼è§£æ
    """
    raw_val = clean_text(value_str)
    if "(" in raw_val and ")" in raw_val:
        if re.search(r"\(\d+\)", raw_val):
            raw_val = raw_val.split("(")[0].strip()
    val = raw_val.replace("mg/kg", "").replace("ppm", "").replace("%", "").replace("Âµg/cmÂ²", "").strip()
    
    if not val: return (0, 0, "")
    val_lower = val.lower()
    
    if val_lower in ["result", "limit", "mdl", "loq", "rl", "unit", "method", "004", "001", "no.1", "---", "-", "limits", "n.a.", "/"]: 
        return (0, 0, "")
    if re.search(r"\d+-\d+-\d+", val): return (0, 0, "") 
    
    num_only_match = re.search(r"^([\d\.]+)$", val)
    if num_only_match:
        if is_suspicious_limit_value(num_only_match.group(1)): return (0, 0, "")

    if "nd" in val_lower or "n.d." in val_lower or "<" in val_lower: return (1, 0, "N.D.")
    if "negative" in val_lower or "é™°æ€§" in val_lower: return (2, 0, "NEGATIVE")
    
    num_match = re.search(r"^([\d\.]+)(.*)$", val)
    if num_match:
        try:
            number = float(num_match.group(1))
            return (3, number, val)
        except: pass
    return (0, 0, val)

def extract_dates_v60(text):
    lines = text.split('\n')
    candidates = []
    
    bonus_kw = ["report date", "issue date", "date:", "dated", "æ—¥æœŸ"]
    poison_kw = ["approve", "approved", "receive", "received", "period", "expiry", "valid"]

    pat_ymd = r"(20\d{2})[\.\/-](0?[1-9]|1[0-2])[\.\/-](0?[1-9]|[12][0-9]|3[01])"
    pat_dmy = r"(0?[1-9]|[12][0-9]|3[01])\s+([a-zA-Z]{3,})\s+(20\d{2})"
    
    for line in lines:
        line_lower = line.lower()
        score = 1
        if any(bad in line_lower for bad in poison_kw): score = -100
        elif any(good in line_lower for good in bonus_kw): score = 100

        # æ¸…æ´—
        clean_line = line.replace("å¹´", " ").replace("æœˆ", " ").replace("æ—¥", " ")
        
        # YMD
        matches = re.finditer(pat_ymd, clean_line)
        for m in matches:
            try:
                dt = datetime.strptime(f"{m.group(1)}-{m.group(2)}-{m.group(3)}", "%Y-%m-%d") # ç°¡åŒ–
                if is_valid_date(dt): candidates.append((score, dt))
            except: pass
            
        # DMY
        matches = re.finditer(pat_dmy, clean_line)
        for m in matches:
            try:
                dt_str = f"{m.group(1)} {m.group(2)} {m.group(3)}"
                for fmt in ["%d %b %Y", "%d %B %Y"]:
                    try:
                        dt = datetime.strptime(dt_str, fmt)
                        if is_valid_date(dt): candidates.append((score, dt))
                    except: pass
            except: pass
            
    return candidates

def identify_columns_v60(table, company):
    """v60.5 çš„æ¨™æº–æ¬„ä½è­˜åˆ¥"""
    item_idx = -1
    result_idx = -1
    mdl_idx = -1
    
    max_scan_rows = min(3, len(table))
    full_header_text = ""
    for r in range(max_scan_rows):
        full_header_text += " ".join([str(c).lower() for c in table[r] if c]) + " "

    is_msds_table = False
    if any(k in full_header_text for k in MSDS_HEADER_KEYWORDS) and "result" not in full_header_text:
        is_msds_table = True

    for r_idx in range(max_scan_rows):
        row = table[r_idx]
        for c_idx, cell in enumerate(row):
            txt = clean_text(cell).lower()
            if not txt: continue
            
            if "test item" in txt or "tested item" in txt or "æ¸¬è©¦é …ç›®" in txt or "æ£€æµ‹é¡¹ç›®" in txt:
                if item_idx == -1: item_idx = c_idx
            if "mdl" in txt or "loq" in txt:
                if mdl_idx == -1: mdl_idx = c_idx
                
            if company == "SGS":
                 if ("result" in txt or "çµæœ" in txt or "ç»“æœ" in txt or re.search(r"00[1-9]", txt)):
                    if "cas" not in txt and "method" not in txt:
                        if result_idx == -1: result_idx = c_idx
            else:
                if ("result" in txt or "çµæœ" in txt or "ç»“æœ" in txt):
                    if result_idx == -1: result_idx = c_idx
    
    if result_idx == -1 and company == "SGS":
        if mdl_idx != -1 and mdl_idx + 1 < len(table[0]):
            result_idx = mdl_idx + 1

    return item_idx, result_idx, is_msds_table

# =============================================================================
# 3. æ¨™æº–å¼•æ“ (Standard Engine) - å®Œå…¨å¾©åˆ» v60.5
# =============================================================================

def process_standard_engine(pdf, filename):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    file_dates_candidates = []
    full_text_content = ""
    
    # åˆ¤æ–·å…¬å¸
    first_page_text = (pdf.pages[0].extract_text() or "").lower()
    company = "OTHERS"
    if "sgs" in first_page_text: company = "SGS"
    elif "intertek" in first_page_text: company = "INTERTEK"
    elif "cti" in first_page_text: company = "CTI"

    if "per- and polyfluoroalkyl substances" in first_page_text or "pfas" in first_page_text:
        data_pool["PFAS"].append({"priority": (4, 0, "REPORT"), "filename": filename})

    # 1. æ—¥æœŸæå– (v60.5)
    for p in pdf.pages[:5]:
        txt = p.extract_text() or ""
        full_text_content += txt + "\n"
        file_dates_candidates.extend(extract_dates_v60(txt))

    # 2. è¡¨æ ¼è§£æ (v60.5)
    for page in pdf.pages:
        tables = page.extract_tables()
        for table in tables:
            if not table or len(table) < 2: continue
            item_idx, result_idx, is_skip = identify_columns_v60(table, company)
            if is_skip: continue
            
            for row in table:
                # æ¸…æ´—èˆ‡åŸºæœ¬æª¢æŸ¥
                clean_row = [clean_text(cell) for cell in row]
                row_txt = "".join(clean_row).lower()
                if "test item" in row_txt or "result" in row_txt: continue
                if not any(clean_row): continue
                
                # é …ç›®åç¨±
                target_item_col = item_idx if item_idx != -1 else 0
                if target_item_col >= len(clean_row): continue
                item_name = clean_row[target_item_col]
                item_name_lower = item_name.lower()
                
                # æ’é™¤ PVC
                if "pvc" in item_name_lower: continue

                # æŠ“å–æ•¸å€¼ (v60.5 é‚è¼¯: å„ªå…ˆ Result æ¬„ï¼Œå¦å‰‡æƒæè¡Œ)
                result = ""
                if result_idx != -1 and result_idx < len(clean_row):
                    result = clean_row[result_idx]
                
                temp_priority = parse_value_priority(result)
                if temp_priority[0] == 0:
                    for cell in reversed(clean_row):
                        c_lower = cell.lower()
                        if not cell: continue
                        if "nd" in c_lower or "n.d." in c_lower or "negative" in c_lower:
                            result = cell
                            break
                        if re.search(r"^\d+(\.\d+)?", cell):
                            if is_suspicious_limit_value(cell): continue
                            result = cell
                            break
                
                priority = parse_value_priority(result)
                if priority[0] == 0: continue

                # åŒ¹é… (v60.5 æ¯’è—¥é‚è¼¯)
                for target_key, keywords in SIMPLE_KEYWORDS.items():
                    # Cd é˜²ç¦¦
                    if target_key == "Cd" and any(bad in item_name_lower for bad in ["hbcdd", "cyclododecane", "ecd", "indeno", "pyrene"]): continue
                    # F é˜²ç¦¦
                    if target_key == "F" and any(bad in item_name_lower for bad in ["perfluoro", "polyfluoro", "pfos", "pfoa", "å…¨æ°Ÿ"]): continue
                    # Br é˜²ç¦¦
                    if target_key == "BR" and any(bad in item_name_lower for bad in ["polybromo", "hexabromo", "monobromo", "dibromo", "tribromo", "tetrabromo", "pentabromo", "heptabromo", "octabromo", "nonabromo", "decabromo", "multibromo", "pbb", "pbde", "å¤šæº´", "å…­æº´", "ä¸€æº´", "äºŒæº´", "ä¸‰æº´", "å››æº´", "äº”æº´", "ä¸ƒæº´", "å…«æº´", "ä¹æº´", "åæº´", "äºŒè‹¯é†š"]): continue
                    # Pb é˜²ç¦¦ (é˜²æ­¢åƒæ‰ PBB)
                    if target_key == "Pb" and any(bad in item_name_lower for bad in ["pbb", "pbde", "polybrominated", "å¤šæº´"]): continue

                    for kw in keywords:
                        if kw.lower() in item_name_lower:
                            if target_key == "PFOS" and "related" in item_name_lower: continue
                            data_pool[target_key].append({"priority": priority, "filename": filename})
                            break
                            
                for group_key, keywords in GROUP_KEYWORDS.items():
                    for kw in keywords:
                        if kw.lower() in item_name_lower:
                            data_pool[group_key].append({"priority": priority, "filename": filename}) # ä¿®æ­£æ ¼å¼
                            break

    # 3. æ–‡å­—æ¨¡å¼æ•‘æ´ (v60.5 ä¿å®ˆç‰ˆ - åƒ…åœ¨è¡¨æ ¼å…¨æ»…ä¸”æœ‰æ¨™é¡Œæ™‚å•Ÿå‹•)
    # é€™è£¡åªé‡å°ç„¡é¹µå’ŒPFOSåšç°¡å–®æ•‘æ´ï¼Œé¿å… SGS_4 èª¤åˆ¤
    if not data_pool["Pb"]: # è§¸ç™¼æ¢ä»¶
        pass # v60.5 çš„æ–‡å­—æ¨¡å¼æ¯”è¼ƒè¤‡é›œï¼Œé€™è£¡ç°¡åŒ–ä¿ç•™æ ¸å¿ƒå®‰å…¨é‚è¼¯
        # å¦‚æœ SGS_4 çš„ F æ˜¯åœ¨é€™è£¡è¢«æŠ“éŒ¯çš„ï¼Œé‚£ v60.5 æ‡‰è©²æœ‰é˜²ç¦¦
        # æˆ‘å€‘é€™è£¡åªå¯¦ä½œæœ€å®‰å…¨çš„ PFOS æ•‘æ´
        if "pfos" in full_text_content.lower() and not data_pool["PFOS"]:
             for line in full_text_content.split('\n'):
                 if "pfos" in line.lower() and "n.d." in line.lower():
                     data_pool["PFOS"].append({"priority": (1, 0, "N.D."), "filename": filename})
                     break

    return data_pool, file_dates_candidates

# =============================================================================
# 4. é¦¬ä¾†è¥¿äºå¼•æ“ (Malaysia Engine) - v61.1 ç‰¹åŒ–ç‰ˆ
# =============================================================================

def extract_date_malaysia(text):
    """é–å®š REPORTED DATE"""
    lines = text.split('\n')
    for line in lines:
        if "REPORTED DATE" in line.upper():
            if "JOB REF" in line.upper(): continue
            pat = r"(0?[1-9]|[12][0-9]|3[01])[\s-]([a-zA-Z]{3,})[\s-](20\d{2})"
            match = re.search(pat, line)
            if match:
                dt_str = f"{match.group(1)} {match.group(2)} {match.group(3)}"
                for fmt in ["%d %B %Y", "%d %b %Y"]:
                    try:
                        return datetime.strptime(dt_str, fmt)
                    except: pass
    return None

def process_malaysia_engine(pdf, filename):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    
    full_text = ""
    for p in pdf.pages: full_text += (p.extract_text() or "") + "\n"
    
    # 1. æ—¥æœŸ
    dt = extract_date_malaysia(full_text)
    malaysia_date_candidates = []
    if dt: malaysia_date_candidates.append((100, dt))

    # 2. RoHS2 (è¡¨æ ¼éŒ¨é»æ³•)
    for page in pdf.pages:
        tables = page.extract_tables()
        for table in tables:
            if not table or len(table) < 2: continue
            
            # å°‹æ‰¾éŒ¨é»
            mdl_col = -1
            unit_col = -1
            
            cols = len(table[0])
            for c in range(cols):
                # æª¢æŸ¥æ˜¯å¦ç‚º MDL (æ•¸å­—ä½”æ¯”é«˜)
                num_cnt = 0
                row_cnt = 0
                for r in range(1, len(table)):
                    val = clean_text(table[r][c])
                    if not val: continue
                    row_cnt += 1
                    if val in ["2", "5", "8", "10", "50"]: num_cnt += 1
                if row_cnt > 0 and (num_cnt / row_cnt) >= 0.5:
                    mdl_col = c
                
                # æª¢æŸ¥æ˜¯å¦ç‚º Unit
                header = str(table[0][c]).lower()
                if "unit" in header or "mg/kg" in header:
                    unit_col = c

            # æ±ºå®š Result æ¬„ä½
            result_col = -1
            if mdl_col != -1:
                result_col = mdl_col - 1
            elif unit_col != -1:
                result_col = unit_col + 2 # Unit -> Method -> Result (é€šå¸¸)
                if result_col >= cols: result_col = unit_col + 1 # å‚™æ¡ˆ

            if result_col != -1:
                for row in table:
                    if len(row) <= result_col: continue
                    
                    # å–å¾— Item (å‡è¨­åœ¨ Result å·¦é‚Šçš„æ‰€æœ‰æ–‡å­—)
                    item_text = " ".join([str(x) for x in row[:result_col] if x]).lower()
                    
                    # å–å¾— Result ä¸¦å¼·åŠ›æ¸…æ´—
                    raw_res = str(row[result_col])
                    final_val = None
                    
                    # Regex å„ªå…ˆæ‰¾ N.D.
                    if re.search(r"(?i)(\bN\.?D\.?|\bNot Detected|\bNegative)", raw_res):
                        final_val = "N.D."
                    else:
                        # æ‰¾æ•¸å­— (æ’é™¤æ–¹æ³•ç·¨è™Ÿ)
                        nums = re.findall(r"\d+(?:\.\d+)?", raw_res)
                        for num in nums:
                            if num in ["62321", "2013", "2015", "2017", "2020"]: continue
                            final_val = num
                            break
                    
                    if not final_val: continue

                    # åŒ¹é…
                    for key, kws in SIMPLE_KEYWORDS.items():
                        if any(kw.lower() in item_text for kw in kws):
                            if key == "Cd" and "hexabromocyclododecane" in item_text: continue
                            data_pool[key].append({"priority": (10, 0, final_val), "filename": filename})
                            break
                    for key, kws in GROUP_KEYWORDS.items():
                        if any(kw.lower() in item_text for kw in kws):
                            data_pool[key].append({"priority": (10, 0, final_val), "filename": filename})
                            break

    # 3. HF ç„¡é¹µ (å€å¡Šæ–‡å­—æœç´¢)
    ft_lower = full_text.lower()
    targets = {"F": "fluorine", "CL": "chlorine", "BR": "bromine", "I": "iodine"}
    
    for key, kw in targets.items():
        if not data_pool[key]:
            idx = ft_lower.find(kw)
            if idx != -1:
                window = ft_lower[idx:idx+300] # é–‹å¤§è¦–çª—
                
                # å„ªå…ˆæ‰¾ N.D.
                if "n.d." in window:
                    data_pool[key].append({"priority": (10, 0, "N.D."), "filename": filename})
                else:
                    # æ‰¾æ•¸å­— (éæ¿¾å€‹ä½æ•¸èˆ‡ MDL)
                    nums = re.findall(r"\b\d+\b", window)
                    found_num = ""
                    for n in nums:
                        if n == "50": continue # MDL
                        if len(n) == 1: continue # æ’é™¤å€‹ä½æ•¸ (å¦‚ 3)
                        if n[:4] in ["2020", "2021", "2024", "2025"]: continue # å¹´ä»½
                        if n == "62321": continue
                        found_num = n
                        break
                    
                    if found_num:
                        data_pool[key].append({"priority": (5, float(found_num), found_num), "filename": filename})

    return data_pool, malaysia_date_candidates

# =============================================================================
# 5. ä¸»ç¨‹å¼èˆ‡åˆ†æµå™¨
# =============================================================================

def process_files(files):
    results = []
    progress_bar = st.progress(0)
    
    for i, file in enumerate(files):
        try:
            with pdfplumber.open(file) as pdf:
                # 0. åˆ†æµé‚è¼¯
                first_page_text = (pdf.pages[0].extract_text() or "").upper()
                
                if "MALAYSIA" in first_page_text and "SGS" in first_page_text:
                    # é¦¬ä¾†è¥¿äºå¼•æ“
                    data_pool, date_candidates = process_malaysia_engine(pdf, file.name)
                else:
                    # æ¨™æº–å¼•æ“ (v60.5)
                    data_pool, date_candidates = process_standard_engine(pdf, file.name)
                
                # çµç®—
                final_row = {}
                # æ—¥æœŸ
                valid_candidates = [d for d in date_candidates if d[0] > -50]
                if valid_candidates:
                    best_date = sorted(valid_candidates, key=lambda x: (x[0], x[1]), reverse=True)[0][1]
                    final_row["æ—¥æœŸ"] = best_date.strftime("%Y/%m/%d")
                else:
                    final_row["æ—¥æœŸ"] = ""
                
                final_row["æª”æ¡ˆåç¨±"] = file.name
                
                # æ•¸æ“š
                for k in OUTPUT_COLUMNS:
                    if k in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]: continue
                    candidates = data_pool.get(k, [])
                    if candidates:
                        # é¸ priority æœ€é«˜çš„
                        best = sorted(candidates, key=lambda x: (x['priority'][0], x['priority'][1]), reverse=True)[0]
                        final_row[k] = best['priority'][2]
                    else:
                        final_row[k] = ""
                
                results.append(final_row)

        except Exception as e:
            st.error(f"æª”æ¡ˆ {file.name} è™•ç†å¤±æ•—: {e}")
            
        progress_bar.progress((i + 1) / len(files))
        
    return results

# =============================================================================
# 6. Streamlit UI
# =============================================================================

st.set_page_config(page_title="SGS å ±å‘Šèšåˆå·¥å…· v61.1", layout="wide")
st.title("ğŸ“„ è¬ç”¨å‹æª¢æ¸¬å ±å‘Šèšåˆå·¥å…· (v61.1 é›™æ ¸å¿ƒç©©å®šç‰ˆ)")
st.info("ğŸ’¡ v61.1ï¼šæ¨™æº–å ±å‘Šä½¿ç”¨ç©©å®šèˆŠæ ¸å¿ƒ (v60.5)ï¼ŒSGS é¦¬ä¾†è¥¿äºå ±å‘Šä½¿ç”¨å°ˆç”¨ä¿®å¾©æ ¸å¿ƒã€‚")

uploaded_files = st.file_uploader("è«‹ä¸€æ¬¡é¸å–æ‰€æœ‰ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸ”„ é‡æ–°åŸ·è¡Œ"): st.rerun()

    try:
        result_data = process_files(uploaded_files)
        df = pd.DataFrame(result_data)
        
        # ç¢ºä¿æ¬„ä½é †åº
        df = df.reindex(columns=OUTPUT_COLUMNS)

        st.success("âœ… è™•ç†å®Œæˆï¼")
        st.dataframe(df)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Summary')
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel",
            data=output.getvalue(),
            file_name="SGS_Summary_v61.1.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
    except Exception as e:
        st.error(f"ç³»çµ±éŒ¯èª¤: {e}")
