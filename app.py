import streamlit as st
import pdfplumber
import pandas as pd
import io
import re
from datetime import datetime

# --- 1. å®šç¾©æ¬„ä½èˆ‡é—œéµå­— ---

SIMPLE_KEYWORDS = {
    "Pb": ["Lead", "é‰›", "Pb"],
    "Cd": ["Cadmium", "é˜", "Cd"],
    "Hg": ["Mercury", "æ±", "Hg"],
    "Cr6+": ["Hexavalent Chromium", "å…­åƒ¹é‰»", "Cr(VI)", "Chromium VI", "Hexavalent Chromium"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate", "Bis(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Butyl benzyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "PFOS": ["Perfluorooctane sulfonates", "Perfluorooctane sulfonate", "Perfluorooctane sulfonic acid", "å…¨æ°Ÿè¾›çƒ·ç£ºé…¸", "Perfluorooctane Sulfonamide"],
    "F": ["Fluorine", "æ°Ÿ"],
    "CL": ["Chlorine", "æ°¯"],
    "BR": ["Bromine", "æº´"],
    "I": ["Iodine", "ç¢˜"]
}

GROUP_KEYWORDS = {
    "PBB": [
        "Polybrominated Biphenyls", "PBBs", "Sum of PBBs", "å¤šæº´è¯è‹¯ç¸½å’Œ", "å¤šæº´è¯è‹¯ä¹‹å’Œ", "å¤šæº´è”è‹¯ä¹‹å’Œ",
        "Polybromobiphenyl", "Polybromobiphenyls",
        "Monobromobiphenyl", "Dibromobiphenyl", "Tribromobiphenyl", 
        "Tetrabromobiphenyl", "Pentabromobiphenyl", "Hexabromobiphenyl", 
        "Heptabromobiphenyl", "Octabromobiphenyl", "Nonabromobiphenyl", 
        "Decabromobiphenyl", 
        "Monobrominated", "Dibrominated", "Tribrominated", 
        "Tetrabrominated", "Pentabrominated", "Hexabrominated", 
        "Heptabrominated", "Octabrominated", "Nonabrominated", 
        "Decabrominated", "bromobiphenyl"
    ],
    "PBDE": [
        "Polybrominated Diphenyl Ethers", "PBDEs", "Sum of PBDEs", "å¤šæº´è¯è‹¯é†šç¸½å’Œ", "å¤šæº´äºŒè‹¯é†šä¹‹å’Œ", "å¤šæº´äºŒè‹¯é†šä¹‹å’Œ",
        "Polybromodiphenyl ether", "Polybromodiphenyl ethers",
        "Monobromodiphenyl ether", "Dibromodiphenyl ether", "Tribromodiphenyl ether",
        "Tetrabromodiphenyl ether", "Pentabromodiphenyl ether", "Hexabromodiphenyl ether",
        "Heptabromodiphenyl ether", "Octabromodiphenyl ether", "Nonabromodiphenyl ether",
        "Decabromodiphenyl ether", 
        "Monobrominated Diphenyl", "Dibrominated Diphenyl", "Tribrominated Diphenyl",
        "Tetrabrominated Diphenyl", "Pentabrominated Diphenyl", "Hexabrominated Diphenyl",
        "Heptabrominated Diphenyl", "Octabrominated Diphenyl", "Nonabrominated Diphenyl",
        "Decabrominated Diphenyl", "bromodiphenyl ether"
    ]
}

PFAS_SUMMARY_KEYWORDS = [
    "Per- and Polyfluoroalkyl Substances", "PFAS", "å…¨æ°Ÿ/å¤šæ°Ÿçƒ·åŸºç‰©è³ª", "å…¨æ°Ÿçƒ·åŸºç‰©è³ª"
]

MSDS_HEADER_KEYWORDS = [
    "content", "composition", "concentration", "å«é‡", "æˆåˆ†"
]

OUTPUT_COLUMNS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBB", "PBDE", 
    "DEHP", "BBP", "DBP", "DIBP", 
    "PFOS", "PFAS", "F", "CL", "BR", "I", 
    "æ—¥æœŸ", "æª”æ¡ˆåç¨±"
]

# --- 2. è¼”åŠ©åŠŸèƒ½ ---

def clean_text(text):
    if not text: return ""
    return str(text).replace('\n', ' ').strip()

def find_report_start_page(pdf):
    for i in range(min(10, len(pdf.pages))):
        text = (pdf.pages[i].extract_text() or "").lower()
        if "test report" in text or "æ¸¬è©¦å ±å‘Š" in text:
            return i
    return 0

def extract_dates_v60_5(text):
    """
    v60.5: è¬èƒ½æ¸…æ´— (å«ä¸­æ–‡) + ç©åˆ†éæ¿¾
    """
    lines = text.split('\n')
    candidates = [] # (score, date_object)
    
    bonus_kw = ["report date", "issue date", "date:", "dated", "æ—¥æœŸ"]
    poison_kw = [
        "approve", "approved", "approval", "approver", 
        "check", "checked", "review", "reviewed",      
        "receive", "received", "receipt",
        "period", "testing period", "started", "from", "to ",
        "æ‰¿èª", "æ ¸å‡†", "æª¢é©—", "æ”¶ä»¶", "æ¥æ”¶", "æœ‰æ•ˆ", "expiry", "valid", "æœŸé–“", "å‘¨æœŸ", "æ™‚é–“"
    ]

    pat_ymd = r"(20\d{2})\s+(0?[1-9]|1[0-2])\s+(0?[1-9]|[12][0-9]|3[01])"
    pat_dmy = r"(0?[1-9]|[12][0-9]|3[01])\s+([a-zA-Z]{3,})\s+(20\d{2})"
    pat_mdy = r"([a-zA-Z]{3,})\s+(0?[1-9]|[12][0-9]|3[01])\s+(20\d{2})"

    for line in lines:
        line_lower = line.lower()
        score = 1
        if any(bad in line_lower for bad in poison_kw):
            score = -100 
        elif any(good in line_lower for good in bonus_kw):
            score = 100 

        clean_line = line.replace(".", " ").replace(",", " ").replace("-", " ").replace("/", " ")
        clean_line = clean_line.replace("å¹´", " ").replace("æœˆ", " ").replace("æ—¥", " ")
        clean_line = " ".join(clean_line.split())

        found_dt = None
        for pat in [pat_ymd, pat_dmy, pat_mdy]:
            matches = re.finditer(pat, clean_line, re.IGNORECASE)
            for match in matches:
                try:
                    groups = match.groups()
                    date_str = " ".join(groups)
                    for fmt in ["%Y %m %d", "%d %b %Y", "%d %B %Y", "%b %d %Y", "%B %d %Y"]:
                        try:
                            dt = datetime.strptime(date_str, fmt)
                            if 2000 <= dt.year <= 2030:
                                found_dt = dt
                                break
                        except: continue
                    if found_dt: break
                except: continue
            if found_dt: break
        
        if found_dt:
            candidates.append((score, found_dt))
            
    return candidates

def is_suspicious_limit_value(val):
    try:
        n = float(val)
        if n in [1000.0, 100.0, 50.0, 25.0, 10.0, 5.0, 2.0]: return True
        return False
    except: return False

def parse_value_priority(value_str):
    raw_val = clean_text(value_str)
    if "(" in raw_val and ")" in raw_val:
        if re.search(r"\(\d+\)", raw_val):
            raw_val = raw_val.split("(")[0].strip()
    val = raw_val.replace("mg/kg", "").replace("ppm", "").replace("%", "").replace("Âµg/cmÂ²", "").strip()
    
    if not val: return (0, 0, "")
    val_lower = val.lower()
    
    if val_lower in ["result", "limit", "mdl", "loq", "rl", "unit", "method", "004", "001", "no.1", "---", "-", "limits", "n.a.", "/"]: 
        return (0, 0, "")
    if re.search(r"\d+-\d+-\d+", val): return (0, 0, "") 
    
    num_only_match = re.search(r"^([\d\.]+)$", val)
    if num_only_match:
        if is_suspicious_limit_value(num_only_match.group(1)): return (0, 0, "")

    if "nd" in val_lower or "n.d." in val_lower or "<" in val_lower: return (1, 0, "N.D.")
    if "negative" in val_lower or "é™°æ€§" in val_lower: return (2, 0, "NEGATIVE")
    
    num_match = re.search(r"^([\d\.]+)(.*)$", val)
    if num_match:
        try:
            number = float(num_match.group(1))
            return (3, number, val)
        except: pass
    return (0, 0, val)

def check_pfas_in_summary(text):
    txt_lower = text.lower()
    for kw in PFAS_SUMMARY_KEYWORDS:
        if kw.lower() in txt_lower: return True
    return False

def identify_company(text):
    txt = text.lower()
    if "sgs" in txt: return "SGS"
    if "intertek" in txt: return "INTERTEK"
    if "cti" in txt or "centre testing" in txt: return "CTI"
    if "ctic" in txt: return "CTIC"
    return "OTHERS"

# --- 3. æ ¸å¿ƒï¼šè¡¨æ ¼è­˜åˆ¥ ---

def identify_columns_by_company(table, company):
    item_idx = -1
    result_idx = -1
    mdl_idx = -1
    limit_idx = -1
    
    max_scan_rows = min(3, len(table))
    full_header_text = ""
    for r in range(max_scan_rows):
        full_header_text += " ".join([str(c).lower() for c in table[r] if c]) + " "

    is_msds_table = False
    if any(k in full_header_text for k in MSDS_HEADER_KEYWORDS) and "result" not in full_header_text and "ç»“æœ" not in full_header_text:
        is_msds_table = True

    for r_idx in range(max_scan_rows):
        row = table[r_idx]
        for c_idx, cell in enumerate(row):
            txt = clean_text(cell).lower()
            if not txt: continue
            
            if "test item" in txt or "tested item" in txt or "æ¸¬è©¦é …ç›®" in txt or "æ£€æµ‹é¡¹ç›®" in txt:
                if item_idx == -1: item_idx = c_idx
            if "mdl" in txt or "loq" in txt:
                if mdl_idx == -1: mdl_idx = c_idx
            if "limit" in txt or "é™å€¼" in txt:
                if limit_idx == -1: limit_idx = c_idx
                
            is_bad_header = any(bad in txt for bad in MSDS_HEADER_KEYWORDS)
            if not is_bad_header:
                if company == "SGS":
                     if ("result" in txt or "çµæœ" in txt or "ç»“æœ" in txt or re.search(r"00[1-9]", txt) or 
                        re.search(r"^[a-z]?\s*-?\s*\d+$", txt) or "no." in txt):
                        if "cas" not in txt and "method" not in txt and "limit" not in txt:
                            if result_idx == -1: result_idx = c_idx
                else:
                    if ("result" in txt or "çµæœ" in txt or "ç»“æœ" in txt or re.search(r"00[1-9]", txt)):
                        if result_idx == -1: result_idx = c_idx
    
    if result_idx == -1 and company == "SGS":
        if mdl_idx != -1 and mdl_idx + 1 < len(table[0]):
            result_idx = mdl_idx + 1

    is_reference_table = False
    if is_msds_table: is_reference_table = True
    elif result_idx == -1:
        if "restricted substances" in full_header_text or "group name" in full_header_text or "substance name" in full_header_text:
            is_reference_table = True
        if company == "INTERTEK" and "limits" in full_header_text:
            is_reference_table = True
        if item_idx == -1:
            is_reference_table = True

    return item_idx, result_idx, is_reference_table

# --- 4. æ ¸å¿ƒï¼šæ–‡å­—æ¨¡å¼ ---

def parse_text_lines(text, data_pool, file_group_data, filename, company, targets=None):
    lines = text.split('\n')
    for line in lines:
        line_clean = clean_text(line)
        line_lower = line_clean.lower()
        if not line_clean: continue
        if any(bad in line_lower for bad in MSDS_HEADER_KEYWORDS): continue

        matched_simple = None
        for key, keywords in SIMPLE_KEYWORDS.items():
            if targets and key not in targets: continue
            
            # --- v60.5: äº‹å‰æƒæ¯’ (Pre-scan Block) ---
            # 1. Cd é˜²ç¦¦ (é‡å° HBCDD / ECD / PAHs)
            # v60.5 æ–°å¢: "indeno", "pyrene" (é˜² PAHs)
            if key == "Cd" and any(bad in line_lower for bad in ["hbcdd", "cyclododecane", "ecd", "indeno", "pyrene"]): 
                continue 
            
            # 2. F é˜²ç¦¦ (é‡å° å…¨æ°ŸåŒ–åˆç‰©)
            if key == "F" and any(bad in line_lower for bad in ["perfluoro", "polyfluoro", "pfos", "pfoa", "å…¨æ°Ÿ"]): 
                continue
            
            # 3. Br é˜²ç¦¦ (é‡å° PBB/PBDE/HBCDD)
            if key == "BR" and any(bad in line_lower for bad in ["polybromo", "hexabromo", "monobromo", "dibromo", "tribromo", "tetrabromo", "pentabromo", "heptabromo", "octabromo", "nonabromo", "decabromo", "multibromo", "pbb", "pbde", "å¤šæº´", "å…­æº´", "ä¸€æº´", "äºŒæº´", "ä¸‰æº´", "å››æº´", "äº”æº´", "ä¸ƒæº´", "å…«æº´", "ä¹æº´", "åæº´", "äºŒè‹¯é†š"]): 
                continue
            
            # 4. Pb é˜²ç¦¦ (é˜²æ­¢åƒæ‰ PBB)
            if key == "Pb" and any(bad in line_lower for bad in ["pbb", "pbde", "polybrominated", "å¤šæº´"]):
                continue

            for kw in keywords:
                if kw.lower() in line_lower and "test item" not in line_lower:
                    matched_simple = key
                    break
            if matched_simple: break
        
        matched_group = None
        # æ³¨æ„ï¼šå¦‚æœä¸æª¢æŸ¥ Pbï¼Œæˆ– Pb è¢«é˜²ç¦¦äº†ï¼Œç¨‹å¼æ‰æœƒèµ°åˆ°é€™è£¡æª¢æŸ¥ PBBï¼Œé€™æ¨£ PBB å°±ä¸æœƒè¢«åƒæ‰
        if not matched_simple:
            for group_key, keywords in GROUP_KEYWORDS.items():
                if targets and group_key not in targets: continue
                for kw in keywords:
                    if kw.lower() in line_lower:
                        matched_group = group_key
                        break
                if matched_group: break
        
        if matched_simple or matched_group:
            parts = line_clean.split()
            if len(parts) < 2: continue
            found_val = ""
            for part in reversed(parts):
                p_lower = part.lower()
                if p_lower in ["mg/kg", "ppm", "2", "5", "10", "50", "100", "1000", "0.1", "-", "---", "unit", "mdl"]: continue
                if "nd" in p_lower:
                    found_val = "N.D."
                    break
                if re.match(r"^\d+.*$", part): 
                    val_check = part.replace("â–²", "").replace("â–³", "")
                    try:
                        f = float(val_check)
                        if f not in [100.0, 1000.0, 50.0]:
                            found_val = part
                            break
                    except: pass
            
            if found_val:
                priority = parse_value_priority(found_val)
                if priority[0] == 0: continue
                if matched_simple:
                    data_pool[matched_simple].append({"priority": priority, "filename": filename})
                elif matched_group:
                    file_group_data[matched_group].append(priority)

# --- ä¸»ç¨‹å¼ ---

def process_files(files):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    all_file_valid_dates = [] 
    global_tracker = {"Pb": {"max_score": -1, "max_value": -1.0, "filename": ""}}
    
    progress_bar = st.progress(0)
    
    for i, file in enumerate(files):
        filename = file.name
        file_group_data = {key: [] for key in GROUP_KEYWORDS.keys()}
        
        try:
            with pdfplumber.open(file) as pdf:
                start_page_idx = find_report_start_page(pdf)
                company = "OTHERS"
                first_page_text = ""
                full_text_content = ""
                
                if len(pdf.pages) > start_page_idx:
                    first_page_text = pdf.pages[start_page_idx].extract_text() or ""
                    company = identify_company(first_page_text)
                    if check_pfas_in_summary(first_page_text):
                        data_pool["PFAS"].append({"priority": (4, 0, "REPORT"), "filename": filename})

                # 1. æ—¥æœŸæå–
                file_dates_candidates = []
                for p_idx in range(start_page_idx, len(pdf.pages)):
                    page = pdf.pages[p_idx]
                    page_txt = page.extract_text() or ""
                    full_text_content += page_txt + "\n"
                    if p_idx < start_page_idx + 5:
                        dates = extract_dates_v60_5(page_txt)
                        file_dates_candidates.extend(dates)
                
                if file_dates_candidates:
                    valid_candidates = [d for d in file_dates_candidates if d[0] > -50]
                    if valid_candidates:
                        best_date = sorted(valid_candidates, key=lambda x: (x[0], x[1]), reverse=True)[0]
                        all_file_valid_dates.append(best_date[1])

                # 2. å¼•æ“ A: è¡¨æ ¼æ¨¡å¼
                for p_idx in range(start_page_idx, len(pdf.pages)):
                    page = pdf.pages[p_idx]
                    tables = page.extract_tables()
                    for table in tables:
                        if not table or len(table) < 2: continue
                        item_idx, result_idx, is_skip_table = identify_columns_by_company(table, company)
                        if is_skip_table: continue 
                        
                        for row_idx, row in enumerate(table):
                            clean_row = [clean_text(cell) for cell in row]
                            row_txt = "".join(clean_row).lower()
                            if "test item" in row_txt or "result" in row_txt or "restricted" in row_txt: continue
                            if not any(clean_row): continue
                            
                            target_item_col = item_idx if item_idx != -1 else 0
                            if target_item_col >= len(clean_row): continue
                            item_name = clean_row[target_item_col]
                            item_name_lower = item_name.lower()
                            
                            if "pvc" in item_name_lower or "polyvinyl" in item_name_lower: continue

                            result = ""
                            if result_idx != -1 and result_idx < len(clean_row):
                                result = clean_row[result_idx]
                            
                            # v59.0 æ™ºæ…§è¡Œæƒæ
                            temp_priority = parse_value_priority(result)
                            if temp_priority[0] == 0: 
                                found_better = False
                                for cell in reversed(clean_row):
                                    c_lower = cell.lower()
                                    if not cell: continue
                                    if "nd" in c_lower or "n.d." in c_lower or "negative" in c_lower:
                                        result = cell
                                        found_better = True
                                        break
                                    if re.search(r"^\d+(\.\d+)?", cell):
                                        if is_suspicious_limit_value(cell): continue 
                                        result = cell
                                        found_better = True
                                        break
                                if not found_better and result_idx == -1: 
                                    pass 

                            priority = parse_value_priority(result)
                            if priority[0] == 0: continue 

                            # åŒ¹é…é‚è¼¯
                            for target_key, keywords in SIMPLE_KEYWORDS.items():
                                # --- v60.5: è¡¨æ ¼æ¨¡å¼æ¯’è—¥é˜²ç¦¦ ---
                                # 1. Cd é˜²ç¦¦ (æ–°å¢ PAHs é˜²ç¦¦)
                                if target_key == "Cd" and any(bad in item_name_lower for bad in ["hbcdd", "cyclododecane", "ecd", "indeno", "pyrene"]): 
                                    continue
                                # 2. F é˜²ç¦¦
                                if target_key == "F" and any(bad in item_name_lower for bad in ["perfluoro", "polyfluoro", "pfos", "pfoa", "å…¨æ°Ÿ"]): 
                                    continue
                                # 3. Br é˜²ç¦¦
                                if target_key == "BR" and any(bad in item_name_lower for bad in ["polybromo", "hexabromo", "monobromo", "dibromo", "tribromo", "tetrabromo", "pentabromo", "heptabromo", "octabromo", "nonabromo", "decabromo", "multibromo", "pbb", "pbde", "å¤šæº´", "å…­æº´", "ä¸€æº´", "äºŒæº´", "ä¸‰æº´", "å››æº´", "äº”æº´", "ä¸ƒæº´", "å…«æº´", "ä¹æº´", "åæº´", "äºŒè‹¯é†š"]): 
                                    continue
                                # 4. Pb é˜²ç¦¦ (é˜²æ­¢åƒæ‰ PBB)
                                if target_key == "Pb" and any(bad in item_name_lower for bad in ["pbb", "pbde", "polybrominated", "å¤šæº´"]):
                                    continue

                                for kw in keywords:
                                    if kw.lower() in item_name_lower:
                                        if target_key == "PFOS" and "related" in item_name_lower: continue 
                                        data_pool[target_key].append({"priority": priority, "filename": filename})
                                        if target_key == "Pb":
                                            score, val = priority[0], priority[1]
                                            if score > global_tracker["Pb"]["max_score"]:
                                                global_tracker["Pb"]["max_score"] = score
                                                global_tracker["Pb"]["max_value"] = val
                                                global_tracker["Pb"]["filename"] = filename
                                            elif score == global_tracker["Pb"]["max_score"] and val > global_tracker["Pb"]["max_value"]:
                                                global_tracker["Pb"]["max_value"] = val
                                                global_tracker["Pb"]["filename"] = filename
                                        break

                            for group_key, keywords in GROUP_KEYWORDS.items():
                                for kw in keywords:
                                    if kw.lower() in item_name_lower:
                                        file_group_data[group_key].append(priority)
                                        break
                
                # 3. å¼•æ“ B: æ–‡å­—æ¨¡å¼ (v59.0 æ“´å¤§æ•‘æ´)
                missing_targets = []
                pb_data = [d for d in data_pool["Pb"] if d['filename'] == filename]
                if not pb_data: missing_targets.append("Pb")
                
                halogen_data = []
                for h in ["F", "CL", "BR", "I"]:
                    halogen_data.extend([d for d in data_pool[h] if d['filename'] == filename])
                
                pfos_data = [d for d in data_pool["PFOS"] if d['filename'] == filename]
                
                trigger_rescue = False
                if company == "SGS":
                    if not pb_data: trigger_rescue = True
                    if ("halogen" in full_text_content.lower() or "å¤ç´ " in full_text_content) and not halogen_data:
                        trigger_rescue = True
                        missing_targets.extend(["F", "CL", "BR", "I"])
                    if "pfos" in full_text_content.lower() and not pfos_data:
                        trigger_rescue = True
                        missing_targets.append("PFOS")

                if trigger_rescue:
                     parse_text_lines(full_text_content, data_pool, file_group_data, filename, company, targets=None)
                     
                     for d in data_pool["Pb"]:
                         if d['filename'] == filename:
                             p = d['priority']
                             if p[0] > global_tracker["Pb"]["max_score"]:
                                 global_tracker["Pb"]["max_score"] = p[0]
                                 global_tracker["Pb"]["max_value"] = p[1]
                                 global_tracker["Pb"]["filename"] = filename
                             elif p[0] == global_tracker["Pb"]["max_score"] and p[1] > global_tracker["Pb"]["max_value"]:
                                 global_tracker["Pb"]["max_value"] = p[1]
                                 global_tracker["Pb"]["filename"] = filename

            # 4. çµç®—
            for group_key, values in file_group_data.items():
                if values:
                    best_in_file = sorted(values, key=lambda x: (x[0], x[1]), reverse=True)[0]
                    data_pool[group_key].append({"priority": best_in_file, "filename": filename})

        except Exception as e:
            st.warning(f"æª”æ¡ˆ {filename} è§£æç•°å¸¸: {e}")
        progress_bar.progress((i + 1) / len(files))

    # 5. èšåˆ
    final_row = {}
    for key in OUTPUT_COLUMNS:
        if key in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]: continue
        candidates = data_pool.get(key, [])
        if not candidates:
            final_row[key] = "" 
            continue
        best_record = sorted(candidates, key=lambda x: (x['priority'][0], x['priority'][1]), reverse=True)[0]
        final_row[key] = best_record['priority'][2]

    final_date_str = ""
    if all_file_valid_dates:
        latest_date = max(all_file_valid_dates)
        final_date_str = latest_date.strftime("%Y/%m/%d")
    
    final_row["æ—¥æœŸ"] = final_date_str
    
    if global_tracker["Pb"]["filename"]:
        final_row["æª”æ¡ˆåç¨±"] = global_tracker["Pb"]["filename"]
    else:
        final_row["æª”æ¡ˆåç¨±"] = files[0].name if files else ""

    return [final_row]

# --- ä»‹é¢ ---
st.set_page_config(page_title="SGS å ±å‘Šèšåˆå·¥å…· v60.5", layout="wide")
st.title("ğŸ“„ è¬ç”¨å‹æª¢æ¸¬å ±å‘Šèšåˆå·¥å…· (v60.5 PAHs/PBB ä¿®å¾©ç‰ˆ)")
st.info("ğŸ’¡ v60.5ï¼šè§£æ±º PAHs å°è‡´çš„ Cd èª¤åˆ¤ï¼Œä¸¦ä¿®å¾© Pb é—œéµå­—åƒæ‰ PBB çš„å•é¡Œã€‚")

uploaded_files = st.file_uploader("è«‹ä¸€æ¬¡é¸å–æ‰€æœ‰ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸ”„ é‡æ–°åŸ·è¡Œ"): st.rerun()

    try:
        result_data = process_files(uploaded_files)
        df = pd.DataFrame(result_data)
        for col in OUTPUT_COLUMNS:
            if col not in df.columns: df[col] = ""
        df = df[OUTPUT_COLUMNS]

        st.success("âœ… è™•ç†å®Œæˆï¼")
        st.dataframe(df)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Summary')
        
        st.download_button("ğŸ“¥ ä¸‹è¼‰ Excel", data=output.getvalue(), file_name="SGS_Summary_v60.5.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
        
    except Exception as e:
        st.error(f"ç³»çµ±éŒ¯èª¤: {e}")
