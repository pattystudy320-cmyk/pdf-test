import streamlit as st
import pdfplumber
import pandas as pd
import re
from dateutil import parser
import io

st.set_page_config(page_title="SGS Report Parser", layout="wide")
st.title("ğŸ“„ SGS Report æª¢æ¸¬çµæœå½™ç¸½å·¥å…· (è¡¨æ ¼å®šä½ç‰ˆ)")

# =========================
# 1. æ¬„ä½é—œéµå­—å®šç¾©
# =========================
# é€™è£¡çš„é—œéµå­—ç”¨ä¾†åŒ¹é…ã€Œè¡¨æ ¼ç¬¬ä¸€æ¬„ã€çš„å…§å®¹
ITEM_KEYWORDS = {
    "Pb": ["Lead", "Pb"],
    "Cd": ["Cadmium", "Cd"],
    "Hg": ["Mercury", "Hg"],
    "CrVI": ["Hexavalent", "Chromium", "CrVI"],
    "PBBs": ["Sum of PBBs", "PBBs"],
    "PBDEs": ["Sum of PBDEs", "PBDEs"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Benzyl butyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "F": ["Fluorine", "F"],
    "CL": ["Chlorine", "Cl"],
    "BR": ["Bromine", "Br"],
    "I": ["Iodine", "I"],
    "PFOS": ["PFOS"]
}

FINAL_COLUMNS = [
    "Pb", "Cd", "Hg", "CrVI", "PBBs", "PBDEs",
    "DEHP", "BBP", "DBP", "DIBP",
    "F", "CL", "BR", "I",
    "PFOS", "PFAS", "DATE"
]

# =========================
# 2. æ ¸å¿ƒåŠŸèƒ½ï¼šè¡¨æ ¼è®€å–
# =========================

def normalize_result(value):
    """
    æ¸…æ´—æŠ“åˆ°çš„çµæœï¼š
    1. çµ±ä¸€ N.D. æ ¼å¼
    2. ç§»é™¤å–®ä½æˆ–é›œè¨Š
    """
    if not value:
        return ""
    
    val_str = str(value).strip()
    
    # åˆ¤æ–· N.D. (åŒ…å« ND, N. D., Not Detected)
    if re.search(r"(\bN\s*\.?\s*D\s*\.?\b)|(Not\s*Detected)", val_str, re.IGNORECASE):
        return "N.D."
    
    if "NEGATIVE" in val_str.upper():
        return "NEGATIVE"

    # å˜—è©¦æŠ“å–æ•¸å­—
    # å…ˆç§»é™¤å–®ä½
    val_str = re.sub(r"mg/kg|ppm|%|wt%", "", val_str, flags=re.IGNORECASE)
    match = re.search(r"\d+(\.\d+)?", val_str)
    if match:
        return match.group(0)
    
    return ""

def extract_data_from_pdf(pdf_file):
    """
    æ··åˆç­–ç•¥ï¼š
    1. å„ªå…ˆå˜—è©¦ extract_tables (è¡¨æ ¼æ¨¡å¼) -> æº–ç¢ºåº¦æœ€é«˜ï¼Œä¸æœƒæŠ“åˆ° MDL
    2. è‹¥è¡¨æ ¼å¤±æ•—ï¼Œå¯å›é€€åˆ°æ–‡å­—æœå°‹ (é€™è£¡ç°¡åŒ–ï¼Œå°ˆæ³¨æ–¼è¡¨æ ¼)
    """
    extracted_data = {key: [] for key in ITEM_KEYWORDS} # å„²å­˜æ‰€æœ‰æŠ“åˆ°çš„æ•¸æ“š
    full_text = ""
    pages_text = []

    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            # 1. æ”¶é›†å…¨æ–‡ (ç”¨æ–¼æŠ“æ—¥æœŸå’Œ PFAS)
            text = page.extract_text() or ""
            pages_text.append(text)
            full_text += text + "\n"

            # 2. è¡¨æ ¼èƒå– (Table Extraction)
            # ä½¿ç”¨å¯¬é¬†è¨­å®šï¼Œé¿å…ç„¡æ¡†ç·šè¡¨æ ¼è®€ä¸åˆ°
            tables = page.extract_tables(table_settings={"vertical_strategy": "text", "horizontal_strategy": "text"})

            for table in tables:
                for row in table:
                    # éæ¿¾æ‰ç©ºè¡Œæˆ–æ¬„ä½éå°‘çš„è¡Œ
                    # é¦¬ä¾†è¥¿äºå ±å‘Šé€šå¸¸æœ‰ 4~6 æ¬„ (Item, Unit, Method, Result, MDL, Limit)
                    # Result é€šå¸¸åœ¨ Index 3 (ç¬¬4æ¬„)
                    if not row or len(row) < 4:
                        continue
                    
                    # æ¸…ç† row ä¸­çš„ None
                    row_clean = [str(cell).strip() if cell else "" for cell in row]
                    
                    first_col = row_clean[0] # æ¸¬é …åç¨±
                    target_col_idx = 3       # çµæœæ¬„ä½é€šå¸¸åœ¨ç¬¬ 4 æ¬„ (Index 3)

                    # æª¢æŸ¥é€™ä¸€è¡Œæ˜¯å¦æ˜¯æˆ‘å€‘è¦çš„æ¸¬é …
                    for item, keywords in ITEM_KEYWORDS.items():
                        # è¦å‰‡ï¼šé—œéµå­—å¿…é ˆå‡ºç¾åœ¨ç¬¬ä¸€æ¬„
                        # ä¾‹å¦‚ keywords=["Lead", "Pb"]ï¼Œåªè¦ç¬¬ä¸€æ¬„åŒ…å« "Lead" å°±ç®—å°æ‡‰åˆ° "Pb"
                        if all(k.lower() in first_col.lower() for k in keywords if len(k) > 1):
                            # ç‰¹åˆ¥è™•ç†: å–®ä¸€å­—æ¯ F, I å®¹æ˜“èª¤åˆ¤ï¼Œéœ€ç²¾ç¢ºæ¯”å°
                            if item in ["F", "I"] and len(first_col) < 20: 
                                if item not in first_col: 
                                    continue
                            
                            # æŠ“å–çµæœæ¬„ä½
                            raw_result = row_clean[target_col_idx]
                            
                            # å¦‚æœç¬¬ 4 æ¬„æ˜¯ç©ºçš„ï¼Œæœ‰å¯èƒ½æ˜¯ N.D. å¯«åœ¨ç¬¬ 3 æ¬„ (å¾ˆå°‘è¦‹)ï¼Œæˆ–æ˜¯åˆä½µå„²å­˜æ ¼
                            # ä½†é¦¬ä¾†è¥¿äºå ±å‘Šå¾ˆæ¨™æº–ï¼Œé€šå¸¸å°±åœ¨ Index 3
                            clean_res = normalize_result(raw_result)
                            if clean_res:
                                extracted_data[item].append(clean_res)

    return full_text, pages_text, extracted_data

def extract_pfas(text):
    return "REPORT" if re.search(r"\bPFAS\b", text, re.IGNORECASE) else ""

def extract_date(first_page_text):
    match = re.search(
        r"(REPORTED DATE|TEST REPORT REPORTED DATE)\s*[:\-]?\s*([^\n]+)",
        first_page_text,
        re.IGNORECASE
    )
    return match.group(2).strip() if match else ""

def normalize_date(date_text):
    if not date_text:
        return ""
    try:
        dt = parser.parse(date_text, dayfirst=True)
        return dt.strftime("%Y/%m/%d")
    except:
        return ""

def merge_results(values):
    """
    å½™ç¸½é‚è¼¯
    """
    nums = []
    has_nd = False
    has_neg = False

    for v in values:
        if not v: continue
        v_upper = str(v).upper()
        
        if "N.D." in v_upper:
            has_nd = True
        elif "NEGATIVE" in v_upper:
            has_neg = True
        else:
            try:
                nums.append(float(v))
            except:
                pass

    if nums:
        return str(max(nums))
    if has_neg:
        return "NEGATIVE"
    if has_nd:
        return "N.D."
    return ""

# =========================
# 3. Streamlit ä¸»ç¨‹å¼
# =========================

uploaded_files = st.file_uploader(
    "è«‹ä¸Šå‚³ SGS PDF Reportï¼ˆå»ºè­°é¦¬ä¾†è¥¿äºç‰ˆæœ¬ï¼‰",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    rows = []
    for file in uploaded_files:
        full_text, pages_text, extracted_data = extract_data_from_pdf(file)
        
        record = {}

        # å¡«å…¥æŠ“åˆ°çš„è¡¨æ ¼æ•¸æ“š
        for item in ITEM_KEYWORDS:
            # å¦‚æœæœ‰æŠ“åˆ°å¤šç­† (ä¾‹å¦‚ PBBs ç´°é …)ï¼Œé€²è¡Œ merge
            values = extracted_data.get(item, [])
            record[item] = merge_results(values)

        # å¡«å…¥æ—¥æœŸèˆ‡ PFAS
        record["PFAS"] = extract_pfas(full_text)
        raw_date = extract_date(pages_text[0]) if pages_text else ""
        record["DATE"] = normalize_date(raw_date)

        rows.append(record)

    df_all = pd.DataFrame(rows)

    # å½™ç¸½é¡¯ç¤º
    merged = {}
    if not df_all.empty:
        for col in FINAL_COLUMNS:
            if col in ["DATE", "PFAS"]:
                continue
            if col in df_all.columns:
                merged[col] = merge_results(df_all[col].tolist())
            else:
                merged[col] = ""

        merged["PFAS"] = "REPORT" if "REPORT" in df_all["PFAS"].tolist() else ""
        
        valid_dates = [d for d in df_all["DATE"] if d]
        merged["DATE"] = max(valid_dates) if valid_dates else ""

        df_final = pd.DataFrame([merged], columns=FINAL_COLUMNS)

        st.subheader("ğŸ“Š å½™ç¸½çµæœï¼ˆè¡¨æ ¼å®šä½ç‰ˆï¼‰")
        st.dataframe(df_final, use_container_width=True)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_final.to_excel(writer, sheet_name="SGS_Result", index=False)

        st.download_button(
            "â¬‡ï¸ ä¸‹è¼‰å…¬å¸åˆ¶å¼ Excel",
            output.getvalue(),
            file_name="SGS_Test_Result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.warning("æœªè®€å–åˆ°æœ‰æ•ˆè³‡æ–™ã€‚")
