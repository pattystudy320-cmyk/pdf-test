import streamlit as st
import pdfplumber
import pandas as pd
import re
import io
from dateutil import parser

# ==========================================
# 1. é—œéµå­—å®šç¾©
# ==========================================

TARGET_ITEMS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBBs", "PBDEs",
    "DEHP", "DBP", "BBP", "DIBP",
    "F", "Cl", "Br", "I",
    "PFOS", "PFAS", "DATE", "FILENAME"
]

# åŒ–å­¸ç‰©è³ªé—œéµå­—æ˜ å°„
KEYWORDS_MAP = {
    r"(?i)\b(Lead|Pb)\b": "Pb",
    r"(?i)\b(Cadmium|Cd)\b": "Cd",
    r"(?i)\b(Mercury|Hg)\b": "Hg",
    r"(?i)\b(Hexavalent Chromium|Cr\(?VI\)?|Cr6\+)\b": "Cr6+",
    r"(?i)\b(DEHP|Di\(2-ethylhexyl\)\s*phthalate)\b": "DEHP",
    r"(?i)\b(DBP|Dibutyl\s*phthalate)\b": "DBP",
    r"(?i)\b(BBP|Butyl\s*benzyl\s*phthalate)\b": "BBP",
    r"(?i)\b(DIBP|Diisobutyl\s*phthalate)\b": "DIBP",
    r"(?i)\b(Fluorine|F)\b": "F",
    r"(?i)\b(Chlorine|Cl)\b": "Cl",
    r"(?i)\b(Bromine|Br)\b": "Br",
    r"(?i)\b(Iodine|I)\b": "I",
    r"(?i)\b(PFOS|Perfluorooctane\s*sulfonates)\b": "PFOS"
}

PBB_SUBITEMS = r"(?i)(Monobromobiphenyl|Dibromobiphenyl|Tribromobiphenyl|Tetrabromobiphenyl|Pentabromobiphenyl|Hexabromobiphenyl|Heptabromobiphenyl|Octabromobiphenyl|Nonabromobiphenyl|Decabromobiphenyl)"
PBDE_SUBITEMS = r"(?i)(Monobromodiphenyl ether|Dibromodiphenyl ether|Tribromodiphenyl ether|Tetrabromodiphenyl ether|Pentabromodiphenyl ether|Hexabromodiphenyl ether|Heptabromodiphenyl ether|Octabromodiphenyl ether|Nonabromodiphenyl ether|Decabromodiphenyl ether)"

# ==========================================
# 2. æ—¥æœŸè™•ç†é‚è¼¯ (æ¨™æº–åŒ–ç‚º YYYY/MM/DD)
# ==========================================

def standardize_date(date_str):
    """å°‡å„ç¨®é›œäº‚æ ¼å¼çµ±ä¸€è½‰ç‚º YYYY/MM/DD"""
    if not date_str: return "Unknown"
    
    clean_str = str(date_str).strip()
    # è™•ç†ä¸­æ–‡ (2024å¹´10æœˆ10æ—¥ -> 2024/10/10)
    clean_str = clean_str.replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")
    # è™•ç†éŸ“æ–‡/ISO å¸¶é»æ ¼å¼ (2024. 10. 17. -> 2024/10/17)
    clean_str = clean_str.replace(".", "/").strip("/")
    
    try:
        dt = parser.parse(clean_str, fuzzy=True)
        return dt.strftime("%Y/%m/%d")
    except:
        return clean_str

def extract_report_date(text_page1):
    """Head-then-Tail ç­–ç•¥ + å¤šèªè¨€é—œéµå­—"""
    lines = text_page1.split('\n')
    BLACKLIST = [r"(?i)Receive", r"(?i)Period", r"(?i)Tested", r"(?i)Due", r"(?i)Sample"]
    LABEL_PATTERNS = [
        r"(?i)Report\s*Date", r"(?i)Issue\s*Date", r"æŠ¥å‘Šæ—¥æœŸ", r"æ—¥æœŸ", r"ë°œí–‰ì¼ì",
        r"(?i)^Date\s*[:ï¼š]", r"(?i)Date\s*[:ï¼š]"
    ]
    # æŠ“å–æ•¸å€¼æ ¼å¼ (202x-xx-xx, 202x/xx/xx, 202x.xx.xx, Feb 27 2025)
    DATE_VALUE_PATTERN = r"(20\d{2}[-./å¹´]\s?\d{1,2}[-./æœˆ]\s?\d{1,2}[æ—¥]?|[A-Za-z]{3}\s+\d{1,2},\s+20\d{2})"

    # ç­–ç•¥ 1: é é¦–æƒæ
    for line in lines[:25]:
        if any(re.search(b, line) for b in BLACKLIST): continue
        for label in LABEL_PATTERNS:
            if re.search(label, line):
                match = re.search(DATE_VALUE_PATTERN, line)
                if match: return standardize_date(match.group(0))
                # å‚™ç”¨: æŠ“å†’è™Ÿå¾Œçš„å…§å®¹
                parts = re.split(r"[:ï¼š]", line, 1)
                if len(parts) > 1 and len(parts[1].strip()) > 5:
                    return standardize_date(parts[1])

    # ç­–ç•¥ 2: é å°¾æƒæ (é‡å° CTI)
    for line in lines[max(0, len(lines)-20):]:
        if re.search(r"(?i)Date", line) or re.search(r"\d{4}", line):
             match = re.search(DATE_VALUE_PATTERN, line)
             if match:
                 if not any(re.search(b, line) for b in BLACKLIST):
                     return standardize_date(match.group(0))
    return "Unknown"

# ==========================================
# 3. æ ¸å¿ƒï¼šæ™ºæ…§æ¬„ä½å®šä½ (Header Mapping)
# ==========================================

def get_valid_column_indices(header_row):
    """
    åˆ†æè¡¨é ­ï¼Œå›å‚³ã€Œéé»‘åå–®ã€çš„æ¬„ä½ç´¢å¼•åˆ—è¡¨ (å³æ½›åœ¨çš„çµæœæ¬„ä½)
    """
    if not header_row: return []
    
    # é»‘åå–®é—œéµå­— (å¿½ç•¥é€™äº›æ¬„ä½)
    IGNORE_KEYWORDS = [
        r"(?i)Limit", r"(?i)é™å€¼", 
        r"(?i)MDL", r"(?i)Method\s*Detection", r"(?i)æ–¹æ³•æ£€å‡ºé™", 
        r"(?i)Unit", r"(?i)å•ä½", 
        r"(?i)Method", r"(?i)Test\s*Method",
        r"(?i)Item", r"(?i)Test\s*Item", r"(?i)æµ‹è¯•é¡¹ç›®" # é …ç›®åç¨±æ¬„ä½ä¹Ÿä¸å«æ•¸å€¼
    ]
    
    valid_indices = []
    for idx, cell in enumerate(header_row):
        cell_str = str(cell).strip()
        is_ignored = False
        for pattern in IGNORE_KEYWORDS:
            if re.search(pattern, cell_str):
                is_ignored = True
                break
        
        if not is_ignored:
            valid_indices.append(idx)
            
    return valid_indices

def clean_value(val_str):
    """æ¸…ç†æ•¸å€¼ï¼Œä¿ç•™ N.D."""
    if not val_str: return None
    val_str = str(val_str).strip()
    if re.search(r"(?i)(n\.?d\.?|not detected|<)", val_str): return "N.D."
    if re.search(r"(?i)negative", val_str): return "NEGATIVE"
    
    # å˜—è©¦æå–æ•¸å­—
    match = re.search(r"(\d+\.?\d*)", val_str)
    if match: return float(match.group(1))
    return None

def extract_value_from_row(row, valid_indices):
    """å¾ä¸€è¡Œä¸­æå–å”¯ä¸€çš„æœ‰æ•ˆçµæœ (æ•¸å­— æˆ– N.D.)"""
    candidates = []
    for idx in valid_indices:
        if idx < len(row):
            val = clean_value(str(row[idx]))
            if val is not None:
                candidates.append(val)
    
    if not candidates: return None
    
    # é‚è¼¯ï¼šåŒä¸€è¡Œæ‡‰è©²åªæœ‰ä¸€å€‹çµæœã€‚å¦‚æœæœ‰æ•¸å­—ï¼Œå„ªå…ˆå›å‚³æ•¸å­— (é˜²æ­¢ N.D. æ··å…¥)
    # å¦‚æœå…¨æ˜¯ N.D.ï¼Œå›å‚³ N.D.
    numbers = [c for c in candidates if isinstance(c, float)]
    if numbers:
        return numbers[0] # å‡è¨­åªæœ‰ä¸€å€‹æœ‰æ•ˆæ•¸å­—çµæœ
    return "N.D." # è‹¥ç„¡æ•¸å­—ï¼Œå‰‡å›å‚³ N.D.

# ==========================================
# 4. æª”æ¡ˆè™•ç†é‚è¼¯
# ==========================================

def process_single_file(uploaded_file):
    filename = uploaded_file.name
    result = {k: None for k in TARGET_ITEMS}
    result["FILENAME"] = filename
    
    pbb_sum = 0
    pbde_sum = 0
    pbb_found = False
    pbde_found = False
    
    try:
        with pdfplumber.open(uploaded_file) as pdf:
            full_text = ""
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if text: full_text += text + "\n"
                if i == 0: result["DATE"] = extract_report_date(text)

                tables = page.extract_tables()
                if tables:
                    for table in tables:
                        # 1. åˆ†æè¡¨é ­ (å‡è¨­ç¬¬ä¸€åˆ—æ˜¯è¡¨é ­)
                        if not table: continue
                        header_row = table[0]
                        valid_indices = get_valid_column_indices(header_row)
                        
                        # è‹¥æ‰¾ä¸åˆ°æœ‰æ•ˆæ¬„ä½ï¼Œå¯èƒ½è¡¨é ­è­˜åˆ¥å¤±æ•—ï¼Œå˜—è©¦å…¨è¡Œæƒæ (Fallback)
                        if not valid_indices: 
                            valid_indices = range(len(header_row)) 

                        # 2. éæ­·æ•¸æ“šè¡Œ
                        for row in table[1:]: # è·³éè¡¨é ­
                            row_str = " ".join([str(cell) for cell in row if cell]).replace("\n", " ")
                            
                            # ä¸€èˆ¬ç‰©è³ªæå–
                            for pattern, key in KEYWORDS_MAP.items():
                                if re.search(pattern, row_str):
                                    val = extract_value_from_row(row, valid_indices)
                                    if val is not None:
                                        # ç°¡å–®æ›´æ–°é‚è¼¯ (è‹¥æœ‰å€¼å‰‡è¦†è“‹)
                                        result[key] = val
                                    break

                            # PBBs åŠ ç¸½
                            if re.search(PBB_SUBITEMS, row_str):
                                pbb_found = True
                                val = extract_value_from_row(row, valid_indices)
                                if isinstance(val, float): pbb_sum += val
                            
                            # PBDEs åŠ ç¸½
                            if re.search(PBDE_SUBITEMS, row_str):
                                pbde_found = True
                                val = extract_value_from_row(row, valid_indices)
                                if isinstance(val, float): pbde_sum += val

            # PFAS
            if "PFAS" in full_text or "Per- and polyfluoroalkyl substances" in full_text:
                result["PFAS"] = "REPORT"
            else:
                result["PFAS"] = ""

            result["PBBs"] = pbb_sum if pbb_found and pbb_sum > 0 else "N.D."
            result["PBDEs"] = pbde_sum if pbde_found and pbde_sum > 0 else "N.D."

            return result

    except Exception as e:
        st.error(f"Error processing {filename}: {e}")
        return None

# ==========================================
# 5. Streamlit ä»‹é¢
# ==========================================

def main():
    st.set_page_config(page_title="RoHS å ±å‘Šå½™æ•´å·¥å…·", layout="wide")
    st.title("ğŸ“„ åŒ–å­¸æ¸¬è©¦å ±å‘Šè‡ªå‹•å½™æ•´å·¥å…·")
    st.markdown("""
    **ç‰ˆæœ¬ç‰¹é»ï¼š**
    1. **ç²¾æº–æŠ“å–**ï¼šè‡ªå‹•è­˜åˆ¥è¡¨é ­ï¼Œæ’é™¤é™å€¼(Limit)èˆ‡æª¢å‡ºé™(MDL)ï¼Œç²¾æº–é–å®šçµæœæ¬„ä½ã€‚
    2. **æ—¥æœŸæ¨™æº–åŒ–**ï¼šæ”¯æ´ä¸­/è‹±/éŸ“æ ¼å¼ï¼Œçµ±ä¸€è½‰ç‚º `YYYY/MM/DD`ã€‚
    3. **N.D.ä¿ç•™**ï¼šExcel è¼¸å‡ºä¿ç•™ "N.D." å­—æ¨£ã€‚
    """)

    uploaded_files = st.file_uploader("è«‹é¸æ“‡ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

    if uploaded_files:
        if st.button("é–‹å§‹åˆ†æ"):
            with st.spinner("æ­£åœ¨è®€å–ä¸¦åˆ†æ PDF..."):
                all_results = []
                progress_bar = st.progress(0)
                
                for i, file in enumerate(uploaded_files):
                    res = process_single_file(file)
                    if res: all_results.append(res)
                    progress_bar.progress((i + 1) / len(uploaded_files))
                
                if not all_results:
                    st.warning("æœªèƒ½æå–æœ‰æ•ˆæ•¸æ“šã€‚")
                    return

                df_detail = pd.DataFrame(all_results)
                # æ¬„ä½æ’åº
                cols = ["FILENAME", "DATE"] + [c for c in TARGET_ITEMS if c not in ["FILENAME", "DATE"]]
                df_detail = df_detail[cols]

                st.success("åˆ†æå®Œæˆï¼")
                st.dataframe(df_detail)

                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df_detail.to_excel(writer, index=False, sheet_name='Summary')
                output.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š",
                    data=output,
                    file_name=f"RoHS_Summary_{pd.Timestamp.now().strftime('%Y%m%d')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

if __name__ == "__main__":
    main()
