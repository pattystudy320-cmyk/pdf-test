import streamlit as st
import pdfplumber
import pandas as pd
import re
import io
import os
from dateutil import parser

# ==========================================
# 1. å…¨å±€é…ç½® (Global Settings)
# ==========================================

# æœ€çµ‚è¼¸å‡ºçš„æ¬„ä½é †åº
TARGET_ITEMS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBBs", "PBDEs",
    "DEHP", "DBP", "BBP", "DIBP",
    "F", "Cl", "Br", "I",
    "PFOS", "PFOA", "PFAS", "DATE", "FILENAME"
]

# åŒ–å­¸ç‰©è³ªé—œéµå­—æ˜ å°„ (Regex -> çµ±ä¸€æ¬„ä½å)
KEYWORDS_MAP = {
    r"(?i)\b(Lead|Pb)\b": "Pb",
    r"(?i)\b(Cadmium|Cd)\b": "Cd",
    r"(?i)\b(Mercury|Hg)\b": "Hg",
    r"(?i)\b(Hexavalent Chromium|Cr\(?VI\)?|Cr6\+)\b": "Cr6+",
    r"(?i)\b(DEHP|Di\(2-ethylhexyl\)\s*phthalate)\b": "DEHP",
    r"(?i)\b(DBP|Dibutyl\s*phthalate)\b": "DBP",
    r"(?i)\b(BBP|Butyl\s*benzyl\s*phthalate)\b": "BBP",
    r"(?i)\b(DIBP|Diisobutyl\s*phthalate)\b": "DIBP",
    r"(?i)\b(Fluorine|F)\b": "F",
    r"(?i)\b(Chlorine|Cl)\b": "Cl",
    r"(?i)\b(Bromine|Br)\b": "Br",
    r"(?i)\b(Iodine|I)\b": "I",
    r"(?i)\b(PFOS|Perfluorooctane\s*sulfonates)\b": "PFOS",
    r"(?i)\b(PFOA|Perfluorooctanoic\s*acid)\b": "PFOA"
}

# PBBs/PBDEs åŠ ç¸½ç”¨é—œéµå­—
PBB_SUBITEMS = r"(?i)(Monobromobiphenyl|Dibromobiphenyl|Tribromobiphenyl|Tetrabromobiphenyl|Pentabromobiphenyl|Hexabromobiphenyl|Heptabromobiphenyl|Octabromobiphenyl|Nonabromobiphenyl|Decabromobiphenyl)"
PBDE_SUBITEMS = r"(?i)(Monobromodiphenyl ether|Dibromodiphenyl ether|Tribromodiphenyl ether|Tetrabromodiphenyl ether|Pentabromodiphenyl ether|Hexabromodiphenyl ether|Heptabromodiphenyl ether|Octabromodiphenyl ether|Nonabromodiphenyl ether|Decabromodiphenyl ether)"

# ==========================================
# 2. é€šç”¨å·¥å…·å‡½æ•¸ (Helper Functions)
# ==========================================

def standardize_date(date_str):
    """
    æ¨™æº–åŒ–æ—¥æœŸæ ¼å¼ç‚º YYYY/MM/DD
    æ”¯æ´ï¼š2024. 10. 17. (éŸ“ç³»), 2024å¹´10æœˆ10æ—¥ (ä¸­ç³»), Jan 08, 2025 (è‹±ç³»)
    """
    if not date_str: return "1900/01/01"
    clean_str = str(date_str).strip()
    
    # è™•ç†ä¸­æ–‡èˆ‡ç‰¹æ®Šç¬¦è™Ÿ
    clean_str = clean_str.replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")
    # è™•ç†é»åˆ†éš” (2024. 10. 17.) -> 2024/10/17
    clean_str = re.sub(r"(\d{4})[\.\s]+(\d{1,2})[\.\s]+(\d{1,2})\.?", r"\1/\2/\3", clean_str)
    
    try:
        dt = parser.parse(clean_str, fuzzy=True)
        return dt.strftime("%Y/%m/%d")
    except:
        return "1900/01/01" # è§£æå¤±æ•—å›å‚³é è¨­èˆŠæ—¥æœŸ

def clean_value(val_str):
    """
    æ•¸æ“šæ¸…æ´—ï¼š
    - è½‰ç‚º N.D. / NEGATIVE / POSITIVE
    - æå–ç´”æ•¸å­— (Float)
    - éæ¿¾æ‰ CAS No (å¦‚ 123-45-6) èˆ‡é•·æ–‡å­—æè¿°
    """
    if not val_str: return None
    val_str = str(val_str).strip()
    
    # æ’é™¤ CAS No. (æ ¼å¼: æ•¸å­—-æ•¸å­—-æ•¸å­—)
    if re.search(r"\b\d{2,}-\d{2,}-\d{2,}\b", val_str): 
        return None
    
    # æ’é™¤éé•·çš„éçµæœæè¿° (ä¾‹å¦‚æ¸¬è©¦æ–¹æ³•åç¨±)
    if len(val_str) > 20 and not re.search(r"(negative|positive|n\.d\.)", val_str, re.I):
        return None

    # æ¨™æº–åŒ–å®šæ€§çµæœ
    if re.search(r"(?i)(n\.?d\.?|not detected|<)", val_str): return "N.D."
    if re.search(r"(?i)(negative|é˜´æ€§|é™°æ€§)", val_str): return "NEGATIVE"
    if re.search(r"(?i)(positive|é˜³æ€§|é™½æ€§)", val_str): return "POSITIVE"
    
    # æå–æ•¸å­—
    match = re.search(r"(\d+\.?\d*)", val_str)
    if match: 
        return float(match.group(1))
    
    return None

def get_value_priority(val):
    """
    æ•¸å€¼å„ªå…ˆç´š (ç”¨æ–¼ Worst-case æ¯”è¼ƒ)ï¼š
    Level 3: å¯¦æ¸¬æ•¸å­— (è¶Šå¤§è¶Šå„ªå…ˆ)
    Level 2: å®šæ€§çµæœ (NEGATIVE/POSITIVE)
    Level 1: N.D.
    Level 0: None (æœªæª¢æ¸¬)
    """
    if isinstance(val, (int, float)): return (3, val)
    if val in ["NEGATIVE", "POSITIVE"]: return (2, 0)
    if val == "N.D.": return (1, 0)
    return (0, 0)

# ==========================================
# 3. å» å•†å°ˆå±¬è§£ææ¨¡çµ„ (Dictionary Logic)
# ==========================================

# --- [SGS Parser] ç‰¹å¾µæ¬Šé‡è¨ˆåˆ†æ³• ---
def parse_sgs(pdf_obj, full_text, first_page_text):
    result = {k: None for k in KEYWORDS_MAP.values()}
    result['PFAS'] = ""
    result['DATE'] = ""

    # 1. æ—¥æœŸæŠ“å– (SGS é é¦–å„ªå…ˆ)
    date_patterns = [r"(?i)Date\s*[:ï¼š]", r"æ—¥æœŸ\s*[:ï¼š]", r"æ—¥æœŸ\(Date\)\s*[:ï¼š]"]
    for line in first_page_text.split('\n')[:25]:
        for pat in date_patterns:
            if re.search(pat, line):
                # æŠ“å– YYYY/MM/DD æˆ– Mon DD, YYYY
                match = re.search(r"(20\d{2}[-./å¹´]\s?\d{1,2}[-./æœˆ]\s?\d{1,2}|[A-Za-z]{3}\s+\d{1,2}[,\s]+\d{4})", line)
                if match: result['DATE'] = standardize_date(match.group(0))
                break
        if result['DATE']: break

    # 2. è¡¨æ ¼æ•¸æ“š (æ¬Šé‡è¨ˆåˆ†æ³•)
    pbb_sum = 0; pbde_sum = 0; pbb_found = False; pbde_found = False
    
    with pdfplumber.open(pdf_obj) as pdf:
        for page in pdf.pages:
            tables = page.extract_tables()
            for table in tables:
                if not table: continue
                header = table[0]
                
                # A. æ¬„ä½è¨ˆåˆ† (Scoring)
                col_scores = {}
                for idx, col in enumerate(header):
                    col_str = str(col).strip()
                    score = 0
                    
                    # æ‰£åˆ†é … (Blacklist)
                    if re.search(r"(?i)(Limit|é™å€¼|MDL|Method|æ–¹æ³•|Unit|å•ä½|å–®ä½|CAS|Item|é¡¹ç›®)", col_str):
                        score -= 1000
                    
                    # åŠ åˆ†é … (Header Whitelist)
                    if re.search(r"(?i)(Result|ç»“æœ|No\.|A\d+)", col_str):
                        score += 50
                    
                    # å·çœ‹æ•¸æ“šç‰¹å¾µ (Data Peeking)
                    if len(table) > 1:
                        sample_val = str(table[1][idx]).strip()
                        if re.search(r"(?i)(N\.?D|Negative|<)", sample_val): # çµæœæ¬„ç‰¹å¾µ
                            score += 100
                        elif re.search(r"\d+-\d+-\d+", sample_val): # CAS No ç‰¹å¾µ
                            score -= 1000
                        elif re.search(r"^\d+$", sample_val) and not re.search(r"N\.?D", sample_val, re.I): 
                            # ç´”æ•¸å­—ä¸”ç„¡ N.D. (å¯èƒ½æ˜¯ Limit æˆ– MDL)
                            score -= 50 
                    
                    col_scores[idx] = score

                # é¸å‡ºåˆ†æ•¸æœ€é«˜çš„æ¬„ä½
                if not col_scores: continue
                best_col_idx = max(col_scores, key=col_scores.get)
                
                # è‹¥æœ€é«˜åˆ†ä»æ˜¯è² çš„ï¼Œä»£è¡¨é€™å¼µè¡¨å¯èƒ½æ²’çµæœï¼Œè·³é
                if col_scores[best_col_idx] < 0: continue

                # B. æ•¸æ“šæå–
                for row in table[1:]:
                    if len(row) <= best_col_idx: continue
                    row_str = " ".join([str(c) for c in row if c]).replace("\n", " ")
                    val = clean_value(row[best_col_idx])
                    
                    # PFAS åš´æ ¼æ¨¡å¼ (è¡¨æ ¼å…§å®¹æƒæ)
                    if "PFAS" in row_str and not result['PFAS']:
                        result['PFAS'] = "REPORT"

                    for pat, key in KEYWORDS_MAP.items():
                        if re.search(pat, row_str):
                            # æ›´æ–°é‚è¼¯: è‹¥æ–°å€¼æ˜¯æ•¸å­—ï¼Œæˆ–åŸå€¼æ˜¯ç©º/NDï¼Œå‰‡æ›´æ–°
                            if val is not None:
                                current_val = result[key]
                                if current_val is None or current_val == "N.D.":
                                    result[key] = val
                                elif isinstance(val, (int, float)) and isinstance(current_val, (int, float)):
                                    result[key] = max(val, current_val)
                            break
                    
                    if re.search(PBB_SUBITEMS, row_str):
                        pbb_found = True
                        if isinstance(val, (int, float)): pbb_sum += val
                    if re.search(PBDE_SUBITEMS, row_str):
                        pbde_found = True
                        if isinstance(val, (int, float)): pbde_sum += val

    # C. PFAS é¦–é æƒæ
    if "PFAS" in first_page_text or "Per- and polyfluoroalkyl" in first_page_text:
        result["PFAS"] = "REPORT"
        
    result["PBBs"] = pbb_sum if pbb_found and pbb_sum > 0 else "N.D."
    result["PBDEs"] = pbde_sum if pbde_found and pbde_sum > 0 else "N.D."
    return result

# --- [CTI Parser] é›™é‡é–å®š + é å°¾æ—¥æœŸ ---
def parse_cti(pdf_obj, full_text, first_page_text):
    result = {k: None for k in KEYWORDS_MAP.values()}
    result['PFAS'] = ""
    result['DATE'] = ""
    
    # 1. æ—¥æœŸæŠ“å– (å„ªå…ˆæƒæé å°¾ï¼Œé¿é–‹ Received Date)
    lines = first_page_text.split('\n')
    # å°‹æ‰¾ "Date:" ä¸”è©²è¡Œä¸å« "Received"
    date_pat = r"(?i)Date\s*[:ï¼š]?\s*([A-Za-z]{3}\.?\s*\d{1,2},?\s*\d{4}|\d{4}[./]\d{1,2}[./]\d{1,2})"
    
    # å¾æœ€å¾Œä¸€è¡Œå¾€ä¸Šæƒæ (Footer)
    for line in reversed(lines):
        if re.search(r"(?i)Received", line): continue # é¿é–‹é€æ¨£æ—¥
        match = re.search(date_pat, line)
        if match:
            result['DATE'] = standardize_date(match.group(1))
            break
            
    # 2. è¡¨æ ¼æ•¸æ“š (ç« ç¯€é–å®š + è¡¨é ­é©—è­‰)
    pbb_sum = 0; pbde_sum = 0; pbb_found = False; pbde_found = False
    result_section_started = False # ç« ç¯€é–‹é—œ

    with pdfplumber.open(pdf_obj) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text() or ""
            
            # A. ç« ç¯€é–å®šï¼šçœ‹åˆ° "Test Result" æ‰é–‹å•Ÿ
            if re.search(r"(?i)(Test Result|æ£€æµ‹ç»“æœ|æª¢æ¸¬çµæœ)", page_text):
                result_section_started = True
            
            if not result_section_started: continue 

            tables = page.extract_tables()
            for table in tables:
                if not table: continue
                header = table[0]
                header_str = " ".join([str(c) for c in header if c])

                # B. è¡¨é ­é©—è­‰ï¼šå¿…é ˆåŒ…å« MDL/Limit/LOQï¼Œæ’é™¤ Method è¡¨
                if not re.search(r"(?i)(MDL|Limit|RL|LOQ|Method\s*Detection)", header_str):
                    continue

                # C. å®šä½ Result æ¬„ä½ (éŒ¨é»æ³•)
                res_idx = -1
                for i, col in enumerate(header):
                    if col and re.search(r"(?i)(Result|ç»“æœ)", str(col)):
                        res_idx = i
                        break
                if res_idx == -1: # æ‰¾ä¸åˆ° Resultï¼Œæ‰¾ MDL å·¦é‚Š
                    for i, col in enumerate(header):
                        if col and re.search(r"(?i)(MDL|LOQ)", str(col)):
                            res_idx = max(0, i - 1)
                            break
                if res_idx == -1: res_idx = 1 # Fallback

                for row in table[1:]:
                    if len(row) <= res_idx: continue
                    row_str = " ".join([str(c) for c in row if c]).replace("\n", " ")
                    val = clean_value(row[res_idx])
                    
                    if "PFAS" in row_str and not result['PFAS']:
                        result['PFAS'] = "REPORT"

                    for pat, key in KEYWORDS_MAP.items():
                        if re.search(pat, row_str):
                            if val is not None:
                                current_val = result[key]
                                if current_val is None or current_val == "N.D.":
                                    result[key] = val
                                elif isinstance(val, (int, float)) and isinstance(current_val, (int, float)):
                                    result[key] = max(val, current_val)
                            break

                    if re.search(PBB_SUBITEMS, row_str):
                        pbb_found = True
                        if isinstance(val, (int, float)): pbb_sum += val
                    if re.search(PBDE_SUBITEMS, row_str):
                        pbde_found = True
                        if isinstance(val, (int, float)): pbde_sum += val

    if "PFAS" in first_page_text: result["PFAS"] = "REPORT"
    result["PBBs"] = pbb_sum if pbb_found and pbb_sum > 0 else "N.D."
    result["PBDEs"] = pbde_sum if pbde_found and pbde_sum > 0 else "N.D."
    return result

# --- [Intertek Parser] N.D. å°èˆª + éŸ“æ–‡æ”¯æ´ ---
def parse_intertek(pdf_obj, full_text, first_page_text):
    result = {k: None for k in KEYWORDS_MAP.values()}
    result['PFAS'] = ""
    result['DATE'] = ""

    # 1. æ—¥æœŸæŠ“å– (å«éŸ“æ–‡æ”¯æ´)
    lines = first_page_text.split('\n')
    # æ”¯æ´: Date, Issue Date, ë°œí–‰ì¼ì(ç™¼è¡Œæ—¥)
    date_pat = r"(?i)(?:Date|Issue Date|ë°œí–‰ì¼ì)\s*[:ï¼š]?\s*([A-Za-z]{3}\s+\d{1,2},?\s*\d{4}|\d{4}[.\s]+\d{1,2}[.\s]+\d{1,2})"
    for line in lines[:25]:
        match = re.search(date_pat, line)
        if match:
            result['DATE'] = standardize_date(match.group(1))
            break
            
    # 2. è¡¨æ ¼æ•¸æ“š (MDL éŒ¨é» + N.D. å°èˆª)
    pbb_sum = 0; pbde_sum = 0; pbb_found = False; pbde_found = False
    
    with pdfplumber.open(pdf_obj) as pdf:
        for page in pdf.pages:
            tables = page.extract_tables()
            for table in tables:
                if not table: continue
                header = table[0]
                
                # A. æ‰¾ MDL éŒ¨é»
                mdl_idx = -1
                for i, col in enumerate(header):
                    if col and re.search(r"(?i)(MDL|RL|Limit of Detection|ê²€ì¶œí•œê³„)", str(col)):
                        mdl_idx = i
                        break
                if mdl_idx == -1: continue 
                
                # B. N.D. å°èˆª (å·çœ‹æ•¸æ“šæ±ºå®š Result åœ¨å·¦é‚Šé‚„æ˜¯å³é‚Š)
                res_idx = -1
                if len(table) > 1:
                    row1 = table[1]
                    left_val = str(row1[mdl_idx-1]) if mdl_idx > 0 else ""
                    right_val = str(row1[mdl_idx+1]) if mdl_idx + 1 < len(row1) else ""
                    
                    # èª°æœ‰ N.D./Negative èª°å°±æ˜¯çµæœ
                    if re.search(r"(?i)(N\.?D|Negative|<)", left_val):
                        res_idx = mdl_idx - 1
                    elif re.search(r"(?i)(N\.?D|Negative|<)", right_val):
                        res_idx = mdl_idx + 1
                    # å¦å‰‡çœ‹è¡¨é ­
                    elif mdl_idx + 1 < len(header) and re.search(r"(?i)(Result|ê²°ê³¼)", str(header[mdl_idx+1])):
                        res_idx = mdl_idx + 1
                    elif mdl_idx - 1 >= 0 and re.search(r"(?i)(Result|ç»“æœ)", str(header[mdl_idx-1])):
                        res_idx = mdl_idx - 1
                
                if res_idx == -1: continue

                # C. æ•¸æ“šæå–
                for row in table[1:]:
                    if len(row) <= res_idx: continue
                    row_str = " ".join([str(c) for c in row if c]).replace("\n", " ")
                    val = clean_value(row[res_idx])
                    
                    if "PFAS" in row_str and not result['PFAS']:
                        result['PFAS'] = "REPORT"

                    for pat, key in KEYWORDS_MAP.items():
                        if re.search(pat, row_str):
                            if val is not None:
                                current_val = result[key]
                                if current_val is None or current_val == "N.D.":
                                    result[key] = val
                                elif isinstance(val, (int, float)) and isinstance(current_val, (int, float)):
                                    result[key] = max(val, current_val)
                            break

                    if re.search(PBB_SUBITEMS, row_str):
                        pbb_found = True
                        if isinstance(val, (int, float)): pbb_sum += val
                    if re.search(PBDE_SUBITEMS, row_str):
                        pbde_found = True
                        if isinstance(val, (int, float)): pbde_sum += val

    if "PFAS" in first_page_text: result["PFAS"] = "REPORT"
    result["PBBs"] = pbb_sum if pbb_found and pbb_sum > 0 else "N.D."
    result["PBDEs"] = pbde_sum if pbde_found and pbde_sum > 0 else "N.D."
    return result

# ==========================================
# 4. ä¸»æ§é‚è¼¯ (Dispatcher & Aggregation)
# ==========================================

def identify_vendor(first_page_text):
    text = first_page_text.lower()
    if "intertek" in text: return "INTERTEK"
    if "cti" in text or "åæµ‹" in text: return "CTI"
    if "sgs" in text: return "SGS"
    return "UNKNOWN"

def aggregate_reports(valid_results):
    """
    åŒæ–™è™Ÿèšåˆé‚è¼¯ï¼š
    1. æª”åï¼šPb æœ€é«˜çš„æª”æ¡ˆ (è‹¥ N.D. å‰‡å–æ—¥æœŸæœ€æ–°çš„)
    2. æ—¥æœŸï¼šæ‰€æœ‰å ±å‘Šä¸­æœ€æ–°çš„æ—¥æœŸ
    3. æ•¸å€¼ï¼šå–æœ€å¤§å€¼ (Worst-case)
    """
    if not valid_results: return pd.DataFrame()

    final_row = {k: None for k in TARGET_ITEMS}
    
    # 1. æ±ºå®šä»£è¡¨æª”å
    sorted_by_pb = sorted(
        valid_results, 
        key=lambda x: (
            get_value_priority(x.get("Pb"))[0], # å„ªå…ˆç´š (æ•¸å­— > N.D.)
            get_value_priority(x.get("Pb"))[1], # æ•¸å€¼å¤§å°
            x.get("DATE", "1900/01/01")         # æ—¥æœŸ
        ), 
        reverse=True
    )
    final_row["FILENAME"] = sorted_by_pb[0]["FILENAME"]

    # 2. æ±ºå®šæœ€æ–°æ—¥æœŸ
    all_dates = [r.get("DATE", "1900/01/01") for r in valid_results if r.get("DATE")]
    final_row["DATE"] = max(all_dates) if all_dates else "Unknown"

    # 3. æ±ºå®šå„æ•¸å€¼ (æœ€å·®æƒ…å¢ƒ)
    for key in TARGET_ITEMS:
        if key in ["FILENAME", "DATE"]: continue
        
        best_val = None
        for res in valid_results:
            val = res.get(key)
            if get_value_priority(val) > get_value_priority(best_val):
                best_val = val
        
        final_row[key] = best_val

    return pd.DataFrame([final_row])

# ==========================================
# 5. Streamlit UI
# ==========================================

def main():
    st.set_page_config(page_title="åŒ–å­¸å ±å‘Šè‡ªå‹•å½™æ•´ç³»çµ± v3.0", layout="wide")
    st.title("ğŸ§ª åŒ–å­¸æ¸¬è©¦å ±å‘Šè‡ªå‹•å½™æ•´ç³»çµ± v3.0")
    st.markdown("""
    **åŠŸèƒ½ç‰¹é»ï¼š**
    * **å» å•†æ”¯æ´**ï¼šSGS (æ¬Šé‡è¨ˆåˆ†)ã€CTI (é å°¾æ—¥æœŸ)ã€Intertek (éŸ“æ–‡/N.D.å°èˆª)ã€‚
    * **PFAS åš´æ ¼åˆ¤æ–·**ï¼šåƒ…ç•¶ "PFAS" é—œéµå­—å‡ºç¾æ™‚æ¨™è¨˜ REPORTã€‚
    * **èšåˆé‚è¼¯**ï¼šå¤šä»½å ±å‘Šè‡ªå‹•åˆä½µï¼Œå–æœ€åš´æ ¼æ•¸å€¼ï¼Œé¡¯ç¤º Pb æœ€é«˜è€…æª”åã€‚
    """)

    uploaded_files = st.file_uploader("è«‹ä¸Šå‚³ PDF å ±å‘Š (å¯å¤šé¸)", type="pdf", accept_multiple_files=True)

    if uploaded_files:
        if st.button("é–‹å§‹åˆ†æ"):
            valid_results = []
            bucket_unknown = []
            bucket_error = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, file in enumerate(uploaded_files):
                status_text.text(f"æ­£åœ¨è™•ç†: {file.name}...")
                try:
                    with pdfplumber.open(file) as pdf:
                        if len(pdf.pages) == 0:
                            bucket_error.append(file.name)
                            continue
                        
                        first_page_text = pdf.pages[0].extract_text()
                        if not first_page_text:
                            bucket_error.append(f"{file.name} (ç„¡æ³•è®€å–æ–‡å­—/åœ–ç‰‡æª”)")
                            continue
                        
                        full_text = ""
                        for page in pdf.pages:
                            txt = page.extract_text()
                            if txt: full_text += txt + "\n"

                    vendor = identify_vendor(first_page_text)
                    
                    data = None
                    if vendor == "SGS":
                        data = parse_sgs(file, full_text, first_page_text)
                    elif vendor == "CTI":
                        data = parse_cti(file, full_text, first_page_text)
                    elif vendor == "INTERTEK":
                        data = parse_intertek(file, full_text, first_page_text)
                    else:
                        bucket_unknown.append(file.name)
                        continue

                    if data:
                        data["FILENAME"] = file.name
                        valid_results.append(data)
                    else:
                        bucket_error.append(f"{file.name} (è§£æå¤±æ•—)")

                except Exception as e:
                    bucket_error.append(f"{file.name} (éŒ¯èª¤: {str(e)})")
                
                progress_bar.progress((i + 1) / len(uploaded_files))

            status_text.text("åˆ†æå®Œæˆï¼")

            # --- é¡¯ç¤ºçµæœ ---
            if valid_results:
                df_final = aggregate_reports(valid_results)
                
                # æ¬„ä½é‡æ–°æ’åº
                cols = ["FILENAME", "DATE"] + [c for c in TARGET_ITEMS if c not in ["FILENAME", "DATE"]]
                df_final = df_final[cols]
                
                st.success(f"âœ… æˆåŠŸè™•ç† {len(valid_results)} ä»½å ±å‘Šï¼Œå·²åˆä½µç‚º 1 ç­†çµæœï¼š")
                st.dataframe(df_final)
                
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    df_final.to_excel(writer, index=False, sheet_name='Summary')
                output.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è¼‰ Excel å ±å‘Š",
                    data=output,
                    file_name=f"Merged_Report_{pd.Timestamp.now().strftime('%Y%m%d')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            else:
                st.warning("æœªæå–åˆ°æœ‰æ•ˆæ•¸æ“šã€‚")

            # --- é¡¯ç¤ºç•°å¸¸ ---
            if bucket_unknown or bucket_error:
                st.divider()
                st.subheader("âš ï¸ ç•°å¸¸å ±å‘Šæ¸…å–®")
                col1, col2 = st.columns(2)
                
                with col1:
                    if bucket_unknown:
                        st.warning(f"ğŸŸ¡ æœªè­˜åˆ¥å» å•† ({len(bucket_unknown)})")
                        for name in bucket_unknown: st.write(f"- {name}")
                
                with col2:
                    if bucket_error:
                        st.error(f"ğŸ”´ è™•ç†å¤±æ•—/åœ–ç‰‡æª” ({len(bucket_error)})")
                        for name in bucket_error: st.write(f"- {name}")

if __name__ == "__main__":
    main()
