import streamlit as st
import pdfplumber
import pandas as pd
import re
from dateutil import parser
import io

# è¨­å®šé é¢è³‡è¨Š
st.set_page_config(page_title="SGS Report Parser", layout="wide")
st.title("ğŸ“„ SGS Report æª¢æ¸¬çµæœå½™ç¸½å·¥å…· (é¦¬ä¾†è¥¿äºä¿®æ­£ç‰ˆ)")

# =========================
# 1. æ¬„ä½å®šç¾©è¦å‰‡
# =========================
ITEM_RULES = {
    "Pb": r"Lead\s*\(Pb\)",
    "Cd": r"Cadmium\s*\(Cd\)",
    "Hg": r"Mercury\s*\(Hg\)",
    "CrVI": r"Hexavalent Chromium",
    "PBBs": r"Sum of PBBs",
    "PBDEs": r"Sum of PBDEs",
    "DEHP": r"DEHP",
    "BBP": r"BBP",
    "DBP": r"DBP",
    "DIBP": r"DIBP",
    "F": r"Fluorine",
    "CL": r"Chlorine",
    "BR": r"Bromine",
    "I": r"Iodine",
    "PFOS": r"PFOS"
}

FINAL_COLUMNS = [
    "Pb", "Cd", "Hg", "CrVI", "PBBs", "PBDEs",
    "DEHP", "BBP", "DBP", "DIBP",
    "F", "CL", "BR", "I",
    "PFOS", "PFAS", "DATE"
]

# =========================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼
# =========================

def extract_text_and_pages(pdf_file):
    """è®€å– PDF æ–‡å­—å…§å®¹"""
    full_text = ""
    pages_text = []
    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            pages_text.append(text)
            full_text += text + "\n"
    return full_text, pages_text

def extract_result(text, keyword):
    """
    ä¿®æ­£ç‰ˆ V4 æ ¸å¿ƒé‚è¼¯ï¼š
    1. å¼·åŠ›æ¸…æ´—ï¼šåœ¨è®€å–æ•¸æ“šå‰ï¼Œå¼·åˆ¶åˆªé™¤ Max, MDL, CAS, Year ç­‰å¹²æ“¾é …ã€‚
    2. N.D. å„ªå…ˆï¼šåªè¦åµæ¸¬åˆ° N.D. è®Šé«”ï¼Œç›´æ¥å›å‚³æ¨™æº– "N.D."ï¼Œä¸çœ‹å¾ŒçºŒæ•¸å­—ã€‚
    3. æŠ“å–æ•¸å€¼ï¼šåªåœ¨æ²’æœ‰ N.D. æ™‚æ‰æŠ“å–å‰©é¤˜çš„ç¬¬ä¸€å€‹æ•¸å­—ã€‚
    """
    lines = text.splitlines()

    for i, line in enumerate(lines):
        # æ­¥é©Ÿ A: é–å®šé—œéµå­—æ‰€åœ¨çš„è¡Œ
        if re.search(keyword, line, re.IGNORECASE):
            # æŠ“å–ä¸Šä¸‹æ–‡ (ç•¶è¡Œ + ä¸‹ä¸€è¡Œ)ï¼Œç¸®å°ç¯„åœé¿å…æŠ“åˆ°éš”å£æ¬„ä½
            context = " ".join(lines[i:i+2])

            # ==========================================
            # æ­¥é©Ÿ B: æ‰‹è¡“å®¤ - å¼·åŠ›åˆ‡é™¤å¹²æ“¾æº (é †åºå¾ˆé‡è¦)
            # ==========================================
            
            # 1. åˆ‡é™¤å–®ä½ (mg/kg, ppm, %, wt%)
            context = re.sub(r"mg/kg|ppm|%|wt%", " ", context, flags=re.IGNORECASE)

            # 2. åˆ‡é™¤ CAS No. (ä¾‹å¦‚ "(CAS No. 84-74-2)" -> åˆªé™¤æ•´å€‹æ‹¬è™Ÿå…§å®¹)
            context = re.sub(r"\(?CAS\s*No\.?[\s\d-]+\)?", " ", context, flags=re.IGNORECASE)

            # 3. åˆ‡é™¤ æ¨™æº–ç·¨è™Ÿèˆ‡å¹´ä»½ (ä¾‹å¦‚ "IEC 62321-5:2013")
            context = re.sub(r"IEC\s*62321[-\d:+A]*", " ", context, flags=re.IGNORECASE)
            # é¡å¤–æ¸…é™¤ç¨ç«‹çš„å¹´ä»½ (1990-2030)
            context = re.sub(r"\b(19|20)\d{2}\b", " ", context)

            # 4. åˆ‡é™¤ Limit / MDL æ¨™ç±¤èˆ‡æ•¸å€¼ (ä¾‹å¦‚ "Max 1000", "MDL 2")
            context = re.sub(r"(Max|Limit|MDL|LOQ)\s*\d+(\.\d+)?", " ", context, flags=re.IGNORECASE)

            # ==========================================
            # æ­¥é©Ÿ C: åˆ¤æ–·çµæœ - N.D. å„ªå…ˆ
            # ==========================================

            # è¦å‰‡ï¼šè©ç•Œ(\b) + N + ä»»æ„é»æˆ–ç©º + D + è©ç•Œ OR Not Detected
            nd_pattern = r"(\bN\s*\.?\s*D\s*\.?\b)|(Not\s*Detected)"
            
            if re.search(nd_pattern, context, re.IGNORECASE):
                return "N.D."

            # åˆ¤æ–· NEGATIVE
            if re.search(r"NEGATIVE", context, re.IGNORECASE):
                return "NEGATIVE"

            # ==========================================
            # æ­¥é©Ÿ D: æŠ“å–æ•¸å€¼
            # ==========================================
            
            # å› ç‚ºä¸Šé¢å·²ç¶“æŠŠ Max, MDL, Year éƒ½åˆªäº†ï¼Œé€™è£¡æŠ“åˆ°çš„é€šå¸¸å°±æ˜¯çµæœ
            nums = re.findall(r"\b\d+(\.\d+)?\b", context)
            
            if nums:
                found_value = nums[0][0] 
                try:
                    val_float = float(found_value)
                    # æœ€å¾Œé˜²å‘†ï¼šå¦‚æœæŠ“åˆ°åƒå¹´ä»½çš„æ•´æ•¸ï¼Œå¿½ç•¥
                    if 1990 <= val_float <= 2030 and val_float.is_integer():
                        continue 
                    return found_value
                except:
                    pass

    return ""

def extract_pfas(text):
    return "REPORT" if re.search(r"\bPFAS\b", text, re.IGNORECASE) else ""

def extract_date(first_page_text):
    match = re.search(
        r"(REPORTED DATE|TEST REPORT REPORTED DATE)\s*[:\-]?\s*([^\n]+)",
        first_page_text,
        re.IGNORECASE
    )
    return match.group(2).strip() if match else ""

def normalize_date(date_text):
    if not date_text:
        return ""
    try:
        dt = parser.parse(date_text, dayfirst=True)
        return dt.strftime("%Y/%m/%d")
    except:
        return ""

def merge_results(values):
    """
    å½™ç¸½é‚è¼¯ï¼šå–æœ€å¤§å€¼ï¼Œè‹¥æœ‰ N.D. å‰‡å„ªå…ˆç´šä½æ–¼æ•¸å€¼
    """
    nums = []
    has_nd = False
    has_neg = False

    for v in values:
        if not v:
            continue
        v_upper = str(v).upper()
        
        if "N.D." in v_upper:
            has_nd = True
        elif "NEGATIVE" in v_upper:
            has_neg = True
        else:
            try:
                nums.append(float(v))
            except:
                pass

    if nums:
        return str(max(nums))
    if has_neg:
        return "NEGATIVE"
    if has_nd:
        return "N.D."
    return ""

# =========================
# 3. Streamlit ä¸»ç¨‹å¼ä»‹é¢
# =========================

uploaded_files = st.file_uploader(
    "è«‹ä¸Šå‚³ SGS PDF Reportï¼ˆå¯ä¸€æ¬¡å¤šé¸ï¼‰",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    rows = []
    for file in uploaded_files:
        full_text, pages_text = extract_text_and_pages(file)
        record = {}

        # æŠ“å–å„é …ç›®
        for item, keyword in ITEM_RULES.items():
            record[item] = extract_result(full_text, keyword)

        # ç‰¹æ®Šé …ç›®èˆ‡æ—¥æœŸ
        record["PFAS"] = extract_pfas(full_text)
        raw_date = extract_date(pages_text[0]) if pages_text else ""
        record["DATE"] = normalize_date(raw_date)

        rows.append(record)

    df_all = pd.DataFrame(rows)

    # åŒæ‰¹æ¬¡å½™ç¸½
    merged = {}
    if not df_all.empty:
        for col in FINAL_COLUMNS:
            if col in ["DATE", "PFAS"]:
                continue
            if col in df_all.columns:
                merged[col] = merge_results(df_all[col].tolist())
            else:
                merged[col] = ""

        merged["PFAS"] = "REPORT" if "REPORT" in df_all["PFAS"].tolist() else ""
        
        valid_dates = [d for d in df_all["DATE"] if d]
        merged["DATE"] = max(valid_dates) if valid_dates else ""

        df_final = pd.DataFrame([merged], columns=FINAL_COLUMNS)

        # é¡¯ç¤ºèˆ‡ä¸‹è¼‰
        st.subheader("ğŸ“Š å½™ç¸½çµæœï¼ˆåŒæ‰¹ SGS Reportï¼‰")
        st.dataframe(df_final, use_container_width=True)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_final.to_excel(writer, sheet_name="SGS_Result", index=False)

        st.download_button(
            "â¬‡ï¸ ä¸‹è¼‰å…¬å¸åˆ¶å¼ Excel",
            output.getvalue(),
            file_name="SGS_Test_Result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.warning("æœªè®€å–åˆ°æœ‰æ•ˆè³‡æ–™ã€‚")
