import streamlit as st
import pdfplumber
import pandas as pd
import io
import re
from datetime import datetime

# =============================================================================
# 1. å®šç¾©æ¬„ä½èˆ‡é—œéµå­—
# =============================================================================

OUTPUT_COLUMNS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBB", "PBDE", 
    "DEHP", "BBP", "DBP", "DIBP", 
    "PFOS", "PFAS", "F", "CL", "BR", "I", 
    "æ—¥æœŸ", "æª”æ¡ˆåç¨±"
]

# --- [Standard Engine] æ¨™æº– SGS / CTI ç”¨çš„é—œéµå­— (Table Based) ---
SIMPLE_KEYWORDS = {
    "Pb": ["Lead", "é‰›", "Pb"],
    "Cd": ["Cadmium", "é˜", "Cd"],
    "Hg": ["Mercury", "æ±", "Hg"],
    "Cr6+": ["Hexavalent Chromium", "å…­åƒ¹é‰»", "Cr(VI)", "Chromium VI", "Hexavalent Chromium"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate", "Bis(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Butyl benzyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "PFOS": ["Perfluorooctane sulfonates", "Perfluorooctane sulfonate", "Perfluorooctane sulfonic acid", "å…¨æ°Ÿè¾›çƒ·ç£ºé…¸", "Perfluorooctane Sulfonamide", "PFOS and its salts", "PFOS åŠå…¶ç›"],
    "F": ["Fluorine", "æ°Ÿ"],
    "CL": ["Chlorine", "æ°¯"],
    "BR": ["Bromine", "æº´"],
    "I": ["Iodine", "ç¢˜", "lodine"] # v63.24: ä¿ç•™ lodine
}

GROUP_KEYWORDS = {
    "PBB": ["Polybrominated Biphenyls", "PBBs", "Sum of PBBs", "å¤šæº´è¯è‹¯ç¸½å’Œ", "å¤šæº´è¯è‹¯ä¹‹å’Œ"],
    "PBDE": ["Polybrominated Diphenyl Ethers", "PBDEs", "Sum of PBDEs", "å¤šæº´è¯è‹¯é†šç¸½å’Œ", "å¤šæº´äºŒè‹¯é†šä¹‹å’Œ"]
}

PFAS_SUMMARY_KEYWORDS = ["Per- and Polyfluoroalkyl Substances", "PFAS", "å…¨æ°Ÿ/å¤šæ°Ÿçƒ·åŸºç‰©è³ª"]
MSDS_HEADER_KEYWORDS = ["content", "composition", "concentration", "å«é‡", "æˆåˆ†"]

# --- [Malaysia Engine] é¦¬ä¾†è¥¿äºå°ˆç”¨ Regex æ˜ å°„ (Text Based) ---
# é€™æ˜¯æ‚¨æä¾›çš„ ITEM_RULESï¼Œæˆ‘å°‡å…¶ Key æ˜ å°„åˆ°ç³»çµ±æ¨™æº–çš„ OUTPUT_COLUMNS
MY_ITEM_RULES = {
    "Pb": r"Lead\s*\(Pb\)",
    "Cd": r"Cadmium\s*\(Cd\)",
    "Hg": r"Mercury\s*\(Hg\)",
    "Cr6+": r"Hexavalent Chromium", # User: CrVI -> System: Cr6+
    "PBB": r"Sum of PBBs",          # User: PBBs -> System: PBB
    "PBDE": r"Sum of PBDEs",        # User: PBDEs -> System: PBDE
    "DEHP": r"DEHP|Di\(2-ethylhexyl\)\s*phthalate",
    "BBP": r"BBP|Benzyl\s*butyl\s*phthalate",
    "DBP": r"DBP|Dibutyl\s*phthalate",
    "DIBP": r"DIBP|Diisobutyl\s*phthalate",
    "F": r"\bFluorine\b",
    "CL": r"\bChlorine\b",
    "BR": r"\bBromine\b",
    "I": r"\bIodine\b",
    "PFOS": r"PFOS",
    "PFAS": r"PFAS"
}

# é¦¬ä¾†è¥¿äºç‰ˆå°ˆç”¨çš„ MDL é»‘åå–® (é˜²æ­¢èª¤æŠ“ MDL ç•¶ä½œçµæœ)
MY_MDL_BLOCKLIST = {
    "Pb": [2.0], "Cd": [2.0], "Hg": [2.0], "Cr6+": [8.0, 10.0],
    "F": [50.0], "CL": [50.0], "BR": [50.0], "I": [50.0],
    "DEHP": [50.0], "BBP": [50.0], "DBP": [50.0], "DIBP": [50.0]
}

# æ‰‹å‹•æœˆä»½å°ç…§è¡¨
MONTH_MAP = {
    'jan': 1, 'january': 1, 'feb': 2, 'february': 2, 'mar': 3, 'march': 3,
    'apr': 4, 'april': 4, 'may': 5, 'jun': 6, 'june': 6, 'jul': 7, 'july': 7,
    'aug': 8, 'august': 8, 'sep': 9, 'september': 9, 'sept': 9, 'oct': 10, 'october': 10,
    'nov': 11, 'november': 11, 'dec': 12, 'december': 12
}

# =============================================================================
# 2. å…±ç”¨è¼”åŠ©å‡½å¼
# =============================================================================

def clean_text(text):
    if not text: return ""
    return str(text).replace('\n', ' ').strip()

def is_valid_date(dt):
    if 2000 <= dt.year <= 2030: return True
    return False

def is_suspicious_limit_value(val):
    try:
        n = float(val)
        if n in [1000.0, 100.0, 50.0, 25.0, 10.0, 5.0, 2.0]: return True
        return False
    except: return False

def parse_value_priority(value_str):
    """
    é€šç”¨å„ªå…ˆç´šè§£æ: N.D. (1) < Negative (2) < Value (3) < Report (4)
    """
    raw_val = clean_text(value_str)
    if "(" in raw_val and ")" in raw_val:
        if re.search(r"\(\d+\)", raw_val):
            raw_val = raw_val.split("(")[0].strip()
    val = raw_val.replace("mg/kg", "").replace("ppm", "").replace("%", "").replace("Âµg/cmÂ²", "").strip()
    
    if not val: return (0, 0, "")
    val_lower = val.lower()
    
    if val_lower in ["result", "limit", "mdl", "loq", "rl", "unit", "method", "004", "001", "no.1", "---", "-", "limits", "n.a.", "/"]: 
        return (0, 0, "")
    if re.search(r"\d+-\d+-\d+", val): return (0, 0, "") 
    
    num_only_match = re.search(r"^([\d\.]+)$", val)
    if num_only_match:
        if is_suspicious_limit_value(num_only_match.group(1)): return (0, 0, "")

    if "nd" in val_lower or "n.d." in val_lower or "<" in val_lower: return (1, 0, "N.D.")
    if "negative" in val_lower or "é™°æ€§" in val_lower: return (2, 0, "NEGATIVE")
    
    num_match = re.search(r"^([\d\.]+)(.*)$", val)
    if num_match:
        try:
            number = float(num_match.group(1))
            return (3, number, val)
        except: pass
    return (0, 0, val)

def identify_company(text):
    txt = text.lower()
    if "sgs" in txt: return "SGS"
    if "intertek" in txt: return "INTERTEK"
    if "cti" in txt or "centre testing" in txt: return "CTI"
    if "ctic" in txt: return "CTIC"
    return "OTHERS"

# =============================================================================
# 3. SGS é¦¬ä¾†è¥¿äºå°ˆç”¨å¼•æ“ (v63.26: æ–‡å­—æƒæé‚è¼¯å®Œå…¨ç§»æ¤ç‰ˆ)
# =============================================================================

def extract_date_malaysia_v7(text):
    """
    ç§»æ¤è‡ªæ‚¨çš„ extract_date å‡½å¼
    ä½¿ç”¨ç²¾ç¢ºçš„ Regex é–å®š REPORTED DATE
    """
    match = re.search(r"(REPORTED DATE|TEST REPORT REPORTED DATE)\s*[:\-]?\s*([^\n]+)", text, re.IGNORECASE)
    if match:
        date_str = match.group(2).strip()
        try:
            # å˜—è©¦è§£ææ—¥æœŸ
            date_str = re.sub(r"[^a-zA-Z0-9\s]", " ", date_str)
            parts = date_str.split()
            d, m, y = None, None, None
            for p in parts:
                if p.isdigit():
                    if len(p) == 4: y = int(p)
                    elif int(p) <= 31: d = int(p)
                elif p.lower() in MONTH_MAP:
                    m = MONTH_MAP[p.lower()]
            
            if d and m and y:
                dt = datetime(y, m, d)
                if is_valid_date(dt):
                    return dt
        except: pass
    return None

def extract_result_malaysia_v7(text, keyword, item_name):
    """
    ç§»æ¤è‡ªæ‚¨çš„ extract_result å‡½å¼ (V7)
    ç‰¹é»ï¼š
    1. ä¸çœ‹è¡¨æ ¼ï¼Œçœ‹æ–‡å­—è¡Œ
    2. å„ªå…ˆæŠ“æ•¸å­— (é¿é–‹ MDL)ï¼Œè‹¥åªå‰© 1 å€‹æ•¸å­—è¦–ç‚º N.D.
    3. å¼·åˆ¶éæ¿¾ MDL é»‘åå–®
    """
    lines = text.splitlines()

    for i, line in enumerate(lines):
        # æ­¥é©Ÿ A: é–å®šé—œéµå­—æ‰€åœ¨çš„è¡Œ
        if re.search(keyword, line, re.IGNORECASE):
            
            # --- Context Window ---
            if item_name == "DEHP":
                context = " ".join(lines[i:i+4]) # DEHP æ“´å¤§
            else:
                context = " ".join(lines[i:i+2]) # ä¸€èˆ¬ 2 è¡Œ (æ‚¨çš„ç¨‹å¼æ”¹ç‚º3è¡Œï¼Œé€™è£¡å–æŠ˜è¡·æˆ–ç…§èˆŠï¼Œå…ˆç…§æ‚¨çš„ç¨‹å¼é‚è¼¯èµ°)
                # æ‚¨çš„ç¨‹å¼è¨»è§£å¯«ä¸€èˆ¬è®€ 2 è¡Œï¼Œä½†ç¨‹å¼ç¢¼è£¡æœ‰ä¸€æ®µæ˜¯ context = " ".join(lines[i:i+3]) ? 
                # ç‚ºäº†ä¿éšªï¼Œé€™è£¡æ¡ç”¨æ‚¨ç¨‹å¼ç¢¼ä¸­é‡å°éDEHPçš„è¨­å®šï¼Œé€šå¸¸ lines[i:i+3] æœƒæ›´ç©©
                context = " ".join(lines[i:i+3])

            # --- Cleaning (é™¤å™ª) ---
            if item_name == "DEHP":
                context = re.sub(r"2-ethylhexyl", " ", context, flags=re.IGNORECASE)
                context = re.sub(r"Di\(2-", " ", context, flags=re.IGNORECASE)

            context = re.sub(r"mg/kg|ppm|%|wt%", " ", context, flags=re.IGNORECASE)
            context = re.sub(r"\(?CAS\s*No\.?[\s\d-]+\)?", " ", context, flags=re.IGNORECASE)
            context = re.sub(r"IEC\s*62321[-\d:+A]*", " ", context, flags=re.IGNORECASE)
            context = re.sub(r"\b(19|20)\d{2}\b", " ", context) 
            context = re.sub(r"(Max|Limit|MDL|LOQ)\s*\d+(\.\d+)?", " ", context, flags=re.IGNORECASE)

            # --- N.D. åˆ¤å®š (å„ªå…ˆ) ---
            nd_pattern = r"(\bN\s*\.?\s*D\s*\.?\b)|(Not\s*Detected)"
            if re.search(nd_pattern, context, re.IGNORECASE):
                return "N.D."
            if re.search(r"NEGATIVE", context, re.IGNORECASE):
                return "NEGATIVE"

            # --- æ•¸å­—æŠ“å– ---
            nums = re.findall(r"\b\d+(?:\.\d+)?\b", context)
            if not nums: continue # é€™è¡Œæ²’çµæœï¼Œæ‰¾ä¸‹ä¸€è¡Œ

            final_val = None

            # ç‰¹æ¬Šé …ç›®: PBB / PBDE (MDL ç‚º Dash "-")
            if item_name in ["PBB", "PBDE"]:
                final_val = nums[0]
            else:
                # ä¸€èˆ¬é …ç›®é‚è¼¯ (æ‚¨çš„ V7 æ ¸å¿ƒ)
                if len(nums) >= 2:
                    # å‰©ä¸‹å…©å€‹ä»¥ä¸Šæ•¸å­— -> å–ç¬¬ 1 å€‹
                    candidate = nums[0]
                    try:
                        f_val = float(candidate)
                        if 1990 <= f_val <= 2030 and f_val.is_integer(): # é˜²å‘†å¹´ä»½
                             candidate = nums[1]
                    except: pass
                    final_val = candidate
                
                elif len(nums) == 1:
                    # [é—œéµ] åªå‰©ä¸‹ä¸€å€‹æ•¸å­—ï¼Œæ¥µå¤§æ©Ÿç‡æ˜¯ MDL -> å¼·åˆ¶ N.D.
                    return "N.D."

            # --- é»‘åå–®éæ¿¾ ---
            if final_val:
                try:
                    val_float = float(final_val)
                    if item_name in MY_MDL_BLOCKLIST:
                        if val_float in MY_MDL_BLOCKLIST[item_name]:
                            return "N.D." # å‘½ä¸­é»‘åå–® (æ˜¯ MDL)
                    return final_val
                except: pass

    return ""

def process_malaysia_engine(pdf, filename):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    
    # 1. å…¨æ–‡æå– (ç”¨æ–¼æ–‡å­—æƒæ)
    full_text = ""
    for p in pdf.pages: 
        full_text += (p.extract_text() or "") + "\n"
    
    # 2. æ—¥æœŸæŠ“å– (ä½¿ç”¨æ‚¨çš„ç²¾ç¢º Regex é‚è¼¯)
    report_date = None
    first_page_text = (pdf.pages[0].extract_text() or "")
    report_date = extract_date_malaysia_v7(first_page_text)
    
    # 3. æ•¸æ“šæŠ“å– (ä½¿ç”¨æ‚¨çš„æ–‡å­—æƒæé‚è¼¯)
    for col_key in OUTPUT_COLUMNS:
        if col_key in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]: continue
        
        # å–å¾—å°æ‡‰çš„ Regex
        keyword = MY_ITEM_RULES.get(col_key)
        if not keyword: continue
        
        # åŸ·è¡Œæƒæ
        val = extract_result_malaysia_v7(full_text, keyword, col_key)
        
        if val:
            # è½‰æ›å„ªå…ˆç´šæ ¼å¼
            # å°æ–¼é¦¬ä¾†è¥¿äºå¼•æ“ï¼ŒæŠ“åˆ°çš„ç›´æ¥è¦–ç‚ºæœ‰æ•ˆæ•¸æ“š
            prio = parse_value_priority(val)
            if prio[0] > 0:
                data_pool[col_key].append({"priority": prio, "filename": filename})

    # 4. æ—¥æœŸæ ¼å¼åŒ–
    date_candidates = []
    if report_date:
        date_candidates.append((100, report_date))

    return data_pool, date_candidates

# =============================================================================
# 4. SGS æ¨™æº–å¼•æ“ (v63.24: å®Œå…¨ä¿ç•™ï¼Œå«é¹µç´ å€å¡Š & æ™ºæ…§è¡¨æ ¼)
# =============================================================================

def extract_dates_v60(text):
    lines = text.split('\n')
    candidates = []
    bonus_kw = ["report date", "issue date", "date:", "dated", "æ—¥æœŸ"]
    poison_kw = ["approve", "approved", "receive", "received", "receipt", "period", "expiry", "valid"]
    pat_chinese = r"(20\d{2})\s*å¹´\s*(0?[1-9]|1[0-2])\s*æœˆ\s*(3[01]|[12][0-9]|0?[1-9])\s*æ—¥"
    pat_ymd = r"(20\d{2})[\.\/-](0?[1-9]|1[0-2])[\.\/-](3[01]|[12][0-9]|0?[1-9])"
    pat_dmy = r"(3[01]|[12][0-9]|0?[1-9])\s+([a-zA-Z]{3,})\s+(20\d{2})"
    pat_mdy = r"([a-zA-Z]{3,})\s+(3[01]|[12][0-9]|0?[1-9])\s+(20\d{2})"

    for line in lines:
        line_lower = line.lower()
        score = 1
        if any(bad in line_lower for bad in poison_kw): score = -100 
        elif any(good in line_lower for good in bonus_kw): score = 100 

        matches_cn = re.finditer(pat_chinese, line)
        for m in matches_cn:
            try:
                dt = datetime.strptime(f"{m.group(1)}-{m.group(2)}-{m.group(3)}", "%Y-%m-%d")
                if is_valid_date(dt): candidates.append((score, dt))
            except: pass

        clean_line = line.replace(".", " ").replace(",", " ").replace("-", " ").replace("/", " ")
        clean_line = clean_line.replace("å¹´", " ").replace("æœˆ", " ").replace("æ—¥", " ")
        clean_line = " ".join(clean_line.split())
        
        for pat in [pat_ymd, pat_dmy, pat_mdy]:
            matches = re.finditer(pat, clean_line)
            for m in matches:
                try:
                    dt_str = " ".join(m.groups())
                    for fmt in ["%Y %m %d", "%d %b %Y", "%d %B %Y", "%b %d %Y", "%B %d %Y"]:
                        try:
                            dt = datetime.strptime(dt_str, fmt)
                            if is_valid_date(dt): 
                                candidates.append((score, dt))
                                break
                        except: pass
                except: pass
    return candidates

def identify_columns_v60(table, company):
    item_idx = -1
    result_idx = -1
    mdl_idx = -1
    max_scan_rows = min(3, len(table))
    full_header_text = ""
    for r in range(max_scan_rows):
        full_header_text += " ".join([str(c).lower() for c in table[r] if c]) + " "

    is_msds_table = False
    if any(k in full_header_text for k in MSDS_HEADER_KEYWORDS) and "result" not in full_header_text and "ç»“æœ" not in full_header_text:
        is_msds_table = True

    for r_idx in range(max_scan_rows):
        row = table[r_idx]
        for c_idx, cell in enumerate(row):
            txt = clean_text(cell).lower()
            if not txt: continue
            if "test item" in txt or "tested item" in txt or "æ¸¬è©¦é …ç›®" in txt or "æ£€æµ‹é¡¹ç›®" in txt or "parameter" in txt:
                if item_idx == -1: item_idx = c_idx
            if "mdl" in txt or "loq" in txt:
                if mdl_idx == -1: mdl_idx = c_idx
            if company == "SGS":
                 if ("result" in txt or "çµæœ" in txt or "ç»“æœ" in txt or re.search(r"00[1-9]", txt) or 
                    re.search(r"^[a-zA-Z]?\s*\.?\s*[a-zA-Z]?\d+", txt) or re.search(r"[a-zA-Z]\s*\.\s*[a-zA-Z]\d+", txt) or "no." in txt):
                    if "cas" not in txt and "method" not in txt and "limit" not in txt:
                        if result_idx == -1: result_idx = c_idx
            else:
                if ("result" in txt or "çµæœ" in txt or "ç»“æœ" in txt or re.search(r"00[1-9]", txt)):
                    if result_idx == -1: result_idx = c_idx
    
    if result_idx == -1 and company == "SGS" and mdl_idx != -1:
        forbidden_headers = ["unit", "method", "limit", "mdl", "loq", "item", "cas"]
        left_idx = mdl_idx - 1
        left_score = 0
        if left_idx >= 0:
            header = clean_text(table[0][left_idx]).lower()
            if not any(fb in header for fb in forbidden_headers):
                for r in range(1, min(5, len(table))):
                    val = clean_text(table[r][left_idx]).lower()
                    if "n.d." in val or "nd" in val or re.search(r"\d", val): left_score += 1
                    if "mg/kg" in val: left_score -= 5
        right_idx = mdl_idx + 1
        right_score = 0
        if right_idx < len(table[0]):
            header = clean_text(table[0][right_idx]).lower()
            if not any(fb in header for fb in forbidden_headers):
                for r in range(1, min(5, len(table))):
                    val = clean_text(table[r][right_idx]).lower()
                    if "n.d." in val or "nd" in val or re.search(r"\d", val): right_score += 1
                    if "mg/kg" in val: right_score -= 5
        if right_score > 0 and right_score >= left_score:
            result_idx = right_idx
        elif left_score > 0:
            result_idx = left_idx

    is_reference_table = False
    if is_msds_table: is_reference_table = True
    elif result_idx == -1:
        if "restricted substances" in full_header_text or "group name" in full_header_text or "substance name" in full_header_text:
            is_reference_table = True
        if company == "INTERTEK" and "limits" in full_header_text:
            is_reference_table = True
        if item_idx == -1:
            is_reference_table = True
    return item_idx, result_idx, is_reference_table, mdl_idx

def parse_text_lines_v60(text, data_pool, file_group_data, filename, company, targets=None):
    lines = text.split('\n')
    for line in lines:
        line_clean = clean_text(line)
        line_lower = line_clean.lower()
        if not line_clean: continue
        if any(bad in line_lower for bad in MSDS_HEADER_KEYWORDS): continue

        matched_simple = None
        for key, keywords in SIMPLE_KEYWORDS.items():
            if targets and key not in targets: continue
            if key == "Cd" and any(bad in line_lower for bad in ["hbcdd", "cyclododecane", "ecd", "indeno"]): continue 
            if key == "F" and any(bad in line_lower for bad in ["perfluoro", "polyfluoro", "pfos", "pfoa", "å…¨æ°Ÿ"]): continue
            if key == "BR" and any(bad in line_lower for bad in ["polybromo", "hexabromo", "monobromo", "dibromo", "tribromo", "tetrabromo", "pentabromo", "heptabromo", "octabromo", "nonabromo", "decabromo", "multibromo", "pbb", "pbde", "å¤šæº´", "å…­æº´", "ä¸€æº´", "äºŒæº´", "ä¸‰æº´", "å››æº´", "äº”æº´", "ä¸ƒæº´", "å…«æº´", "ä¹æº´", "åæº´", "äºŒè‹¯é†š"]): continue
            if key == "Pb" and any(bad in line_lower for bad in ["pbb", "pbde", "polybrominated", "å¤šæº´"]): continue
            for kw in keywords:
                if kw.lower() in line_lower and "test item" not in line_lower:
                    matched_simple = key
                    break
            if matched_simple: break
        
        matched_group = None
        if not matched_simple:
            for group_key, keywords in GROUP_KEYWORDS.items():
                if targets and group_key not in targets: continue
                for kw in keywords:
                    if kw.lower() in line_lower:
                        matched_group = group_key
                        break
                if matched_group: break
        
        if matched_simple or matched_group:
            parts = line_clean.split()
            if len(parts) < 2: continue
            found_val = ""
            for part in reversed(parts):
                p_lower = part.lower()
                if p_lower in ["mg/kg", "ppm", "2", "5", "10", "50", "100", "1000", "0.1", "-", "---", "unit", "mdl"]: continue
                if "nd" in p_lower:
                    found_val = "N.D."
                    break
                if re.match(r"^\d+.*$", part): 
                    val_check = part.replace("â–²", "").replace("â–³", "")
                    try:
                        f = float(val_check)
                        if f not in [100.0, 1000.0, 50.0]:
                            found_val = part
                            break
                    except: pass
            if found_val:
                priority = parse_value_priority(found_val)
                if priority[0] == 0: continue
                if matched_simple:
                    data_pool[matched_simple].append({"priority": priority, "filename": filename})
                elif matched_group:
                    file_group_data[matched_group].append(priority)

def process_halogen_block(pdf, filename, data_pool):
    for page in pdf.pages:
        text = (page.extract_text() or "").lower()
        if "halogen" in text:
            tables = page.extract_tables()
            for table in tables:
                if not table or len(table) < 2: continue
                for row in table:
                    clean_row = [clean_text(cell) for cell in row]
                    row_txt = "".join(clean_row).lower()
                    matched_key = None
                    if "fluorine" in row_txt: matched_key = "F"
                    elif "chlorine" in row_txt: matched_key = "CL"
                    elif "bromine" in row_txt: matched_key = "BR"
                    elif "iodine" in row_txt or "lodine" in row_txt: matched_key = "I"
                    
                    if matched_key:
                        result_val = ""
                        for cell in reversed(clean_row):
                            c_lower = cell.lower()
                            if "mg/kg" in c_lower or "ppm" in c_lower or "limit" in c_lower or "unit" in c_lower: continue
                            if "nd" in c_lower or "n.d." in c_lower:
                                result_val = cell
                                break
                            if re.search(r"^\d+(\.\d+)?", cell):
                                if is_suspicious_limit_value(cell): continue
                                result_val = cell
                                break
                        if result_val:
                            priority = parse_value_priority(result_val)
                            if priority[0] > 0:
                                data_pool[matched_key].append({"priority": priority, "filename": filename})

def process_standard_engine(pdf, filename, company):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    file_dates_candidates = []
    full_text_content = ""
    
    first_page_text = (pdf.pages[0].extract_text() or "").lower()
    if "per- and polyfluoroalkyl substances" in first_page_text or "pfas" in first_page_text:
        data_pool["PFAS"].append({"priority": (4, 0, "REPORT"), "filename": filename})

    for p in pdf.pages[:5]:
        txt = p.extract_text() or ""
        full_text_content += txt + "\n"
        file_dates_candidates.extend(extract_dates_v60(txt))

    file_group_data = {key: [] for key in GROUP_KEYWORDS.keys()}

    for page in pdf.pages:
        tables = page.extract_tables()
        for table in tables:
            if not table or len(table) < 2: continue
            item_idx, result_idx, is_skip, mdl_idx = identify_columns_v60(table, company)
            
            force_scan = False
            if is_skip:
                table_str = str(table).lower()
                if any(k in table_str for k in ["fluorine", "chlorine", "bromine", "iodine", "lodine"]):
                    force_scan = True
                    is_skip = False
                    if item_idx == -1: item_idx = 0

            if is_skip: continue
            
            for row in table:
                raw_item_cell = str(row[item_idx]) if item_idx < len(row) and row[item_idx] else ""
                raw_result_cell = str(row[result_idx]) if result_idx != -1 and result_idx < len(row) and row[result_idx] else ""
                
                rows_to_process = []
                if "\n" in raw_item_cell:
                    split_items = [x.strip() for x in raw_item_cell.split('\n') if x.strip()]
                    if "\n" in raw_result_cell:
                        split_results = [x.strip() for x in raw_result_cell.split('\n') if x.strip()]
                    else:
                        split_results = [raw_result_cell.strip()] if raw_result_cell.strip() else []

                    if len(split_items) > 1 and len(split_results) == 1:
                        for si in split_items:
                            virtual_row = list(row)
                            virtual_row[item_idx] = si
                            if result_idx != -1: virtual_row[result_idx] = split_results[0]
                            rows_to_process.append(virtual_row)
                    elif len(split_items) == len(split_results):
                        for si, sr in zip(split_items, split_results):
                            virtual_row = list(row)
                            virtual_row[item_idx] = si
                            if result_idx != -1: virtual_row[result_idx] = sr
                            rows_to_process.append(virtual_row)
                    else:
                        rows_to_process.append(row)
                else:
                    rows_to_process.append(row)

                for proc_row in rows_to_process:
                    clean_row = [clean_text(cell) for cell in proc_row]
                    row_txt = "".join(clean_row).lower()
                    if "test item" in row_txt or "result" in row_txt: continue
                    if not any(clean_row): continue
                    
                    target_item_col = item_idx if item_idx != -1 else 0
                    if target_item_col >= len(clean_row): continue
                    item_name = clean_row[target_item_col]
                    item_name_lower = item_name.lower()
                    
                    if "pvc" in item_name_lower: continue

                    result = ""
                    if result_idx != -1 and result_idx < len(clean_row):
                        result = clean_row[result_idx]
                    
                    if result == "" and force_scan:
                        for cell in reversed(clean_row):
                            c_lower = cell.lower()
                            if "mg/kg" in c_lower or "ppm" in c_lower: continue
                            if "nd" in c_lower or "n.d." in c_lower:
                                result = cell
                                break
                            if re.search(r"^\d+(\.\d+)?", cell):
                                if is_suspicious_limit_value(cell): continue
                                result = cell
                                break

                    temp_priority = parse_value_priority(result)
                    if temp_priority[0] == 0:
                        for cell in reversed(clean_row):
                            c_lower = cell.lower()
                            if not cell: continue
                            if "nd" in c_lower or "n.d." in c_lower or "negative" in c_lower:
                                result = cell
                                break
                            if re.search(r"^\d+(\.\d+)?", cell):
                                if is_suspicious_limit_value(cell): continue
                                result = cell
                                break
                    
                    priority = parse_value_priority(result)
                    if priority[0] == 0: continue

                    for target_key, keywords in SIMPLE_KEYWORDS.items():
                        if target_key == "Cd" and any(bad in item_name_lower for bad in ["hbcdd", "cyclododecane", "ecd", "indeno"]): continue
                        if target_key == "F" and any(bad in item_name_lower for bad in ["perfluoro", "polyfluoro", "pfos", "pfoa", "å…¨æ°Ÿ"]): continue
                        if target_key == "BR" and any(bad in item_name_lower for bad in ["polybromo", "hexabromo", "monobromo", "dibromo", "tribromo", "tetrabromo", "pentabromo", "heptabromo", "octabromo", "nonabromo", "decabromo", "multibromo", "pbb", "pbde", "å¤šæº´", "å…­æº´", "ä¸€æº´", "äºŒæº´", "ä¸‰æº´", "å››æº´", "äº”æº´", "ä¸ƒæº´", "å…«æº´", "ä¹æº´", "åæº´", "äºŒè‹¯é†š"]): continue
                        if target_key == "Pb" and any(bad in item_name_lower for bad in ["pbb", "pbde", "polybrominated", "å¤šæº´"]): continue

                        for kw in keywords:
                            if kw.lower() in item_name_lower:
                                if target_key == "PFOS" and "related" in item_name_lower: continue 
                                data_pool[target_key].append({"priority": priority, "filename": filename})
                    
                    for group_key, keywords in GROUP_KEYWORDS.items():
                        for kw in keywords:
                            if kw.lower() in item_name_lower:
                                file_group_data[group_key].append(priority)
                                break

    if not (data_pool["F"] and data_pool["CL"] and data_pool["BR"] and data_pool["I"]):
        process_halogen_block(pdf, filename, data_pool)

    if company == "SGS":
        missing_targets = []
        pb_data = [d for d in data_pool["Pb"] if d['filename'] == filename]
        halogen_data = []
        for h in ["F", "CL", "BR", "I"]:
            halogen_data.extend([d for d in data_pool[h] if d['filename'] == filename])
        pfos_data = [d for d in data_pool["PFOS"] if d['filename'] == filename]
        
        trigger_rescue = False
        if not pb_data: trigger_rescue = True
        if ("halogen" in full_text_content.lower() or "å¤ç´ " in full_text_content) and not halogen_data:
            trigger_rescue = True
        if "pfos" in full_text_content.lower() and not pfos_data:
            trigger_rescue = True

        if trigger_rescue:
             parse_text_lines_v60(full_text_content, data_pool, file_group_data, filename, company, targets=None)

    for group_key, values in file_group_data.items():
        if values:
            best_in_file = sorted(values, key=lambda x: (x[0], x[1]), reverse=True)[0]
            data_pool[group_key].append({"priority": best_in_file, "filename": filename})

    return data_pool, file_dates_candidates

# =============================================================================
# 5. CTI å°ˆç”¨å¼•æ“ (v63.13: Global Token Stream)
# =============================================================================

def extract_dates_v63_13_global(text):
    candidates = []
    poison_kw = ["received", "receive", "expiry", "valid", "process"]
    backup_kw = ["testing", "period", "test"]
    
    clean_text_str = re.sub(r'[^a-z0-9]', ' ', text.lower())
    tokens = clean_text_str.split()
    
    for i in range(len(tokens) - 2):
        t1, t2, t3 = tokens[i], tokens[i+1], tokens[i+2]
        dt = None
        
        try:
            if t1 in MONTH_MAP and t2.isdigit() and t3.isdigit() and len(t3) == 4:
                m, d, y = MONTH_MAP[t1], int(t2), int(t3)
                dt = datetime(y, m, d)
            elif t1.isdigit() and t2 in MONTH_MAP and t3.isdigit() and len(t3) == 4:
                d, m, y = int(t1), MONTH_MAP[t2], int(t3)
                dt = datetime(y, m, d)
            elif t1.isdigit() and len(t1) == 4 and t2.isdigit() and t3.isdigit():
                y, m, d = int(t1), int(t2), int(t3)
                dt = datetime(y, m, d)
                
            if dt and is_valid_date(dt):
                start_lookback = max(0, i - 10)
                context_window = tokens[start_lookback : i]
                score = 100 
                if any(p in context_window for p in poison_kw):
                    score = -1000 
                elif any(b in context_window for b in backup_kw):
                    score = 10 
                candidates.append((score, dt))
        except: pass
    return candidates

def process_cti_engine(pdf, filename):
    data_pool = {key: [] for key in OUTPUT_COLUMNS if key not in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]}
    
    text_for_dates = ""
    for p in pdf.pages[:3]: 
        text_for_dates += (p.extract_text() or "") + " " 
    
    date_candidates = extract_dates_v63_13_global(text_for_dates)
    final_dates = []
    
    if date_candidates:
        valid_entries = [entry for entry in date_candidates if entry[0] > 0]
        if valid_entries:
            best_entry = sorted(valid_entries, key=lambda x: (x[1], x[0]), reverse=True)[0]
            final_dates.append(best_entry)

    for page in pdf.pages:
        tables = page.extract_tables()
        for table in tables:
            if not table or len(table) < 2: continue
            
            item_col_idx = -1
            mdl_col_idx = -1
            cols = len(table[0])
            
            for c in range(cols):
                num_count = 0
                row_count = 0
                for r in range(1, len(table)):
                    val = clean_text(table[r][c]).replace("mg/kg", "").replace("~", "").replace("$", "").replace("%", "").strip()
                    if not val: continue
                    row_count += 1
                    if val in ["2", "5", "8", "10", "50", "100", "0.01", "0.010", "0.005", "20", "25"]: num_count += 1
                if row_count > 0 and (num_count / row_count) >= 0.5:
                    mdl_col_idx = c
                    break
            
            for c in range(cols):
                header = str(table[0][c]).lower()
                if "item" in header or "é …ç›®" in header or "é¡¹ç›®" in header:
                    item_col_idx = c
                    break
            if item_col_idx == -1: item_col_idx = 0 
            
            if mdl_col_idx == -1: continue 
            
            result_col_indices = [c for c in range(item_col_idx + 1, mdl_col_idx)]
            if not result_col_indices:
                if mdl_col_idx > 0: result_col_indices = [mdl_col_idx - 1]
            
            for row in table:
                if len(row) <= mdl_col_idx: continue
                item_text = clean_text(row[item_col_idx]).lower()
                row_candidates = []
                for c_idx in result_col_indices:
                    if c_idx < len(row):
                        val_str = str(row[c_idx])
                        prio = parse_value_priority(val_str)
                        if prio[0] > 0: 
                            row_candidates.append(prio)
                
                if not row_candidates: continue
                best_prio = sorted(row_candidates, key=lambda x: (x[0], x[1]), reverse=True)[0]
                
                for key, kws in SIMPLE_KEYWORDS.items():
                    if key == "Cd" and any(bad in item_text for bad in ["hbcdd", "cyclododecane", "ecd"]): continue 
                    if key == "F" and any(bad in item_text for bad in ["perfluoro", "polyfluoro", "pfos", "pfoa", "å…¨æ°Ÿ"]): continue
                    if key == "BR" and any(bad in item_text for bad in ["polybromo", "hexabromo", "monobromo", "dibromo", "tribromo", "tetrabromo", "pentabromo", "heptabromo", "octabromo", "nonabromo", "decabromo", "multibromo", "pbb", "pbde", "å¤šæº´", "å…­æº´", "ä¸€æº´", "äºŒæº´", "ä¸‰æº´", "å››æº´", "äº”æº´", "ä¸ƒæº´", "å…«æº´", "ä¹æº´", "åæº´", "äºŒè‹¯é†š"]): continue
                    if key == "Pb" and any(bad in item_text for bad in ["pbb", "pbde", "polybrominated", "å¤šæº´"]): continue

                    if any(kw.lower() in item_text for kw in kws):
                        data_pool[key].append({"priority": best_prio, "filename": filename})
                        break
                
                for key, kws in GROUP_KEYWORDS.items():
                    if any(kw.lower() in item_text for kw in kws):
                        data_pool[key].append({"priority": best_prio, "filename": filename})
                        break

    return data_pool, final_dates

# =============================================================================
# 6. ä¸»ç¨‹å¼èˆ‡åˆ†æµå™¨
# =============================================================================

def process_files(files):
    results = []
    progress_bar = st.progress(0)
    
    for i, file in enumerate(files):
        try:
            with pdfplumber.open(file) as pdf:
                first_page_text = (pdf.pages[0].extract_text() or "").upper()
                company = identify_company(first_page_text)
                
                # åˆ†æµé‚è¼¯
                if "MALAYSIA" in first_page_text and "SGS" in first_page_text:
                    # é€šé“ C: é¦¬ä¾†è¥¿äºå°ˆç”¨ (æ–‡å­—æƒæ)
                    data_pool, date_candidates = process_malaysia_engine(pdf, file.name)
                elif company == "CTI":
                    # é€šé“ B: CTI å°ˆç”¨
                    data_pool, date_candidates = process_cti_engine(pdf, file.name)
                else:
                    # é€šé“ A: æ¨™æº–/ä¸­åœ‹ SGS å°ˆç”¨ (ä¿ç•™ v63.24 æ‰€æœ‰åŠŸèƒ½)
                    data_pool, date_candidates = process_standard_engine(pdf, file.name, company)
                
                final_row = {}
                valid_candidates = [d for d in date_candidates if d[0] > -50]
                if valid_candidates:
                    best_date = sorted(valid_candidates, key=lambda x: (x[0], x[1]), reverse=True)[0][1]
                    final_row["æ—¥æœŸ"] = best_date.strftime("%Y/%m/%d")
                else:
                    final_row["æ—¥æœŸ"] = ""
                
                final_row["æª”æ¡ˆåç¨±"] = file.name
                
                for k in OUTPUT_COLUMNS:
                    if k in ["æ—¥æœŸ", "æª”æ¡ˆåç¨±"]: continue
                    candidates = data_pool.get(k, [])
                    if candidates:
                        best = sorted(candidates, key=lambda x: (x['priority'][0], x['priority'][1]), reverse=True)[0]
                        final_row[k] = best['priority'][2]
                    else:
                        final_row[k] = ""
                
                results.append(final_row)

        except Exception as e:
            st.error(f"æª”æ¡ˆ {file.name} è™•ç†å¤±æ•—: {e}")
            
        progress_bar.progress((i + 1) / len(files))
        
    return results

def find_report_start_page(pdf):
    for i in range(min(10, len(pdf.pages))):
        text = (pdf.pages[i].extract_text() or "").lower()
        if "test report" in text or "æ¸¬è©¦å ±å‘Š" in text:
            return i
    return 0

# =============================================================================
# 7. UI
# =============================================================================

st.set_page_config(page_title="SGS/CTI å ±å‘Šèšåˆå·¥å…· v63.26", layout="wide")
st.title("ğŸ“„ è¬ç”¨å‹æª¢æ¸¬å ±å‘Šèšåˆå·¥å…· (v63.26 é¦¬ä¾†è¥¿äºé‚è¼¯ç§»æ¤ç‰ˆ)")
st.info("ğŸ’¡ v63.26ï¼šé‡å°é¦¬ä¾†è¥¿äºå ±å‘Šé€²è¡Œã€Œå¤–ç§‘æ‰‹è¡“å¼ã€é‚è¼¯ç§»æ¤ï¼Œå®Œå…¨æ¨æ£„è¡¨æ ¼è§£æï¼Œæ”¹ç”¨æ‚¨é©—è­‰éæœ‰æ•ˆçš„æ–‡å­—æƒæèˆ‡ Regex è¦å‰‡ï¼›åŒæ™‚ç¢ºä¿æ¨™æº– SGS èˆ‡ CTI å¼•æ“å®Œå…¨ä¸å—å½±éŸ¿ã€‚")

uploaded_files = st.file_uploader("è«‹ä¸€æ¬¡é¸å–æ‰€æœ‰ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸ”„ é‡æ–°åŸ·è¡Œ"): st.rerun()

    try:
        result_data = process_files(uploaded_files)
        df = pd.DataFrame(result_data)
        df = df.reindex(columns=OUTPUT_COLUMNS)

        st.success("âœ… è™•ç†å®Œæˆï¼")
        st.dataframe(df)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Summary')
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel",
            data=output.getvalue(),
            file_name="SGS_CTI_Summary_v63.26.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
    except Exception as e:
        st.error(f"ç³»çµ±éŒ¯èª¤: {e}")
