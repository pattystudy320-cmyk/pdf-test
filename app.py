import streamlit as st
import pdfplumber
import pandas as pd
import re
from dateutil import parser
import io

st.set_page_config(page_title="SGS Report Parser", layout="wide")
st.title("ğŸ“„ SGS Report æª¢æ¸¬çµæœå½™ç¸½å·¥å…· (åº§æ¨™åˆ‡å‰²ç‰ˆ)")

# =========================
# 1. æ¸¬é …é—œéµå­—å®šç¾©
# =========================
# ç”¨ä¾†å®šä½ Y è»¸é«˜åº¦ (Row)
ITEM_KEYWORDS = {
    "Pb": "Lead",
    "Cd": "Cadmium",
    "Hg": "Mercury",
    "CrVI": "Hexavalent Chromium",
    "PBBs": "Sum of PBBs",
    "PBDEs": "Sum of PBDEs",
    "DEHP": "Di(2-ethylhexyl) phthalate",
    "BBP": "Benzyl butyl phthalate",
    "DBP": "Dibutyl phthalate",
    "DIBP": "Diisobutyl phthalate",
    "F": "Fluorine",
    "CL": "Chlorine",
    "BR": "Bromine",
    "I": "Iodine",
    "PFOS": "PFOS"
}

FINAL_COLUMNS = [
    "Pb", "Cd", "Hg", "CrVI", "PBBs", "PBDEs",
    "DEHP", "BBP", "DBP", "DIBP",
    "F", "CL", "BR", "I",
    "PFOS", "PFAS", "DATE"
]

# =========================
# 2. æ ¸å¿ƒåŠŸèƒ½ï¼šåº§æ¨™å®šä½èˆ‡åˆ‡å‰²
# =========================

def get_result_column_x_range(page):
    """
    æƒæé é¢æ¨™é¡Œï¼Œæ‰¾å‡º 'Result' æ¬„ä½çš„å·¦å³é‚Šç•Œ (Xè»¸ç¯„åœ)
    å›å‚³: (x0, x1) æˆ– None
    """
    words = page.extract_words()
    
    result_header = None
    mdl_header = None
    
    # å°‹æ‰¾è¡¨é ­é—œéµå­—
    for w in words:
        text = w["text"].strip()
        # æ‰¾ Result æ¨™é¡Œ
        if text == "Result" or text == "Result(s)":
            # æœ‰æ™‚å€™è¡¨é ­æœƒæœ‰å…©è¡Œï¼Œå–æœ€ä¸Šé¢çš„
            if result_header is None or w["top"] < result_header["top"]:
                result_header = w
        
        # æ‰¾ MDL æ¨™é¡Œ (ä½œç‚ºå³é‚Šç•Œ)
        if text == "MDL" or text == "LOQ":
            if mdl_header is None or w["top"] < mdl_header["top"]:
                mdl_header = w
    
    if result_header:
        x0 = result_header["x0"] - 5  # å·¦é‚Šç•Œç¨å¾®å¯¬ä¸€é»ï¼Œæ€•å°é½Šèª¤å·®
        
        # å¦‚æœæœ‰æ‰¾åˆ° MDLï¼Œå³é‚Šç•Œå°±æ˜¯ MDL çš„å·¦é‚Š
        if mdl_header:
            x1 = mdl_header["x0"] - 2 # ä¸è¦åœ¨é‚Šç•Œé‡ç–Šï¼Œç¨å¾®ç•™ç©º
        else:
            # æ²’æ‰¾åˆ° MDLï¼Œå°±å‡è¨­ä¸€å€‹å¯¬åº¦ (ä¾‹å¦‚ 80 å–®ä½)
            x1 = x0 + 80 
            
        return (x0, x1)
    
    return None

def extract_value_by_crop(page, keyword, x_range):
    """
    å·²çŸ¥ Result æ¬„ä½çš„ X ç¯„åœ (x_range)ï¼Œ
    æœå°‹ keyword (å¦‚ Cadmium) çš„ Y é«˜åº¦ï¼Œ
    ç„¶å¾Œåˆ‡å‰²å‡ºè©²å€åŸŸçš„æ–‡å­—ã€‚
    """
    if not x_range:
        return ""
    
    result_x0, result_x1 = x_range
    words = page.extract_words()
    
    # 1. æ‰¾åˆ°æ¸¬é …åç¨±çš„ Y åº§æ¨™
    target_row_top = None
    target_row_bottom = None
    
    for w in words:
        # ç°¡å–®æ¨¡ç³Šæ¯”å°ï¼šåªè¦æ¸¬é …é—œéµå­—å‡ºç¾åœ¨å­—è©ä¸­
        if keyword.lower() in w["text"].lower():
            # ç‚ºäº†é¿å…æŠ“åˆ°å…§æ–‡ï¼Œé€šå¸¸æ¸¬é …éƒ½åœ¨å·¦å´ (x < 300)
            if w["x0"] < 300:
                target_row_top = w["top"]
                target_row_bottom = w["bottom"]
                break # æ‰¾åˆ°å°±åœï¼Œå‡è¨­æ¸¬é …åç¨±åªå‡ºç¾ä¸€æ¬¡æˆ–å–ç¬¬ä¸€æ¬¡å‡ºç¾
    
    if target_row_top is not None:
        # 2. å®šç¾©åˆ‡å‰²æ¡† (Bounding Box)
        # (x0, top, x1, bottom)
        # Y è»¸ç¨å¾®æ”¾å¯¬ä¸€é» (+- 2)ï¼Œé¿å…åˆ‡åˆ°å­—
        crop_box = (
            result_x0, 
            target_row_top - 2, 
            result_x1, 
            target_row_bottom + 2
        )
        
        try:
            # 3. åŸ·è¡Œåˆ‡å‰²ä¸¦æŠ“å­—
            cropped_page = page.crop(crop_box)
            text = cropped_page.extract_text()
            return text.strip() if text else ""
        except Exception:
            # ç™¼ç”Ÿåˆ‡å‰²éŒ¯èª¤ (ä¾‹å¦‚åº§æ¨™è¶…å‡ºç¯„åœ)
            return ""

    return ""

def normalize_result(value):
    """
    æ¸…æ´—çµæœï¼šçµ±ä¸€ N.D.ï¼Œæ’é™¤å–®ä½
    """
    if not value:
        return ""
    
    val_str = str(value).strip()
    
    # ç§»é™¤å¸¸è¦‹å–®ä½èˆ‡é›œè¨Š
    val_str = re.sub(r"mg/kg|ppm|%|wt%", "", val_str, flags=re.IGNORECASE)
    
    # åˆ¤æ–· N.D. (åŒ…å« ND, N. D., Not Detected)
    # é€™è£¡ä½¿ç”¨å¯¬é¬†åˆ¤å®šï¼Œåªè¦æœ‰ N å’Œ D ä¸”éå–®å­—ä¸€éƒ¨åˆ†
    if re.search(r"(\bN\s*\.?\s*D\s*\.?\b)|(Not\s*Detected)", val_str, re.IGNORECASE):
        return "N.D."
    
    if "NEGATIVE" in val_str.upper():
        return "NEGATIVE"

    # æŠ“å–æ•¸å­— (æ”¯æ´å°æ•¸é»)
    match = re.search(r"\d+(\.\d+)?", val_str)
    if match:
        return match.group(0)
    
    return ""

def extract_pfas(text):
    return "REPORT" if re.search(r"\bPFAS\b", text, re.IGNORECASE) else ""

def extract_date(first_page_text):
    match = re.search(
        r"(REPORTED DATE|TEST REPORT REPORTED DATE)\s*[:\-]?\s*([^\n]+)",
        first_page_text,
        re.IGNORECASE
    )
    return match.group(2).strip() if match else ""

def normalize_date(date_text):
    if not date_text:
        return ""
    try:
        dt = parser.parse(date_text, dayfirst=True)
        return dt.strftime("%Y/%m/%d")
    except:
        return ""

def merge_results(values):
    nums = []
    has_nd = False
    has_neg = False

    for v in values:
        if not v: continue
        v_upper = str(v).upper()
        
        if "N.D." in v_upper:
            has_nd = True
        elif "NEGATIVE" in v_upper:
            has_neg = True
        else:
            try:
                nums.append(float(v))
            except:
                pass

    if nums:
        return str(max(nums))
    if has_neg:
        return "NEGATIVE"
    if has_nd:
        return "N.D."
    return ""

# =========================
# 3. ä¸»ç¨‹å¼æµç¨‹
# =========================

uploaded_files = st.file_uploader(
    "è«‹ä¸Šå‚³ SGS PDF Report (åº§æ¨™åˆ‡å‰²ç‰ˆ)",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    rows = []
    for file in uploaded_files:
        full_text = ""
        pages_text = []
        extracted_data = {key: [] for key in ITEM_KEYWORDS}
        
        with pdfplumber.open(file) as pdf:
            for page in pdf.pages:
                # 1. æ”¶é›†å…¨æ–‡ (çµ¦ PFAS å’Œ Date ç”¨)
                text = page.extract_text() or ""
                full_text += text + "\n"
                pages_text.append(text)
                
                # 2. åº§æ¨™åˆ‡å‰²é‚è¼¯
                # å…ˆæ‰¾å‡ºé€™é æœ‰æ²’æœ‰ Result æ¬„ä½
                x_range = get_result_column_x_range(page)
                
                if x_range:
                    # å¦‚æœé€™é æœ‰ Result è¡¨é ­ï¼Œå°±å»æœåˆ®å„å€‹æ¸¬é …
                    for item, keyword in ITEM_KEYWORDS.items():
                        # ç‰¹åˆ¥è™•ç† PBBs/PBDEs é€™ç¨®æ¨™é¡Œ
                        # é€™è£¡ä½¿ç”¨ç²¾ç¢ºé—œéµå­—å»å°æ‡‰é«˜åº¦
                        raw_val = extract_value_by_crop(page, keyword, x_range)
                        clean_val = normalize_result(raw_val)
                        if clean_val:
                            extracted_data[item].append(clean_val)
        
        # æ•´ç†å–®æª”çµæœ
        record = {}
        for item in ITEM_KEYWORDS:
            record[item] = merge_results(extracted_data[item])

        record["PFAS"] = extract_pfas(full_text)
        raw_date = extract_date(pages_text[0]) if pages_text else ""
        record["DATE"] = normalize_date(raw_date)

        rows.append(record)

    df_all = pd.DataFrame(rows)

    # å½™ç¸½é¡¯ç¤º
    merged = {}
    if not df_all.empty:
        for col in FINAL_COLUMNS:
            if col in ["DATE", "PFAS"]:
                continue
            if col in df_all.columns:
                merged[col] = merge_results(df_all[col].tolist())
            else:
                merged[col] = ""

        merged["PFAS"] = "REPORT" if "REPORT" in df_all["PFAS"].tolist() else ""
        
        valid_dates = [d for d in df_all["DATE"] if d]
        merged["DATE"] = max(valid_dates) if valid_dates else ""

        df_final = pd.DataFrame([merged], columns=FINAL_COLUMNS)

        st.subheader("ğŸ“Š å½™ç¸½çµæœï¼ˆåº§æ¨™åˆ‡å‰²ç‰ˆï¼‰")
        st.dataframe(df_final, use_container_width=True)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_final.to_excel(writer, sheet_name="SGS_Result", index=False)

        st.download_button(
            "â¬‡ï¸ ä¸‹è¼‰å…¬å¸åˆ¶å¼ Excel",
            output.getvalue(),
            file_name="SGS_Test_Result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.warning("æœªè®€å–åˆ°æœ‰æ•ˆè³‡æ–™ã€‚")
