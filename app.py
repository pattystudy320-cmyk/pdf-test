import pdfplumber
import os
import pandas as pd
import re
import io
import streamlit as st # ç¢ºä¿æ‚¨æœ‰å¼•å…¥ streamlit

class ReportParserV55:
    def __init__(self):
        # 1. å®šç¾©ç›®æ¨™åŒ–å­¸ç‰©è³ªé—œéµå­— (Regex æ”¯æ´æ¨¡ç³ŠåŒ¹é…)
        # å¢åŠ äº† PBBs, PBDEs, PAEs (DEHP, DBP, BBP, DIBP), é¹µç´  (F, Cl, Br)
        self.target_map = {
            'Pb': r'Lead|Pb|é‰›',
            'Cd': r'Cadmium|Cd|é˜',
            'Hg': r'Mercury|Hg|æ±',
            'Cr6+': r'Hexavalent Chromium|Cr\(VI\)|å…­åƒ¹é‰»',
            'PBB': r'Polybrominated Biphenyls?|PBBs?', # è™•ç†ç¾¤çµ„åç¨±
            'PBDE': r'Polybrominated Diphenyl Ethers?|PBDEs?', # è™•ç†ç¾¤çµ„åç¨±
            'DEHP': r'Di\(2-ethylhexyl\) phthalate|DEHP|é„°è‹¯äºŒç”²é…¸äºŒ\(2-ä¹™åŸºå·±åŸº\)é…¯',
            'DBP': r'Dibutyl phthalate|DBP|é„°è‹¯äºŒç”²é…¸äºŒä¸é…¯',
            'BBP': r'Butyl benzyl phthalate|BBP|é„°è‹¯äºŒç”²é…¸ä¸åŸºè‹¯ç”²é…¯',
            'DIBP': r'Diisobutyl phthalate|DIBP|é„°è‹¯äºŒç”²é…¸äºŒç•°ä¸é…¯',
            'F': r'Fluorine|F|æ°Ÿ',
            'Cl': r'Chlorine|Cl|æ°¯',
            'Br': r'Bromine|Br|æº´',
            'PFOA': r'Perfluorooctanoic acid|PFOA',
            'PFOS': r'Perfluorooctane sulfonic acid|PFOS',
            'PFAS_General': r'Total Fluorine|PFAS'
        }
        # å¸¸è¦‹å–®ä½ï¼Œç”¨ä¾†ä½œç‚ºå®šä½éŒ¨é»
        self.unit_keywords = ['mg/kg', 'ppm', 'ug/l', 'wt%', '%']
        # æ‰€æœ‰éœ€è¦çš„æ¬„ä½åç¨±ï¼Œç”¨æ–¼ç¢ºä¿ DataFrame é †åº
        self.all_fields = ['æª”æ¡ˆåç¨±', 'å¯¦é©—å®¤', 'DATE', 'Pb', 'Cd', 'Hg', 'Cr6+', 'PBB', 'PBDE', 'DEHP', 'DBP', 'BBP', 'DIBP', 'F', 'Cl', 'Br', 'PFOA', 'PFOS', 'PFAS_General']

    def clean_text(self, text):
        if not text: return ""
        return str(text).replace('\n', ' ').strip()

    def is_valid_result(self, value):
        """åˆ¤æ–·æ˜¯å¦ç‚ºæª¢æ¸¬çµæœ (å« ND æˆ– æœ‰æ•ˆæ•¸å­—)"""
        if not value: return False
        val = str(value).replace(' ', '').upper()
        
        # å…è¨±çš„éæ•¸å­—çµæœ
        if val in ['ND', 'N.D.', 'NEGATIVE', 'NOTDETECTED', 'N.D']: return True
        
        try:
            # ç§»é™¤ < æˆ– > ç¬¦è™Ÿå¾Œåˆ¤æ–·æ˜¯å¦ç‚ºæ•¸å­—
            val_clean = val.replace('<', '').replace('>', '')
            num = float(val_clean)
            
            # æ’é™¤å¸¸è¦‹çš„ MDL æˆ– Limit æ•¸å­—å¹²æ“¾ (å¯è¦–éœ€æ±‚èª¿æ•´)
            if val_clean in ['2', '5', '8', '10', '50', '100', '1000']: 
                return False 
            return True
        except ValueError:
            return False

    def get_dynamic_column_indices(self, table_header):
        """
        [å„ªåŒ–é‡é»]ï¼šå‹•æ…‹å°‹æ‰¾æ•¸æ“šæ‰€åœ¨æ¬„ä½ç´¢å¼•
        ä¸åªæ‰¾ 'Result'ï¼Œä¹Ÿæ‰¾ '001', 'A1' æˆ–ä½åœ¨ 'Unit' ä¹‹å¾Œçš„æ¬„ä½
        """
        indices = []
        sample_id_pattern = re.compile(r'^(NO\.)?\d{2,3}$|^[A-Z]\d{1,2}$|^RESULT', re.I)
        
        unit_idx = -1
        for i, cell in enumerate(table_header):
            cell_txt = self.clean_text(cell).upper()
            
            # 1. åŒ¹é…æ¨£å“ç·¨è™Ÿ (å¦‚ 001, A1) æˆ–æ¨™é¡Œ Result
            if sample_id_pattern.search(cell_txt):
                indices.append(i)
            
            # 2. è¨˜éŒ„ Unit å–®ä½æ¬„ä½ç½®
            if any(u.upper() in cell_txt for u in self.unit_keywords):
                unit_idx = i
        
        # 3. å¦‚æœæ²’æ‰¾åˆ°æ¨£å“ç·¨è™Ÿï¼Œé€šå¸¸ Unit çš„ä¸‹ä¸€æ¬„å°±æ˜¯çµæœ
        if not indices and unit_idx != -1:
            indices.append(unit_idx + 1)
            
        return sorted(list(set(indices)))

    def parse_smart_table(self, tables):
        """é€šç”¨æ™ºèƒ½è§£æé‚è¼¯"""
        # åˆå§‹åŒ–å­—å…¸ï¼Œç¢ºä¿æ‰€æœ‰æ¬„ä½éƒ½åœ¨
        data = {k: "" for k in self.target_map.keys()}
        
        for table in tables:
            if not table or len(table) < 2: continue
            
            # é å…ˆæ¸…ç†æ•´å¼µè¡¨
            clean_table = [[self.clean_text(cell) for cell in row] for row in table]
            
            # ç¬¬ä¸€æ­¥ï¼šå˜—è©¦å¾å‰å…©è¡Œæ‰¾å‡ºæ•¸æ“šæ¬„ä½ç´¢å¼• (Index)
            data_cols = self.get_dynamic_column_indices(clean_table[0])
            if not data_cols and len(clean_table) > 1:
                data_cols = self.get_dynamic_column_indices(clean_table[1])

            # ç¬¬äºŒæ­¥ï¼šé€è¡Œæ¯”å°é—œéµå­—
            for row in clean_table:
                row_str = " ".join(row).upper()
                
                for key, pattern in self.target_map.items():
                    # é¿å… PFAS èª¤æŠ“ PFOA/PFOS
                    if key == 'PFAS_General' and ('PFOA' in row_str or 'PFOS' in row_str):
                        continue

                    # è‹¥åŒ¹é…åˆ°é—œéµå­—ä¸”è©²é …å°šæœªæœ‰å€¼
                    if re.search(pattern, row_str, re.I) and data[key] == "":
                        found_val = ""
                        
                        # å„ªå…ˆå¾å®šä½åˆ°çš„æ¬„ä½å–å€¼
                        for idx in data_cols:
                            if idx < len(row) and self.is_valid_result(row[idx]):
                                found_val = row[idx]
                                break
                        
                        # è‹¥å®šä½å¤±æ•—ï¼Œå›é€€è‡³ç”±å¾Œå¾€å‰æœå°‹
                        if not found_val:
                            for cell in reversed(row):
                                if self.is_valid_result(cell):
                                    found_val = cell
                                    break
                        
                        data[key] = found_val
        return data

    def process_file(self, file_path):
        filename = os.path.basename(file_path)
        try:
            with pdfplumber.open(file_path) as pdf:
                first_page_text = pdf.pages[0].extract_text() or ""
                lab_type = "SGS/CTI"
                if "INTERTEK" in first_page_text.upper(): lab_type = "INTERTEK"
                
                all_tables = []
                for page in pdf.pages:
                    tables = page.extract_tables()
                    if tables: all_tables.extend(tables)
                
                extracted_data = self.parse_smart_table(all_tables)

                # --- æ–°å¢ï¼šæå–å ±å‘Šæ—¥æœŸ ---
                report_date = ""
                # å˜—è©¦ç”¨æ­£å‰‡è¡¨é”å¼å°‹æ‰¾æ—¥æœŸ (å¸¸è¦‹æ ¼å¼ YYYY-MM-DD æˆ– DD MMM YYYY)
                date_match = re.search(r'\d{1,4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,4}æ—¥?', first_page_text)
                if date_match:
                    report_date = date_match.group(0).replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")

                # ç¢ºä¿çµæœå­—å…¸åŒ…å«æ‰€æœ‰éœ€è¦çš„æ¬„ä½
                result = {field: "" for field in self.all_fields}
                result.update({"æª”æ¡ˆåç¨±": filename, "å¯¦é©—å®¤": lab_type, "DATE": report_date})
                # å°‡è§£æåˆ°çš„æ•¸æ“šåˆä½µ
                for key, value in extracted_data.items():
                    if key in result:
                        result[key] = value

                return result
                
        except Exception as e:
            # éŒ¯èª¤ç™¼ç”Ÿæ™‚ï¼Œå›å‚³åŒ…å«éŒ¯èª¤è¨Šæ¯çš„å­—å…¸ï¼ŒåŒæ™‚ç¢ºä¿æ¬„ä½é½Šå…¨
            error_dict = {field: "" for field in self.all_fields}
            error_dict.update({"æª”æ¡ˆåç¨±": filename, "å¯¦é©—å®¤": "Error", "Pb": f"éŒ¯èª¤: {str(e)}"})
            return error_dict

# ==========================================
# Streamlit åŸ·è¡Œå€
# ==========================================
if __name__ == "__main__":
    st.title("ğŸ“„ åŒ–å­¸åˆ†æå ±å‘Š PDF è‡ªå‹•è§£æå™¨ (V55.0)")
    st.write("æœ¬å·¥å…·æ”¯æ´SGSã€Intertekã€CTIå ±å‘Šï¼Œè«‹ä¸Šå‚³ PDF æª”æ¡ˆä»¥é–‹å§‹åˆ†æã€‚")

    # å»ºç«‹ä¸Šå‚³å…ƒä»¶
    uploaded_files = st.file_uploader("é¸æ“‡ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

    if uploaded_files:
        parser = ReportParserV55()
        all_results = []
        
        progress_bar = st.progress(0)
        for i, uploaded_file in enumerate(uploaded_files):
            # å°‡ä¸Šå‚³çš„æª”æ¡ˆæš«å­˜åˆ°æœ¬åœ°ä»¥ä¾¿è®€å–
            with open(uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            res = parser.process_file(uploaded_file.name)
            all_results.append(res)
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.progress((i + 1) / len(uploaded_files))
            # åˆªé™¤æš«å­˜æª”
            os.remove(uploaded_file.name)

        df = pd.DataFrame(all_results)
        
        # ç¢ºä¿ Excel æ¬„ä½é †åºæ­£ç¢º
        df = df[[c for c in parser.all_fields if c in df.columns]]

        # é¡¯ç¤ºçµæœé è¦½
        st.subheader("ğŸ“Š è§£æçµæœé è¦½")
        st.dataframe(df)

        # è£½ä½œ Excel ä¸‹è¼‰æŒ‰éˆ•
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False)
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel çµæœæª”",
            data=output.getvalue(),
            file_name="Analysis_Results.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
