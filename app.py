import streamlit as st
import pdfplumber
import pandas as pd
import re
from dateutil import parser
import io

st.set_page_config(page_title="SGS Report Parser", layout="wide")
st.title("ğŸ“„ SGS Report æª¢æ¸¬çµæœå½™ç¸½å·¥å…· (ä¿®æ­£ç‰ˆ)")

# =========================
# æ¬„ä½å®šç¾©ï¼ˆé †åºå›ºå®šï¼‰
# =========================
ITEM_RULES = {
    "Pb": r"Lead\s*\(Pb\)",
    "Cd": r"Cadmium\s*\(Cd\)",
    "Hg": r"Mercury\s*\(Hg\)",
    "CrVI": r"Hexavalent Chromium",
    "PBBs": r"Sum of PBBs",
    "PBDEs": r"Sum of PBDEs",
    "DEHP": r"DEHP",
    "BBP": r"BBP",
    "DBP": r"DBP",
    "DIBP": r"DIBP",
    "F": r"Fluorine",
    "CL": r"Chlorine",
    "BR": r"Bromine",
    "I": r"Iodine",
    "PFOS": r"PFOS"
}

FINAL_COLUMNS = [
    "Pb","Cd","Hg","CrVI","PBBs","PBDEs",
    "DEHP","BBP","DBP","DIBP",
    "F","CL","BR","I",
    "PFOS","PFAS","DATE"
]

# =========================
# å·¥å…·å‡½å¼
# =========================
def extract_text_and_pages(pdf_file):
    full_text = ""
    pages_text = []
    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            pages_text.append(text)
            full_text += text + "\n"
    return full_text, pages_text


def extract_result(text, keyword):
    """
    ä¿®æ­£å¾Œçš„é‚è¼¯ï¼š
    1. å…ˆæŠ“å–é—œéµå­—é™„è¿‘çš„æ–‡å­—ã€‚
    2. å¼·åŠ›æ¸…é™¤ CAS No (é¿å…æŠ“åˆ° 117-81-7 çš„ 117)ã€‚
    3. å¼·åŠ›æ¸…é™¤ IEC æ¨™æº–èˆ‡å¹´ä»½ (é¿å…æŠ“åˆ° 2017)ã€‚
    4. å„ªå…ˆåˆ¤æ–· N.D. (é¿å…æŠ“åˆ°å¾Œé¢çš„ MDL æ•¸å€¼å¦‚ 2, 5, 50)ã€‚
    5. æœ€å¾Œæ‰æŠ“å–æ•¸å€¼ã€‚
    """
    lines = text.splitlines()

    for i, line in enumerate(lines):
        # æ‰¾åˆ°é—œéµå­—ï¼ˆä¾‹å¦‚ Lead, Cadmiumï¼‰
        if re.search(keyword, line, re.IGNORECASE):
            # æŠ“å–ä¸Šä¸‹æ–‡ï¼ˆç•¶è¡Œ + å¾Œå…©è¡Œï¼‰åˆä½µè™•ç†ï¼Œè™•ç†è·¨è¡Œå•é¡Œ
            context = " ".join(lines[i:i+3])

            # --- æ­¥é©Ÿ 1: æ¸…é™¤å¹²æ“¾é›œè¨Š (é †åºå¾ˆé‡è¦) ---
            
            # (A) æ¸…é™¤ CAS No. (ä¾‹å¦‚ "(CAS No. 117-81-7)"ï¼Œä¸€å®šè¦åœ¨æ‰¾æ•¸å­—å‰åˆªæ‰)
            context = re.sub(r"\(?CAS\s*No\.?\s*[\d-]+\)?", " ", context, flags=re.IGNORECASE)

            # (B) æ¸…é™¤ IEC æ¨™æº–ç·¨è™Ÿèˆ‡å¹´ä»½ (ä¾‹å¦‚ "IEC 62321-4:2013+A1:2017")
            # é€™æœƒæŠŠ 2017 é€™ç¨®å¹´ä»½æ¸…æ‰ï¼Œé¿å…æ± (Hg) æŠ“éŒ¯
            context = re.sub(r"IEC\s*62321[-\d:+A]*", " ", context, flags=re.IGNORECASE)

            # (C) æ¸…é™¤ Limit é™åˆ¶å€¼ (ä¾‹å¦‚ "Max 1000")
            context = re.sub(r"Max\s*\d+", " ", context, flags=re.IGNORECASE)

            # --- æ­¥é©Ÿ 2: å„ªå…ˆåˆ¤æ–·çµæœç‹€æ…‹ ---

            # å…ˆæ‰¾ N.D. (Not Detected)
            # åªè¦çœ‹åˆ° N.D. å°±ç›´æ¥å›å‚³ï¼Œé€™æ¨£å°±ä¸æœƒå»æŠ“å¾Œé¢çš„ MDL (ä¾‹å¦‚ 2, 5, 50)
            if re.search(r"\bN\.?D\.?\b", context, re.IGNORECASE):
                return "N.D."

            # å†æ‰¾ NEGATIVE
            if re.search(r"NEGATIVE", context, re.IGNORECASE):
                return "NEGATIVE"

            # --- æ­¥é©Ÿ 3: æœ€å¾Œæ‰æŠ“æ•¸å­— ---
            
            # å°‹æ‰¾å‰©ä¸‹çš„æ•¸å­— (åŒ…å«å°æ•¸é»)
            num = re.search(r"\b(\d+(\.\d+)?)\b", context)
            if num:
                value_str = num.group(1)
                
                # é˜²å‘†æ©Ÿåˆ¶ï¼šéæ¿¾æ‰åƒå¹´ä»½çš„æ•´æ•¸ (ä¾‹å¦‚ 2024, 2025)
                # å¦‚æœæŠ“åˆ°çš„æ•¸å­—æ˜¯æ•´æ•¸ï¼Œä¸”åœ¨ 1990-2030 ä¹‹é–“ï¼Œå¾ˆå¯èƒ½æ˜¯æ¼ç¶²ä¹‹é­šçš„å¹´ä»½
                try:
                    val_float = float(value_str)
                    if 1990 <= val_float <= 2030 and val_float.is_integer():
                        continue # è·³éé€™å€‹æ•¸å­—ï¼Œå¯èƒ½æ˜¯å¹´ä»½
                except:
                    pass
                
                return value_str

    return ""


def extract_pfas(text):
    # åªè¦å…§æ–‡å‡ºç¾ PFAS é—œéµå­—ï¼Œå°±è¦–ç‚ºæœ‰æ¸¬
    return "REPORT" if re.search(r"\bPFAS\b", text, re.IGNORECASE) else ""


def extract_date(first_page_text):
    match = re.search(
        r"(REPORTED DATE|TEST REPORT REPORTED DATE)\s*[:\-]?\s*([^\n]+)",
        first_page_text,
        re.IGNORECASE
    )
    return match.group(2).strip() if match else ""


def normalize_date(date_text):
    if not date_text:
        return ""
    try:
        # å˜—è©¦è§£ææ—¥æœŸæ ¼å¼
        dt = parser.parse(date_text, dayfirst=True)
        return dt.strftime("%Y/%m/%d")
    except:
        return ""


def merge_results(values):
    """
    å½™ç¸½é‚è¼¯ï¼š
    1. å¦‚æœæœ‰å¤šå€‹æ•¸å€¼ï¼Œå–æœ€å¤§å€¼ã€‚
    2. å¦‚æœæœ‰ N.D. æˆ– NEGATIVEï¼Œå„ªå…ˆç´šä½æ–¼æ•¸å€¼ã€‚
    """
    nums = []
    has_nd = False
    has_neg = False

    for v in values:
        if not v:
            continue
        
        v_upper = str(v).upper()
        
        if "N.D." in v_upper:
            has_nd = True
        elif "NEGATIVE" in v_upper:
            has_neg = True
        else:
            try:
                nums.append(float(v))
            except:
                pass

    if nums:
        return str(max(nums)) # æœ‰æ•¸å€¼å›å‚³æœ€å¤§å€¼
    if has_neg:
        return "NEGATIVE"
    if has_nd:
        return "N.D."
    
    return ""


# =========================
# ä¸»æµç¨‹ (UI)
# =========================
uploaded_files = st.file_uploader(
    "è«‹ä¸Šå‚³ SGS PDF Reportï¼ˆå¯ä¸€æ¬¡å¤šé¸ï¼‰",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    rows = []

    for file in uploaded_files:
        # è®€å– PDF
        full_text, pages_text = extract_text_and_pages(file)

        record = {}

        # é€ä¸€æŠ“å–å„æª¢æ¸¬é …ç›®
        for item, keyword in ITEM_RULES.items():
            record[item] = extract_result(full_text, keyword)

        # PFAS ç‰¹åˆ¥è™•ç†ï¼ˆæ˜¯å¦æœ‰æ¸¬ï¼‰
        record["PFAS"] = extract_pfas(full_text)

        # DATEï¼ˆåªçœ‹ç¬¬ä¸€é ï¼‰
        raw_date = extract_date(pages_text[0]) if pages_text else ""
        record["DATE"] = normalize_date(raw_date)

        rows.append(record)

    df_all = pd.DataFrame(rows)

    # ===== åŒæ‰¹ PDF å½™ç¸½ï¼ˆæœ€åš´æ ¼çµæœï¼‰=====
    merged = {}
    if not df_all.empty:
        for col in FINAL_COLUMNS:
            if col in ["DATE", "PFAS"]:
                continue
            # ç¢ºä¿æ¬„ä½å­˜åœ¨ï¼Œé¿å…å ±éŒ¯
            if col in df_all.columns:
                merged[col] = merge_results(df_all[col].tolist())
            else:
                merged[col] = ""

        # PFAS: åªè¦æœ‰ä¸€ä»½æ˜¯ REPORTï¼Œçµæœå°±æ˜¯ REPORT
        merged["PFAS"] = "REPORT" if "REPORT" in df_all["PFAS"].tolist() else ""
        
        # DATE: å–æ—¥æœŸæœ€å¤§å€¼ (æœ€æ–°çš„æ—¥æœŸ)
        valid_dates = [d for d in df_all["DATE"] if d]
        merged["DATE"] = max(valid_dates) if valid_dates else ""

        df_final = pd.DataFrame([merged], columns=FINAL_COLUMNS)

        # ===== é¡¯ç¤ºçµæœ =====
        st.subheader("ğŸ“Š å½™ç¸½çµæœï¼ˆåŒæ‰¹ SGS Reportï¼‰")
        st.dataframe(df_final, use_container_width=True)

        # ===== Excel åŒ¯å‡º =====
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_final.to_excel(writer, sheet_name="SGS_Result", index=False)

        st.download_button(
            "â¬‡ï¸ ä¸‹è¼‰å…¬å¸åˆ¶å¼ Excel",
            output.getvalue(),
            file_name="SGS_Test_Result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.warning("æœªåµæ¸¬åˆ°ä»»ä½•è³‡æ–™ï¼Œè«‹æª¢æŸ¥ PDF å…§å®¹ã€‚")
