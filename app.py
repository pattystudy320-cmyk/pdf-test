import streamlit as st
import pdfplumber
import pandas as pd
import io
import re
from datetime import datetime

# =============================================================================
# 1. å…±ç”¨è¨­å®šèˆ‡åŸºç¤å‡½å¼
# =============================================================================

OUTPUT_COLUMNS = [
    "Pb", "Cd", "Hg", "Cr6+", "PBB", "PBDE", 
    "DEHP", "BBP", "DBP", "DIBP", 
    "PFOS", "PFAS", "F", "CL", "BR", "I", 
    "æ—¥æœŸ", "æª”æ¡ˆåç¨±"
]

SIMPLE_KEYWORDS = {
    "Pb": ["Lead", "é‰›", "Pb"],
    "Cd": ["Cadmium", "é˜", "Cd"],
    "Hg": ["Mercury", "æ±", "Hg"],
    "Cr6+": ["Hexavalent Chromium", "å…­åƒ¹é‰»", "Cr(VI)", "Chromium VI"],
    "DEHP": ["DEHP", "Di(2-ethylhexyl) phthalate"],
    "BBP": ["BBP", "Butyl benzyl phthalate"],
    "DBP": ["DBP", "Dibutyl phthalate"],
    "DIBP": ["DIBP", "Diisobutyl phthalate"],
    "PFOS": ["Perfluorooctane sulfonates", "Perfluorooctane sulfonate", "PFOS"],
    "F": ["Fluorine", "æ°Ÿ"],
    "CL": ["Chlorine", "æ°¯"],
    "BR": ["Bromine", "æº´"],
    "I": ["Iodine", "ç¢˜"]
}

GROUP_KEYWORDS = {
    "PBB": ["Polybrominated Biphenyls", "PBBs", "Sum of PBBs", "å¤šæº´è¯è‹¯"],
    "PBDE": ["Polybrominated Diphenyl Ethers", "PBDEs", "Sum of PBDEs", "å¤šæº´äºŒè‹¯é†š"]
}

def clean_text(text):
    if not text: return ""
    return str(text).replace('\n', ' ').strip()

def is_valid_date(dt):
    if 2000 <= dt.year <= 2030: return True
    return False

def extract_date_general(text):
    """é€šç”¨æ—¥æœŸæå– (é©ç”¨æ–¼æ¨™æº–å ±å‘Š)"""
    lines = text.split('\n')
    candidates = []
    
    # é—œéµå­—åŠ åˆ†
    bonus_kw = ["date:", "dated", "æ—¥æœŸ"]
    # é—œéµå­—æ‰£åˆ† (æ’é™¤æ”¶ä»¶æ—¥ã€åˆ°æœŸæ—¥)
    poison_kw = ["received", "expiry", "period", "started", "checked", "approved"]

    pat_ymd = r"(20\d{2})[\./-](0?[1-9]|1[0-2])[\./-](0?[1-9]|[12][0-9]|3[01])"
    pat_dmy = r"(0?[1-9]|[12][0-9]|3[01])[\s-]([a-zA-Z]{3,})[\s-](20\d{2})"
    
    for line in lines:
        line_lower = line.lower()
        score = 1
        if any(p in line_lower for p in poison_kw): score = -10
        if any(b in line_lower for b in bonus_kw): score = 10
        
        # ç°¡æ˜“æ¸…æ´—
        clean = line.replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")
        
        # åŒ¹é… YMD
        matches = re.finditer(pat_ymd, clean)
        for m in matches:
            try:
                dt = datetime.strptime(f"{m.group(1)}-{m.group(2)}-{m.group(3)}", "%Y-%m-%d")
                if is_valid_date(dt): candidates.append((score, dt))
            except: pass
            
        # åŒ¹é… DMY (25-Aug-2025)
        matches = re.finditer(pat_dmy, line)
        for m in matches:
            try:
                dt_str = f"{m.group(1)} {m.group(2)} {m.group(3)}"
                for fmt in ["%d %b %Y", "%d %B %Y"]:
                    try:
                        dt = datetime.strptime(dt_str, fmt)
                        if is_valid_date(dt): 
                            candidates.append((score, dt))
                            break
                    except: pass
            except: pass

    if not candidates: return None
    # å–æœ€é«˜åˆ†ä¸”æœ€æ–°çš„æ—¥æœŸ
    candidates.sort(key=lambda x: (x[0], x[1]), reverse=True)
    return candidates[0][1]

def extract_date_malaysia(text):
    """é¦¬ä¾†è¥¿äºå°ˆç”¨æ—¥æœŸæå– (é–å®š REPORTED DATE)"""
    lines = text.split('\n')
    for line in lines:
        if "REPORTED DATE" in line.upper():
            # æ’é™¤ Job Ref å¹²æ“¾
            if "JOB REF" in line.upper(): continue
            
            # æ ¼å¼: 23-January-2025 æˆ– 23 Jan 2025
            pat = r"(0?[1-9]|[12][0-9]|3[01])[\s-]([a-zA-Z]{3,})[\s-](20\d{2})"
            match = re.search(pat, line)
            if match:
                dt_str = f"{match.group(1)} {match.group(2)} {match.group(3)}"
                for fmt in ["%d %B %Y", "%d %b %Y"]:
                    try:
                        dt = datetime.strptime(dt_str, fmt)
                        if is_valid_date(dt): return dt
                    except: pass
    return None

def extract_value_std(val_str):
    """æ¨™æº–æ•¸å€¼æ¸…æ´—"""
    val = clean_text(val_str).lower()
    if not val: return None
    if val in ["-", "---", "n.a.", "/"]: return None
    
    if "n.d." in val or "not detected" in val or "negative" in val or "<" in val:
        return "N.D."
    
    # æå–æ•¸å­—
    match = re.search(r"^\d+(\.\d+)?", val.replace("mg/kg","").strip())
    if match:
        try:
            f = float(match.group(0))
            # æ’é™¤å¸¸è¦‹ MDL/Limit
            if f in [2.0, 5.0, 10.0, 50.0, 100.0, 1000.0]: return None 
            return match.group(0)
        except: pass
    return val # Return original if unsure

# =============================================================================
# 2. å¼•æ“ A: æ¨™æº–å¼•æ“ (åŸºæ–¼ v60.3) - ç”¨æ–¼ å°ç£/ä¸­åœ‹/å…¶ä»–
# =============================================================================

def process_standard(pdf, filename):
    data = {k: [] for k in OUTPUT_COLUMNS}
    data["æª”æ¡ˆåç¨±"] = filename
    full_text = ""
    
    # æ—¥æœŸæå–
    dates = []
    for p in pdf.pages[:3]:
        txt = p.extract_text() or ""
        full_text += txt + "\n"
        dt = extract_date_general(txt)
        if dt: dates.append(dt)
    
    if dates:
        data["æ—¥æœŸ"] = max(dates).strftime("%Y/%m/%d")

    # è¡¨æ ¼æ¨¡å¼
    for page in pdf.pages:
        tables = page.extract_tables()
        for table in tables:
            if not table or len(table) < 2: continue
            
            # ç°¡å–®æ¬„ä½å®šä½ (Item vs Result)
            item_idx, result_idx = -1, -1
            
            # æƒæè¡¨é ­
            headers = [str(x).lower() for x in table[0] if x]
            for i, h in enumerate(headers):
                if "item" in h or "é …ç›®" in h: item_idx = i
                if "result" in h or "çµæœ" in h: result_idx = i
            
            # å¦‚æœæ‰¾ä¸åˆ° Resultï¼Œå˜—è©¦æ‰¾ MDL å³é‚Š (æ¨™æº–é‚è¼¯)
            if result_idx == -1:
                for i, h in enumerate(headers):
                    if "mdl" in h or "loq" in h:
                        if i + 1 < len(headers): result_idx = i + 1
                        break
            
            if item_idx == -1: continue # é€£é …ç›®æ¬„éƒ½æ‰¾ä¸åˆ°å°±è·³é

            for row in table[1:]:
                if len(row) <= max(item_idx, result_idx): continue
                
                item_name = clean_text(row[item_idx])
                item_lower = item_name.lower()
                
                # æ’é™¤æœ‰æ©Ÿæ°Ÿèª¤åˆ¤ (SGS_4 ä¿®å¾©é—œéµ)
                if "aminium" in item_lower or "piperazine" in item_lower or "sulfonate" in item_lower:
                    # é™¤éæ˜ç¢ºå¯«äº† Fluorine (ç„¡é¹µ)
                    if "fluorine" not in item_lower: continue

                # æŠ“å–æ•¸å€¼
                raw_val = ""
                if result_idx != -1:
                    raw_val = row[result_idx]
                else:
                    # æƒæè¡Œå°¾
                    for cell in reversed(row):
                        if cell and ("n.d." in str(cell).lower() or re.match(r"\d+", str(cell))):
                            raw_val = cell
                            break
                
                val = extract_value_std(raw_val)
                if not val: continue

                # åŒ¹é…æ¬„ä½
                for key, kws in SIMPLE_KEYWORDS.items():
                    if key == "F" and "perfluoro" in item_lower: continue # å†æ¬¡é˜²ç¦¦
                    
                    if any(kw.lower() in item_lower for kw in kws):
                        data[key].append(val)
                        break
                
                for key, kws in GROUP_KEYWORDS.items():
                    if any(kw.lower() in item_lower for kw in kws):
                        data[key].append(val)
                        break

    # æ–‡å­—æ¨¡å¼æ•‘æ´ (åƒ…é‡å°ç„¡é¹µ/PFOA)
    # v60.3 çš„ä¿å®ˆæ•‘æ´: åªæœ‰ç•¶è¡¨æ ¼å®Œå…¨æ²’æŠ“åˆ°æ™‚æ‰å•Ÿå‹•ï¼Œä¸”ä¸ä½¿ç”¨å¯¬é¬†åŒ¹é…
    ft_lower = full_text.lower()
    
    # ç„¡é¹µæ•‘æ´
    if not data["F"] and ("halogen" in ft_lower or "å¤ç´ " in ft_lower):
        # ç°¡å–®è¡Œæƒæ
        for line in full_text.split('\n'):
            l_lower = line.lower()
            if "fluorine" in l_lower and "n.d." in l_lower:
                data["F"].append("N.D.")
            if "chlorine" in l_lower and "n.d." in l_lower:
                data["CL"].append("N.D.")
            if "bromine" in l_lower and "n.d." in l_lower:
                data["BR"].append("N.D.")
            if "iodine" in l_lower and "n.d." in l_lower:
                data["I"].append("N.D.")

    return data

# =============================================================================
# 3. å¼•æ“ B: é¦¬ä¾†è¥¿äºå°ˆç”¨å¼•æ“ (v61.0 æš´åŠ›ç‰ˆ)
# =============================================================================

def process_malaysia(pdf, filename):
    data = {k: [] for k in OUTPUT_COLUMNS}
    data["æª”æ¡ˆåç¨±"] = filename
    
    full_text = ""
    for p in pdf.pages:
        full_text += (p.extract_text() or "") + "\n"
        
    # 1. æ—¥æœŸæå–
    dt = extract_date_malaysia(full_text)
    if dt: data["æ—¥æœŸ"] = dt.strftime("%Y/%m/%d")

    # 2. RoHS2 (è¡¨æ ¼ MDL éŒ¨é»æ³•)
    for page in pdf.pages:
        tables = page.extract_tables()
        for table in tables:
            if not table or len(table) < 2: continue
            
            # å°‹æ‰¾ MDL æ¬„ä½ (ç‰¹å¾µ: å…¨æ˜¯æ•¸å­—)
            mdl_col_idx = -1
            cols = len(table[0])
            for c in range(cols):
                num_count = 0
                total_count = 0
                for r in range(1, len(table)): # è·³éæ¨™é¡Œ
                    val = clean_text(table[r][c])
                    if not val: continue
                    total_count += 1
                    # æª¢æŸ¥æ˜¯å¦ç‚ºå¸¸è¦‹ MDL æ•¸å­—
                    if val in ["2", "5", "8", "50", "100"]:
                        num_count += 1
                
                if total_count > 0 and (num_count / total_count) > 0.5:
                    mdl_col_idx = c
                    break # æ‰¾åˆ°ä¸€å€‹åƒ MDL çš„å°±åœ
            
            # å¦‚æœæ‰¾åˆ° MDLï¼ŒResult å°±åœ¨å·¦é‚Š (MDL-1)
            if mdl_col_idx > 0:
                result_col_idx = mdl_col_idx - 1
                
                # é–‹å§‹æš´åŠ›æŠ“å–
                for row in table:
                    if len(row) <= mdl_col_idx: continue
                    
                    # åˆ¤æ–· Item (é€šå¸¸åœ¨ç¬¬ 0 æ¬„ï¼Œä½†ä¹Ÿå¯èƒ½è·Ÿ Method é»åœ¨ä¸€èµ·)
                    # ç­–ç•¥: æŠŠ row[0] åˆ° row[result_col_idx-1] å…¨éƒ¨åˆèµ·ä¾†ç•¶ Item Description
                    item_text = " ".join([str(x) for x in row[:result_col_idx] if x]).lower()
                    
                    # æå– Result æ¬„ä½çš„å…§å®¹
                    raw_res = str(row[result_col_idx])
                    
                    # å¼·åŠ› Regex æ¸…æ´— (å–å‡º N.D. æˆ–æ•¸å€¼)
                    # æ’é™¤ 62321, IEC ç­‰æ–¹æ³•ç·¨è™Ÿ
                    final_val = None
                    
                    # å„ªå…ˆæ‰¾ N.D.
                    if re.search(r"(?i)\bn\.?d\.?", raw_res):
                        final_val = "N.D."
                    else:
                        # æ‰¾æ•¸å­— (æ’é™¤æ–¹æ³•ç·¨è™Ÿ)
                        nums = re.findall(r"\d+(?:\.\d+)?", raw_res)
                        for num in nums:
                            if num in ["62321", "2013", "2015", "2017"]: continue # æ’é™¤å¹´ä»½èˆ‡æ¨™æº–è™Ÿ
                            if float(num) > 10000: continue # æ’é™¤å¤§ç·¨è™Ÿ
                            final_val = num
                            break
                    
                    if not final_val: continue

                    # åŒ¹é…é …ç›®
                    for key, kws in SIMPLE_KEYWORDS.items():
                        if any(kw.lower() in item_text for kw in kws):
                            # Cd é˜²ç¦¦
                            if key == "Cd" and "hexabromocyclododecane" in item_text: continue
                            data[key].append(final_val)
                            break
                    for key, kws in GROUP_KEYWORDS.items():
                        if any(kw.lower() in item_text for kw in kws):
                            data[key].append(final_val)
                            break

    # 3. HF ç„¡é¹µ (å€å¡Šæ–‡å­—æœç´¢æ³•)
    # é‡å°ç„¡é¹µæ•¸æ“šæ›è¡Œåš´é‡çš„å•é¡Œï¼Œæ”¾æ£„è¡¨æ ¼ï¼Œç›´æ¥æƒæ–‡å­—å€å¡Š
    ft_lower = full_text.lower()
    
    targets = {
        "F": "fluorine",
        "CL": "chlorine",
        "BR": "bromine",
        "I": "iodine"
    }
    
    for key, kw in targets.items():
        if not data[key]: # å¦‚æœè¡¨æ ¼æ²’æŠ“åˆ°
            idx = ft_lower.find(kw)
            if idx != -1:
                # é–‹çª—æœç´¢: å¾€å¾Œçœ‹ 200 å­—å…ƒ
                window = ft_lower[idx:idx+200]
                
                # æ‰¾ N.D.
                if "n.d." in window:
                    data[key].append("N.D.")
                else:
                    # æ‰¾æ•¸å­— (æ’é™¤ MDL 50)
                    nums = re.findall(r"\b\d+\b", window)
                    for n in nums:
                        if n == "50": continue # é¦¬ä¾†è¥¿äºç„¡é¹µ MDL å‡ç‚º 50
                        if n in ["2020", "62321"]: continue # æ’é™¤å¹´ä»½æ¨™æº–
                        data[key].append(n)
                        break

    return data

# =============================================================================
# 4. ä¸»ç¨‹å¼èˆ‡åˆ†æµå™¨
# =============================================================================

def process_files(files):
    results = []
    progress_bar = st.progress(0)
    
    for i, file in enumerate(files):
        try:
            with pdfplumber.open(file) as pdf:
                # 0. è®€å–ç¬¬ä¸€é åˆ¤æ–·å¼•æ“
                first_page_text = (pdf.pages[0].extract_text() or "").upper()
                
                # åˆ†æµé‚è¼¯
                if "MALAYSIA" in first_page_text and "SGS" in first_page_text:
                    # é€²å…¥é¦¬ä¾†è¥¿äºå¼•æ“
                    file_data = process_malaysia(pdf, file.name)
                else:
                    # é€²å…¥æ¨™æº–å¼•æ“ (v60.3)
                    file_data = process_standard(pdf, file.name)
                
                # è³‡æ–™æ•´ç†: å– list ä¸­çš„ç¬¬ä¸€å€‹å€¼ (é€šå¸¸æ˜¯æœ€å„ªè§£)
                final_row = {}
                for k in OUTPUT_COLUMNS:
                    if k == "æª”æ¡ˆåç¨±":
                        final_row[k] = file.name
                    elif k == "æ—¥æœŸ":
                        final_row[k] = file_data.get("æ—¥æœŸ", "")
                    else:
                        vals = file_data.get(k, [])
                        # éæ¿¾é‡è¤‡èˆ‡ç„¡æ•ˆå€¼
                        valid_vals = [v for v in vals if v]
                        if valid_vals:
                            final_row[k] = valid_vals[0] # å–ç¬¬ä¸€å€‹æŠ“åˆ°çš„
                        else:
                            final_row[k] = ""
                
                results.append(final_row)

        except Exception as e:
            st.error(f"è™•ç†æª”æ¡ˆ {file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
        progress_bar.progress((i + 1) / len(files))
        
    return results

# =============================================================================
# 5. Streamlit ä»‹é¢
# =============================================================================

st.set_page_config(page_title="SGS å ±å‘Šèšåˆå·¥å…· v61.0", layout="wide")
st.title("ğŸ“„ è¬ç”¨å‹æª¢æ¸¬å ±å‘Šèšåˆå·¥å…· (v61.0 é›™æ ¸å¿ƒå¼•æ“ç‰ˆ)")
st.info("ğŸ’¡ v61.0ï¼šå°å…¥ã€Œè‡ªå‹•åˆ†æµã€æŠ€è¡“ã€‚æ¨™æº–å ±å‘Šä½¿ç”¨ç©©å®šèˆŠæ ¸å¿ƒï¼ŒSGS é¦¬ä¾†è¥¿äºå ±å‘Šä½¿ç”¨å°ˆç”¨æš´åŠ›æ ¸å¿ƒã€‚")

uploaded_files = st.file_uploader("è«‹ä¸€æ¬¡é¸å–æ‰€æœ‰ PDF æª”æ¡ˆ", type="pdf", accept_multiple_files=True)

if uploaded_files:
    if st.button("ğŸ”„ é‡æ–°åŸ·è¡Œ"): st.rerun()

    try:
        result_data = process_files(uploaded_files)
        df = pd.DataFrame(result_data)
        
        # ç¢ºä¿æ¬„ä½é †åº
        df = df.reindex(columns=OUTPUT_COLUMNS)

        st.success("âœ… è™•ç†å®Œæˆï¼")
        st.dataframe(df)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='Summary')
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel",
            data=output.getvalue(),
            file_name="SGS_Summary_v61.0.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
    except Exception as e:
        st.error(f"ç³»çµ±éŒ¯èª¤: {e}")
