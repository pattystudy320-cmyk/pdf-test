import streamlit as st
import pdfplumber
import pandas as pd
import re
from datetime import datetime

# --- è¨­å®šé é¢ ---
st.set_page_config(page_title="é€šç”¨æª¢æ¸¬å ±å‘Šæ“·å–å·¥å…· (V17 è·¨è¡Œä¿®æ­£ç‰ˆ)", layout="wide")
st.title("ğŸ§ª é€šç”¨å‹ç¬¬ä¸‰æ–¹æª¢æ¸¬å ±å‘Šæ•¸æ“šæ“·å–å·¥å…· (V17 è·¨è¡Œä¿®æ­£ç‰ˆ)")
st.markdown("""
**V17 ç‰ˆæœ¬ä¿®æ­£é‡é»ï¼š**
1.  **ğŸ“„ PBBs/PBDEs è·¨è¡Œæƒæ**ï¼šè§£æ±º CTI æ•¸å€¼æ‰åˆ°ä¸‹ä¸€è¡Œçš„å•é¡Œï¼Œä¸¦é©ç”¨æ–¼ Intertek éŸ“åœ‹ç‰ˆç­‰ç‰¹æ®Šæ’ç‰ˆã€‚
2.  **ğŸ¯ SGS çµ•å°ä½ç½®**ï¼šé‡å° SGS å ±å‘Šï¼Œè‹¥ç„¡æ³•å®šä½æ¬„ä½ï¼Œå„ªå…ˆé–å®šã€Œæœ€å³æ¬„ã€ã€‚
3.  **ğŸ›¡ï¸ æ¨£å“ ID é–å®š**ï¼šè‡ªå‹•æŠ“å– A1, A2, 001 ç­‰ç·¨è™Ÿä½œç‚ºæ¬„ä½å®šä½ä¾æ“šã€‚
""")

# --- 1. é—œéµå­—å®šç¾© ---
TARGET_FIELDS = {
    "Lead": {"name": "Pb", "keywords": [r"^Lead\b", r"^Pb\b", r"é“…", r"Lead \(Pb\)", r"Pb"]},
    "Cadmium": {"name": "Cd", "keywords": [r"^Cadmium\b", r"^Cd\b", r"é•‰", r"Cadmium \(Cd\)", r"Cd"]},
    "Mercury": {"name": "Hg", "keywords": [r"^Mercury\b", r"^Hg\b", r"æ±", r"Mercury \(Hg\)", r"Hg"]},
    "Hexavalent Chromium": {"name": "Cr(VI)", "keywords": [r"Hexavalent Chromium", r"Cr\(VI\)", r"Cr6\+", r"å…­ä»·é“¬", r"å…­åƒ¹é‰»"]},
    "DEHP": {"name": "DEHP", "keywords": [r"Bis\(2-ethylhexyl\) phthalate", r"DEHP", r"é‚»è‹¯äºŒç”²é…¸äºŒ\(2-ä¹™åŸºå·±åŸº\)é…¯"]},
    "BBP": {"name": "BBP", "keywords": [r"Butyl benzyl phthalate", r"BBP", r"é‚»è‹¯äºŒç”²é…¸ä¸åŸºè‹„åŸºé…¯", r"é‚»è‹¯äºŒç”²é…¸ä¸è‹„é…¯"]},
    "DBP": {"name": "DBP", "keywords": [r"Dibutyl phthalate", r"DBP", r"é‚»è‹¯äºŒç”²é…¸äºŒä¸é…¯"]},
    "DIBP": {"name": "DIBP", "keywords": [r"Diisobutyl phthalate", r"DIBP", r"é‚»è‹¯äºŒç”²é…¸äºŒå¼‚ä¸é…¯"]},
    "Fluorine": {"name": "F", "keywords": [r"Fluorine", r"æ°Ÿ", r"Fluorine \(F\)"]},
    "Chlorine": {"name": "Cl", "keywords": [r"Chlorine", r"æ°¯", r"Chlorine \(Cl\)"]},
    "Bromine": {"name": "Br", "keywords": [r"Bromine", r"æº´", r"Bromine \(Br\)"]},
    "Iodine": {"name": "I", "keywords": [r"Iodine", r"ç¢˜", r"Iodine \(I\)"]},
    "PFOS": {"name": "PFOS", "keywords": [r"Perfluorooctane Sulfonates", r"PFOS", r"å…¨æ°Ÿè¾›çƒ·ç£ºé…¸"]},
}

# æœ‰æ©Ÿç‰©é—œéµå­— (ç´”æ–‡å­—æƒæç”¨)
PBBS_KEYWORDS = [r"Monobromobiphenyl", r"Dibromobiphenyl", r"Tribromobiphenyl", r"Tetrabromobiphenyl", 
                 r"Pentabromobiphenyl", r"Hexabromobiphenyl", r"Heptabromobiphenyl", r"Octabromobiphenyl", 
                 r"Nonabromobiphenyl", r"Decabromobiphenyl", 
                 r"ä¸€æº´è”è‹¯", r"äºŒæº´è”è‹¯", r"ä¸‰æº´è”è‹¯", r"å››æº´è”è‹¯", r"äº”æº´è”è‹¯", 
                 r"å…­æº´è”è‹¯", r"ä¸ƒæº´è”è‹¯", r"å…«æº´è”è‹¯", r"ä¹æº´è”è‹¯", r"åæº´è”è‹¯"]

PBDES_KEYWORDS = [r"Monobromodiphenyl ether", r"Dibromodiphenyl ether", r"Tribromodiphenyl ether", 
                  r"Tetrabromodiphenyl ether", r"Pentabromodiphenyl ether", r"Hexabromodiphenyl ether", 
                  r"Heptabromodiphenyl ether", r"Octabromodiphenyl ether", r"Nonabromodiphenyl ether", 
                  r"Decabromodiphenyl ether", 
                  r"ä¸€æº´äºŒè‹¯é†š", r"äºŒæº´äºŒè‹¯é†š", r"ä¸‰æº´äºŒè‹¯é†š", r"å››æº´äºŒè‹¯é†š", r"äº”æº´äºŒè‹¯é†š", 
                  r"å…­æº´äºŒè‹¯é†š", r"ä¸ƒæº´äºŒè‹¯é†š", r"å…«æº´äºŒè‹¯é†š", r"ä¹æº´äºŒè‹¯é†š", r"åæº´äºŒè‹¯é†š"]

# --- 2. è¼”åŠ©å‡½å¼ ---

def clean_text(text):
    if not text: return ""
    return re.sub(r'\s+', ' ', str(text)).strip()

def parse_date_obj(date_str):
    clean = re.sub(r"Date:|Issue Date:|Report Date:|æ—¥æœŸ\s*\(?Date\)?[:ï¼š]?", "", date_str, flags=re.IGNORECASE).strip()
    clean = clean.replace("/", "-").replace(".", "-").replace(" ", "-")
    
    formats = ["%Y-%m-%d", "%d-%b-%Y", "%d-%B-%Y", "%b-%d-%Y", "%B-%d-%Y", "%d-%b-%y", "%d-%B-%y"]
    for fmt in formats:
        try: return datetime.strptime(clean, fmt)
        except: continue
            
    try:
        m = re.search(r"(\d{4})[-/. ](\d{1,2})[-/. ](\d{1,2})", date_str)
        if m: return datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
        
        m2 = re.search(r"(\d{1,2})[-/\s]([A-Za-z]{3})[-/\s,.]+(\d{4})", date_str, re.IGNORECASE)
        if m2: return datetime.strptime(f"{m2.group(1)}-{m2.group(2)}-{m2.group(3)}", "%d-%b-%Y")
        
        m3 = re.search(r"([A-Za-z]{3})\.?\s+(\d{1,2})[,\s]+(\d{4})", date_str, re.IGNORECASE)
        if m3: return datetime.strptime(f"{m3.group(2)}-{m3.group(1)}-{m3.group(3)}", "%d-%b-%Y")
    except: pass
    return None

def find_date_in_first_page(text):
    lines = text.split('\n')
    candidates = []
    blacklist = ["RECEIVED", "PERIOD", "STARTED", "SUBMITTED", "COMPLETED", "TESTING", "æ”¶ä»¶", "æ¥æ”¶", "å‘¨æœŸ", "æœŸé—´"]
    
    for line in lines:
        upper_line = line.upper()
        if any(bad in upper_line for bad in blacklist): continue
            
        if re.search(r"\d{4}[-/. ]\d{1,2}[-/. ]\d{1,2}", line) or \
           (re.search(r"[A-Za-z]{3}", line) and re.search(r"\d{4}", line)):
            candidates.append(line)
            
    valid_dates = []
    for c in candidates:
        dt = parse_date_obj(c)
        if dt and 2015 <= dt.year <= 2030: valid_dates.append(dt)
    
    if valid_dates:
        return max(valid_dates).strftime("%Y/%m/%d")
    return ""

def extract_value_logic(val_str, strict_numeric=False):
    if not val_str: return None, ""
    val_upper = str(val_str).upper().replace(" ", "")
    
    if re.search(r"\b\d{2,7}-\d{2}-\d\b", val_str): return None, "" # CAS No.

    if "N.D." in val_upper or "ND" in val_upper or "<" in val_upper: return 0, "N.D."
    
    if "NEGATIVE" in val_upper or "é˜´æ€§" in val_upper: 
        if strict_numeric: return None, ""
        return 0.0001, "NEGATIVE"
        
    if "POSITIVE" in val_upper or "é˜³æ€§" in val_upper: 
        if strict_numeric: return None, ""
        return 999999, "POSITIVE"
    
    val_clean = re.sub(r"(mg/kg|ppm|%|Âµg/cmÂ²|ug/cm2)", "", val_str, flags=re.IGNORECASE)
    match = re.search(r"(\d+(\.\d+)?)", val_clean)
    
    if match:
        num = float(match.group(1))
        if 2010 <= num <= 2030: return None, "" # Exclude years
        return num, match.group(1)
    
    return None, ""

def check_pfas_in_section(full_text):
    start_keywords = ["TEST REQUESTED", "æµ‹è¯•éœ€æ±‚", "æ£€æµ‹è¦æ±‚", "TEST REQUEST"]
    end_keywords = ["TEST METHOD", "TEST RESULTS", "CONCLUSION", "æµ‹è¯•ç»“æœ", "ç»“è®º", "æ£€æµ‹æ–¹æ³•"]
    upper = full_text.upper()
    
    start_idx = -1
    for kw in start_keywords:
        idx = upper.find(kw)
        if idx != -1: 
            start_idx = idx
            break
    if start_idx == -1: return ""
    
    end_idx = len(upper)
    for kw in end_keywords:
        idx = upper.find(kw, start_idx)
        if idx != -1: 
            end_idx = idx
            break
            
    target_text = upper[start_idx:end_idx]
    if "PFAS" in target_text or "PER- AND POLYFLUOROALKYL" in target_text: return "REPORT"
    return ""

# --- 3. æ ¸å¿ƒè™•ç†é‚è¼¯ ---

def find_sample_ids(full_text_pages_1_2):
    """é è®€æ¨£å“ç·¨è™Ÿ (A1, A2, 001...)"""
    ids = []
    patterns = [
        r"(?:Sample|Specimen)\s*(?:No\.|ID|Ref\.?)\s*[:ï¼š]?\s*([A-Za-z0-9\-]+)",
        r"(?:SN\s*ID)\s*[:ï¼š]?\s*([A-Za-z0-9\-]+)",
        r"(?:æ ·å“|æ¨£å“)\s*(?:ç¼–å·|åºå·|ID)\s*[:ï¼š]?\s*([A-Za-z0-9\-]+)"
    ]
    for line in full_text_pages_1_2.split('\n'):
        for pat in patterns:
            m = re.search(pat, line, re.IGNORECASE)
            if m:
                found_id = m.group(1).strip()
                if len(found_id) < 10: ids.append(found_id.upper())
    return list(set(ids))

def get_column_score(header_cells, sample_ids, is_sgs=False):
    """V17 æ¬„ä½å®šä½ï¼šæ”¯æ´ Sample ID èˆ‡ SGS çµ•å°ä½ç½®"""
    scores = {}
    num_cols = len(header_cells)
    
    result_kw = ["RESULT", "ç»“æœ", "SAMPLE", "ID", "001", "002", "A1", "A2", "DATA", "å«é‡"]
    known_cols_kw = ["ITEM", "METHOD", "UNIT", "MDL", "LOQ", "LIMIT", "REQUIREMENT", "é¡¹ç›®", "æ–¹æ³•", "å•ä½", "é™å€¼", "CAS"]
    
    for i, cell in enumerate(header_cells):
        if not cell: continue
        txt = clean_text(str(cell)).upper()
        score = 0
        
        if any(k in txt for k in known_cols_kw): score -= 500
        if any(res in txt for res in result_kw): score += 100
        if txt in sample_ids: score += 200 # å‘½ä¸­é è®€çš„ Sample ID
        
        if score == 0: score += 50 # æœªçŸ¥æ¬„ä½å¯èƒ½æ˜¯çµæœ
        scores[i] = score

    if not scores: return -1
    best_col = max(scores, key=scores.get)
    
    # SGS ç‰¹æ®Šè¦å‰‡ï¼šå¦‚æœæ²’æœ‰æ˜ç¢ºçš„ Result æ¬„ä½ï¼Œä½†æœ‰ Limit/MDLï¼Œå„ªå…ˆä¿¡ä»»æœ€å¾Œä¸€æ¬„
    if is_sgs and scores[best_col] <= 50: 
        return num_cols - 1
        
    if scores[best_col] < 0: return -1
    return best_col

def process_file(uploaded_file):
    filename = uploaded_file.name
    results = {k: {"val": None, "display": ""} for k in TARGET_FIELDS.keys()}
    results["PBBs"] = {"val": None, "display": "", "sum_val": 0}
    results["PBDEs"] = {"val": None, "display": "", "sum_val": 0}
    results["PFAS"] = ""
    results["Date"] = ""
    
    full_text_content = ""
    is_sgs = "SGS" in filename.upper() # ç°¡å–®åˆ¤æ–·æ˜¯å¦ç‚º SGS
    
    with pdfplumber.open(uploaded_file) as pdf:
        # A. å…¨æ–‡æƒæ
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                full_text_content += text + "\n"
                if "SGS" in text.upper(): is_sgs = True
                if i == 0: results["Date"] = find_date_in_first_page(text)

        sample_ids = find_sample_ids(full_text_content[:3000])
        results["PFAS"] = check_pfas_in_section(full_text_content)

        # --- è»Œé“ A: PBBs/PBDEs è·¨è¡Œæ–‡å­—æƒæ (V17) ---
        text_lines = full_text_content.split('\n')
        num_lines = len(text_lines)
        
        for i in range(num_lines):
            line = text_lines[i].upper()
            
            def process_text_sum_multiline(keywords, cat_key, current_idx):
                if any(k.upper() in line for k in keywords):
                    # 1. å…ˆåœ¨ç•¶å‰è¡Œæ‰¾
                    found_val = False
                    potential_vals = []
                    
                    # æª¢æŸ¥ç•¶å‰è¡Œ
                    parts = text_lines[current_idx].split()
                    for part in parts:
                        v, d = extract_value_logic(part)
                        if v is not None:
                            if v in [5, 10, 25, 50, 100, 1000] and d != "N.D.": continue
                            potential_vals.append(v)
                    
                    # 2. å¦‚æœç•¶å‰è¡Œæ²’æ‰¾åˆ°ï¼Œæª¢æŸ¥ä¸‹ä¸€è¡Œ (è·¨è¡Œè™•ç†)
                    if not potential_vals and current_idx + 1 < num_lines:
                        parts_next = text_lines[current_idx + 1].split()
                        for part in parts_next:
                            v, d = extract_value_logic(part)
                            if v is not None:
                                if v in [5, 10, 25, 50, 100, 1000] and d != "N.D.": continue
                                potential_vals.append(v)

                    if potential_vals:
                        val = potential_vals[-1]
                        if val > 0:
                            results[cat_key]["sum_val"] += val
                            results[cat_key]["val"] = 1

            process_text_sum_multiline(PBBS_KEYWORDS, "PBBs", i)
            process_text_sum_multiline(PBDES_KEYWORDS, "PBDEs", i)

        # --- è»Œé“ B: é‡é‡‘å±¬/å–®é … è¡¨æ ¼å®šä½ (V17) ---
        for page in pdf.pages:
            tables = page.extract_tables()
            if not tables: continue
            
            for table in tables:
                if not table or len(table) < 2: continue
                
                header_row_idx = -1
                result_col_idx = -1
                
                for r_idx, row in enumerate(table[:6]):
                    row_str = " ".join([str(c).upper() for c in row if c])
                    if ("ITEM" in row_str or "é¡¹ç›®" in row_str or "TEST" in row_str) and \
                       ("UNIT" in row_str or "MDL" in row_str or "LIMIT" in row_str or "RESULT" in row_str):
                        header_row_idx = r_idx
                        result_col_idx = get_column_score(row, sample_ids, is_sgs)
                        break
                
                if header_row_idx == -1: continue
                
                for r_idx in range(header_row_idx + 1, len(table)):
                    row = table[r_idx]
                    if not row: continue
                    
                    item_name = clean_text(row[0])
                    if len(row) > 1: item_name += " " + clean_text(row[1])
                    item_upper = item_name.upper()
                    
                    for field, config in TARGET_FIELDS.items():
                        for kw in config["keywords"]:
                            if re.search(kw, item_upper, re.IGNORECASE):
                                if field == "Chlorine" and ("POLYVINYL" in item_upper or "PVC" in item_upper): continue
                                
                                val_text = ""
                                if result_col_idx != -1 and len(row) > result_col_idx:
                                    val_text = clean_text(row[result_col_idx])
                                else:
                                    val_text = clean_text(row[-1]) # Fallback to last column
                                
                                is_strict = (field in ["Chlorine", "Bromine", "PFOS"])
                                v_num, v_disp = extract_value_logic(val_text, strict_numeric=is_strict)
                                
                                if v_num is not None:
                                    if v_num in [1000] and v_disp != "N.D.": continue
                                    
                                    curr = results[field]["val"]
                                    if curr is None or v_num > curr:
                                        results[field]["val"] = v_num
                                        results[field]["display"] = v_disp
                                    elif v_num == 0 and (curr is None or curr == 0):
                                        if v_disp == "NEGATIVE": results[field]["display"] = "NEGATIVE"
                                        elif not results[field]["display"]: results[field]["display"] = "N.D."
                                        results[field]["val"] = 0

    # --- æœ€çµ‚æ•´ç† ---
    if results["PBBs"]["sum_val"] > 0:
        results["PBBs"]["display"] = str(round(results["PBBs"]["sum_val"], 2))
    elif results["PBBs"]["val"] is None:
        results["PBBs"]["display"] = ""
    else:
        results["PBBs"]["display"] = "N.D."

    if results["PBDEs"]["sum_val"] > 0:
        results["PBDEs"]["display"] = str(round(results["PBDEs"]["sum_val"], 2))
    elif results["PBDEs"]["val"] is None:
        results["PBDEs"]["display"] = ""
    else:
        results["PBDEs"]["display"] = "N.D."

    final_output = {
        "File Name": filename,
        "Pb": results["Lead"]["display"],
        "Cd": results["Cadmium"]["display"],
        "Hg": results["Mercury"]["display"],
        "Cr(VI)": results["Hexavalent Chromium"]["display"],
        "PBBs": results["PBBs"]["display"],
        "PBDEs": results["PBDEs"]["display"],
        "DEHP": results["DEHP"]["display"],
        "BBP": results["BBP"]["display"],
        "DBP": results["DBP"]["display"],
        "DIBP": results["DIBP"]["display"],
        "F": results["Fluorine"]["display"],
        "Cl": results["Chlorine"]["display"],
        "Br": results["Bromine"]["display"],
        "I": results["Iodine"]["display"],
        "PFOS": results["PFOS"]["display"],
        "PFAS": results["PFAS"],
        "Date": results["Date"],
        "_sort_pb": results["Lead"]["val"],
        "_sort_max": max([v["val"] for k, v in results.items() if isinstance(v, dict) and v["val"] is not None])
    }
    
    return final_output, None

# --- ä¸»ä»‹é¢ ---
uploaded_files = st.file_uploader("è«‹ä¸Šå‚³ PDF æª¢æ¸¬å ±å‘Š (æ”¯æ´ SGS, CTI, Intertek ç­‰)", type="pdf", accept_multiple_files=True)

if uploaded_files:
    all_data = []
    scanned_files = []

    with st.spinner('æ­£åœ¨é€²è¡Œ V17 å¼•æ“åˆ†æ (è·¨è¡Œæ–‡å­—æµ + æ¨£å“ ID é–å®š)...'):
        for pdf_file in uploaded_files:
            data, scanned_name = process_file(pdf_file)
            if scanned_name:
                scanned_files.append(scanned_name)
            else:
                all_data.append(data)

    if all_data:
        df = pd.DataFrame(all_data)
        if "_sort_pb" in df.columns:
            df = df.sort_values(by=["_sort_pb", "_sort_max"], ascending=[False, False])
            display_df = df.drop(columns=["_sort_pb", "_sort_max"])
        else:
            display_df = df
        
        st.success(f"âœ… æˆåŠŸæ“·å– {len(all_data)} ä»½å ±å‘Šï¼(V17 æ ¸å¿ƒ)")
        st.dataframe(display_df, use_container_width=True)
        
        csv = display_df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ Excel/CSV å ±è¡¨",
            data=csv,
            file_name="rohs_report_v17_final.csv",
            mime="text/csv",
        )

    if scanned_files:
        st.error("âš ï¸ ä»¥ä¸‹æª”æ¡ˆç‚ºæƒæåœ–ç‰‡ (ç„¡æ³•æ“·å–æ–‡å­—)ï¼š")
        for f in scanned_files:
            st.write(f"- {f}")
else:
    st.info("è«‹ä¸Šå‚³ PDF æª”æ¡ˆä»¥é–‹å§‹åˆ†æã€‚")
