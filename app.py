import streamlit as st
import pdfplumber
import pandas as pd
import re
from dateutil import parser
import io

# è¨­å®šé é¢è³‡è¨Š
st.set_page_config(page_title="SGS Report Parser", layout="wide")
st.title("ğŸ“„ SGS Report æª¢æ¸¬çµæœå½™ç¸½å·¥å…· (é¦¬ä¾†è¥¿äºä¿®æ­£ç‰ˆ)")

# =========================
# 1. æ¬„ä½å®šç¾©è¦å‰‡
# =========================
ITEM_RULES = {
    "Pb": r"Lead\s*\(Pb\)",
    "Cd": r"Cadmium\s*\(Cd\)",
    "Hg": r"Mercury\s*\(Hg\)",
    "CrVI": r"Hexavalent Chromium",
    "PBBs": r"Sum of PBBs",
    "PBDEs": r"Sum of PBDEs",
    "DEHP": r"DEHP",
    "BBP": r"BBP",
    "DBP": r"DBP",
    "DIBP": r"DIBP",
    "F": r"Fluorine",
    "CL": r"Chlorine",
    "BR": r"Bromine",
    "I": r"Iodine",
    "PFOS": r"PFOS"
}

FINAL_COLUMNS = [
    "Pb", "Cd", "Hg", "CrVI", "PBBs", "PBDEs",
    "DEHP", "BBP", "DBP", "DIBP",
    "F", "CL", "BR", "I",
    "PFOS", "PFAS", "DATE"
]

# =========================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼
# =========================

def extract_text_and_pages(pdf_file):
    """è®€å– PDF æ–‡å­—å…§å®¹"""
    full_text = ""
    pages_text = []
    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            pages_text.append(text)
            full_text += text + "\n"
    return full_text, pages_text

def extract_result(text, keyword):
    """
    ä¿®æ­£ç‰ˆ V4.1 (ä¿®å¾© IndexError èˆ‡å¼•ç”¨æ¨™ç±¤å•é¡Œ)ï¼š
    1. å¼·åŠ›æ¸…æ´—ï¼šåœ¨è®€å–æ•¸æ“šå‰ï¼Œå¼·åˆ¶åˆªé™¤ Max, MDL, CAS, Year ç­‰å¹²æ“¾é …ã€‚
    2. N.D. å„ªå…ˆï¼šåªè¦åµæ¸¬åˆ° N.D. è®Šé«”ï¼Œç›´æ¥å›å‚³æ¨™æº– "N.D."ã€‚
    3. æŠ“å–æ•¸å€¼ï¼šä¿®å¾© regex æŠ“å–é‚è¼¯ï¼Œé¿å…ç©ºå€¼éŒ¯èª¤ã€‚
    """
    lines = text.splitlines()

    for i, line in enumerate(lines):
        # æ­¥é©Ÿ A: é–å®šé—œéµå­—æ‰€åœ¨çš„è¡Œ
        if re.search(keyword, line, re.IGNORECASE):
            # æŠ“å–ä¸Šä¸‹æ–‡ (ç•¶è¡Œ + ä¸‹ä¸€è¡Œ)
            context = " ".join(lines[i:i+2])

            # ==========================================
            # æ­¥é©Ÿ B: æ‰‹è¡“å®¤ - å¼·åŠ›åˆ‡é™¤å¹²æ“¾æº
            # ==========================================
            
            # 1. åˆ‡é™¤å–®ä½ (mg/kg, ppm, %, wt%)
            context = re.sub(r"mg/kg|ppm|%|wt%", " ", context, flags=re.IGNORECASE)

            # 2. åˆ‡é™¤ CAS No.
            context = re.sub(r"\(?CAS\s*No\.?[\s\d-]+\)?", " ", context, flags=re.IGNORECASE)

            # 3. åˆ‡é™¤ æ¨™æº–ç·¨è™Ÿèˆ‡å¹´ä»½ (å¦‚ IEC 62321-5:2013)
            context = re.sub(r"IEC\s*62321[-\d:+A]*", " ", context, flags=re.IGNORECASE)
            context = re.sub(r"\b(19|20)\d{2}\b", " ", context) # ç§»é™¤ 19xx æˆ– 20xx å¹´ä»½

            # 4. åˆ‡é™¤ Limit / MDL æ¨™ç±¤èˆ‡æ•¸å€¼ (å¦‚ Max 1000, MDL 2)
            context = re.sub(r"(Max|Limit|MDL|LOQ)\s*\d+(\.\d+)?", " ", context, flags=re.IGNORECASE)

            # ==========================================
            # æ­¥é©Ÿ C: åˆ¤æ–·çµæœ - N.D. å„ªå…ˆ
            # ==========================================

            # å¯¬é¬†åˆ¤å®š N.D. (åŒ…å« ND, N. D., Not Detected)
            nd_pattern = r"(\bN\s*\.?\s*D\s*\.?\b)|(Not\s*Detected)"
            
            if re.search(nd_pattern, context, re.IGNORECASE):
                return "N.D."

            if re.search(r"NEGATIVE", context, re.IGNORECASE):
                return "NEGATIVE"

            # ==========================================
            # æ­¥é©Ÿ D: æŠ“å–æ•¸å€¼ (ä¿®å¾© Bug çš„éƒ¨åˆ†)
            # ==========================================
            
            # ä¿®æ­£å¾Œçš„ Regexï¼šä½¿ç”¨éæ•æ‰ç¾¤çµ„ (?:\.\d+)? ç¢ºä¿ findall å›å‚³å®Œæ•´å­—ä¸²
            nums = re.findall(r"\b\d+(?:\.\d+)?\b", context)
            
            if nums:
                # nums ç¾åœ¨æœƒæ˜¯ä¸€å€‹å­—ä¸²åˆ—è¡¨ï¼Œä¾‹å¦‚ ['50.0', '50']
                # æˆ‘å€‘ç›´æ¥å–ç¬¬ä¸€å€‹
                found_value = nums[0] 
                
                try:
                    val_float = float(found_value)
                    # æœ€å¾Œé˜²å‘†ï¼šå¦‚æœæ•¸å­—çœ‹èµ·ä¾†åƒå¹´ä»½ (1990-2030) ä¸”æ˜¯æ•´æ•¸ï¼Œå¿½ç•¥å®ƒ
                    if 1990 <= val_float <= 2030 and val_float.is_integer():
                        continue 
                    return found_value
                except:
                    pass

    return ""

def extract_pfas(text):
    return "REPORT" if re.search(r"\bPFAS\b", text, re.IGNORECASE) else ""

def extract_date(first_page_text):
    match = re.search(
        r"(REPORTED DATE|TEST REPORT REPORTED DATE)\s*[:\-]?\s*([^\n]+)",
        first_page_text,
        re.IGNORECASE
    )
    return match.group(2).strip() if match else ""

def normalize_date(date_text):
    if not date_text:
        return ""
    try:
        dt = parser.parse(date_text, dayfirst=True)
        return dt.strftime("%Y/%m/%d")
    except:
        return ""

def merge_results(values):
    nums = []
    has_nd = False
    has_neg = False

    for v in values:
        if not v:
            continue
        v_upper = str(v).upper()
        
        if "N.D." in v_upper:
            has_nd = True
        elif "NEGATIVE" in v_upper:
            has_neg = True
        else:
            try:
                nums.append(float(v))
            except:
                pass

    if nums:
        return str(max(nums))
    if has_neg:
        return "NEGATIVE"
    if has_nd:
        return "N.D."
    return ""

# =========================
# 3. Streamlit ä¸»ç¨‹å¼ä»‹é¢
# =========================

uploaded_files = st.file_uploader(
    "è«‹ä¸Šå‚³ SGS PDF Reportï¼ˆå¯ä¸€æ¬¡å¤šé¸ï¼‰",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    rows = []
    for file in uploaded_files:
        full_text, pages_text = extract_text_and_pages(file)
        record = {}

        for item, keyword in ITEM_RULES.items():
            record[item] = extract_result(full_text, keyword)

        record["PFAS"] = extract_pfas(full_text)
        raw_date = extract_date(pages_text[0]) if pages_text else ""
        record["DATE"] = normalize_date(raw_date)

        rows.append(record)

    df_all = pd.DataFrame(rows)

    merged = {}
    if not df_all.empty:
        for col in FINAL_COLUMNS:
            if col in ["DATE", "PFAS"]:
                continue
            if col in df_all.columns:
                merged[col] = merge_results(df_all[col].tolist())
            else:
                merged[col] = ""

        merged["PFAS"] = "REPORT" if "REPORT" in df_all["PFAS"].tolist() else ""
        
        valid_dates = [d for d in df_all["DATE"] if d]
        merged["DATE"] = max(valid_dates) if valid_dates else ""

        df_final = pd.DataFrame([merged], columns=FINAL_COLUMNS)

        st.subheader("ğŸ“Š å½™ç¸½çµæœï¼ˆåŒæ‰¹ SGS Reportï¼‰")
        st.dataframe(df_final, use_container_width=True)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_final.to_excel(writer, sheet_name="SGS_Result", index=False)

        st.download_button(
            "â¬‡ï¸ ä¸‹è¼‰å…¬å¸åˆ¶å¼ Excel",
            output.getvalue(),
            file_name="SGS_Test_Result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.warning("æœªè®€å–åˆ°æœ‰æ•ˆè³‡æ–™ã€‚")
