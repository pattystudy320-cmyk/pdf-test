import streamlit as st
import pdfplumber
import pandas as pd
import re
from dateutil import parser
import io

# è¨­å®šé é¢è³‡è¨Š
st.set_page_config(page_title="SGS Report Parser", layout="wide")
st.title("ğŸ“„ SGS Report æª¢æ¸¬çµæœå½™ç¸½å·¥å…· (æœ€çµ‚é‚è¼¯ç‰ˆ)")

# =========================
# 1. æ¬„ä½å®šç¾©è¦å‰‡ (Regex)
# =========================
ITEM_RULES = {
    "Pb": r"Lead\s*\(Pb\)",
    "Cd": r"Cadmium\s*\(Cd\)",
    "Hg": r"Mercury\s*\(Hg\)",
    "CrVI": r"Hexavalent Chromium",
    "PBBs": r"Sum of PBBs",
    "PBDEs": r"Sum of PBDEs",
    "DEHP": r"DEHP|Di\(2-ethylhexyl\)\s*phthalate",
    "BBP": r"BBP|Benzyl\s*butyl\s*phthalate",
    "DBP": r"DBP|Dibutyl\s*phthalate",
    "DIBP": r"DIBP|Diisobutyl\s*phthalate",
    "F": r"\bFluorine\b",   # åŠ  \b é¿å…æŠ“åˆ°éƒ¨åˆ†å–®å­—
    "CL": r"\bChlorine\b",
    "BR": r"\bBromine\b",
    "I": r"\bIodine\b",
    "PFOS": r"PFOS"
}

FINAL_COLUMNS = [
    "Pb", "Cd", "Hg", "CrVI", "PBBs", "PBDEs",
    "DEHP", "BBP", "DBP", "DIBP",
    "F", "CL", "BR", "I",
    "PFOS", "PFAS", "DATE"
]

# =========================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼
# =========================

def extract_text_and_pages(pdf_file):
    """è®€å– PDF æ–‡å­—å…§å®¹"""
    full_text = ""
    pages_text = []
    with pdfplumber.open(pdf_file) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            pages_text.append(text)
            full_text += text + "\n"
    return full_text, pages_text

def extract_result(text, keyword, item_name):
    """
    æœ€çµ‚ç‰ˆé‚è¼¯ V5 (æ•¸å­—è¨ˆæ•¸æ³•):
    1. æ‰¾åˆ°é—œéµå­—æ‰€åœ¨è¡Œã€‚
    2. å¼·åŠ›æ¸…é™¤é›œè¨Š (Year, CAS, Limit)ã€‚
    3. å„ªå…ˆæª¢æŸ¥ N.D.ã€‚
    4. è¨ˆç®—å‰©é¤˜æ•¸å­—æ•¸é‡ï¼š
       - PBBs/PBDEs: å‰©ä¸‹ 1 å€‹æ•¸å­— -> è¦–ç‚º Result (å›  MDL ç‚º -)ã€‚
       - å…¶ä»–é …ç›®: å‰©ä¸‹ 1 å€‹æ•¸å­— -> è¦–ç‚º MDL -> å›å‚³ N.D.ã€‚
                 å‰©ä¸‹ 2 å€‹æ•¸å­— -> ç¬¬ä¸€å€‹ç‚º Resultã€‚
    """
    lines = text.splitlines()

    for i, line in enumerate(lines):
        # æ­¥é©Ÿ A: é–å®šé—œéµå­—æ‰€åœ¨çš„è¡Œ
        if re.search(keyword, line, re.IGNORECASE):
            # æŠ“å–ä¸Šä¸‹æ–‡ (ç•¶è¡Œ + ä¸‹ä¸€è¡Œ)ï¼Œç¸®å°ç¯„åœ
            context = " ".join(lines[i:i+2])

            # ==========================================
            # æ­¥é©Ÿ B: æ‰‹è¡“å®¤ - å¼·åŠ›åˆ‡é™¤é›œè¨Š
            # ==========================================
            
            # 1. åˆ‡é™¤å–®ä½
            context = re.sub(r"mg/kg|ppm|%|wt%", " ", context, flags=re.IGNORECASE)

            # 2. åˆ‡é™¤ CAS No.
            context = re.sub(r"\(?CAS\s*No\.?[\s\d-]+\)?", " ", context, flags=re.IGNORECASE)

            # 3. åˆ‡é™¤ æ¨™æº–ç·¨è™Ÿèˆ‡å¹´ä»½ (IEC 62321...:2017)
            context = re.sub(r"IEC\s*62321[-\d:+A]*", " ", context, flags=re.IGNORECASE)
            context = re.sub(r"\b(19|20)\d{2}\b", " ", context) # ç§»é™¤ 20xx å¹´ä»½

            # 4. åˆ‡é™¤ Limit / MDL æ¨™ç±¤èˆ‡æ•¸å€¼ (Max 1000, MDL 2)
            context = re.sub(r"(Max|Limit|MDL|LOQ)\s*\d+(\.\d+)?", " ", context, flags=re.IGNORECASE)

            # ==========================================
            # æ­¥é©Ÿ C: N.D. åˆ¤å®š (æœ€é«˜å„ªå…ˆç´š)
            # ==========================================
            
            # åªè¦æœ‰ N å’Œ Dï¼Œä¸”éå–®å­—ä¸€éƒ¨åˆ† (ä¾‹å¦‚ N.D., ND, N. D., Not Detected)
            nd_pattern = r"(\bN\s*\.?\s*D\s*\.?\b)|(Not\s*Detected)"
            if re.search(nd_pattern, context, re.IGNORECASE):
                return "N.D."
            
            if re.search(r"NEGATIVE", context, re.IGNORECASE):
                return "NEGATIVE"

            # ==========================================
            # æ­¥é©Ÿ D: æ•¸å­—è¨ˆæ•¸æ³• (æ ¸å¿ƒé‚è¼¯)
            # ==========================================
            
            # æŠ“å‡ºå‰©é¤˜çš„æ‰€æœ‰æ•¸å­— (æ”¯æ´æ•´æ•¸èˆ‡å°æ•¸)
            nums = re.findall(r"\b\d+(?:\.\d+)?\b", context)
            
            if not nums:
                # æ²’æ•¸å­—ä¹Ÿæ²’ N.D.ï¼Œä¿å®ˆå›å‚³ N.D.
                return "N.D."

            # --- ä¾ç…§ Item æ±ºå®šç­–ç•¥ ---
            
            # ç‰¹æ¬Šé …ç›®: PBBs / PBDEs (MDL å¯èƒ½æ˜¯ Dash "-")
            if item_name in ["PBBs", "PBDEs"]:
                # å¦‚æœæœ‰æ•¸å­—ï¼Œå°±ç›´æ¥æŠ“ç¬¬ä¸€å€‹ (å¿½ç•¥åªæœ‰ä¸€å€‹æ•¸å­—åªèƒ½æ˜¯ MDL çš„è¦å‰‡)
                return nums[0]
            
            # ä¸€èˆ¬é …ç›®: Pb, Cd, F, Cl ç­‰ (MDL å¿…å¡«)
            else:
                if len(nums) >= 2:
                    # å‰©ä¸‹å…©å€‹ä»¥ä¸Šæ•¸å­—ï¼š[çµæœ] [MDL]
                    # ç¬¬ä¸€å€‹æ˜¯çµæœ
                    found_val = nums[0]
                    # é˜²å‘†ï¼šå¦‚æœæ˜¯å¹´ä»½æ®˜æ¸£ (1990-2030 æ•´æ•¸)ï¼Œè·³é
                    try:
                        f_val = float(found_val)
                        if 1990 <= f_val <= 2030 and f_val.is_integer():
                            # å¦‚æœç¬¬ä¸€å€‹åƒæ˜¯å¹´ä»½ï¼Œä¸”é‚„æœ‰ç¬¬äºŒå€‹æ•¸å­—ï¼Œé‚£å°±å–ç¬¬äºŒå€‹
                            return nums[1]
                    except:
                        pass
                    return found_val
                
                elif len(nums) == 1:
                    # åªå‰©ä¸‹ä¸€å€‹æ•¸å­—ï¼
                    # æ¥µå¤§æ©Ÿç‡æ˜¯ N.D. æ²’æŠ“åˆ°ï¼Œå‰©ä¸‹çš„é€™å€‹æ˜¯ MDL (å¦‚ 2.0, 50.0)
                    # å¼·åˆ¶åˆ¤å®šç‚º N.D.
                    return "N.D."

    return ""

def extract_pfas(text):
    return "REPORT" if re.search(r"\bPFAS\b", text, re.IGNORECASE) else ""

def extract_date(first_page_text):
    match = re.search(
        r"(REPORTED DATE|TEST REPORT REPORTED DATE)\s*[:\-]?\s*([^\n]+)",
        first_page_text,
        re.IGNORECASE
    )
    return match.group(2).strip() if match else ""

def normalize_date(date_text):
    if not date_text:
        return ""
    try:
        dt = parser.parse(date_text, dayfirst=True)
        return dt.strftime("%Y/%m/%d")
    except:
        return ""

def merge_results(values):
    """
    å½™ç¸½é‚è¼¯ï¼šå–æœ€å¤§å€¼ï¼Œè‹¥æœ‰ N.D. å‰‡å„ªå…ˆç´šä½æ–¼æ•¸å€¼
    """
    nums = []
    has_nd = False
    has_neg = False

    for v in values:
        if not v:
            continue
        v_upper = str(v).upper()
        
        if "N.D." in v_upper:
            has_nd = True
        elif "NEGATIVE" in v_upper:
            has_neg = True
        else:
            try:
                nums.append(float(v))
            except:
                pass

    if nums:
        return str(max(nums))
    if has_neg:
        return "NEGATIVE"
    if has_nd:
        return "N.D."
    return ""

# =========================
# 3. Streamlit ä¸»ç¨‹å¼ä»‹é¢
# =========================

uploaded_files = st.file_uploader(
    "è«‹ä¸Šå‚³ SGS PDF Reportï¼ˆå¯ä¸€æ¬¡å¤šé¸ï¼‰",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    rows = []
    for file in uploaded_files:
        full_text, pages_text = extract_text_and_pages(file)
        record = {}

        # æŠ“å–å„é …ç›®ï¼Œå‚³å…¥ item key ä»¥ä¾¿å€åˆ†ç‰¹æ¬Šé‚è¼¯
        for item, keyword in ITEM_RULES.items():
            record[item] = extract_result(full_text, keyword, item)

        # ç‰¹æ®Šé …ç›®èˆ‡æ—¥æœŸ
        record["PFAS"] = extract_pfas(full_text)
        raw_date = extract_date(pages_text[0]) if pages_text else ""
        record["DATE"] = normalize_date(raw_date)

        rows.append(record)

    df_all = pd.DataFrame(rows)

    # åŒæ‰¹æ¬¡å½™ç¸½
    merged = {}
    if not df_all.empty:
        for col in FINAL_COLUMNS:
            if col in ["DATE", "PFAS"]:
                continue
            if col in df_all.columns:
                merged[col] = merge_results(df_all[col].tolist())
            else:
                merged[col] = ""

        merged["PFAS"] = "REPORT" if "REPORT" in df_all["PFAS"].tolist() else ""
        
        valid_dates = [d for d in df_all["DATE"] if d]
        merged["DATE"] = max(valid_dates) if valid_dates else ""

        df_final = pd.DataFrame([merged], columns=FINAL_COLUMNS)

        # é¡¯ç¤ºèˆ‡ä¸‹è¼‰
        st.subheader("ğŸ“Š å½™ç¸½çµæœï¼ˆåŒæ‰¹ SGS Reportï¼‰")
        st.dataframe(df_final, use_container_width=True)

        output = io.BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_final.to_excel(writer, sheet_name="SGS_Result", index=False)

        st.download_button(
            "â¬‡ï¸ ä¸‹è¼‰å…¬å¸åˆ¶å¼ Excel",
            output.getvalue(),
            file_name="SGS_Test_Result.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    else:
        st.warning("æœªè®€å–åˆ°æœ‰æ•ˆè³‡æ–™ã€‚")
